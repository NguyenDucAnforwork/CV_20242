{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Font_Detect_Updated v1.ipynb","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11356819,"sourceType":"datasetVersion","datasetId":7107437},{"sourceId":11724279,"sourceType":"datasetVersion","datasetId":7359793}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ndannnop/computer-vision?scriptVersionId=238783849\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pybcf pysam keras-layer-normalization","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-05-09T15:53:16.391988Z","iopub.execute_input":"2025-05-09T15:53:16.392426Z","iopub.status.idle":"2025-05-09T15:53:25.902723Z","shell.execute_reply.started":"2025-05-09T15:53:16.392402Z","shell.execute_reply":"2025-05-09T15:53:25.901982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cd /","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T02:33:24.196014Z","iopub.execute_input":"2025-04-30T02:33:24.196254Z","iopub.status.idle":"2025-04-30T02:33:24.202224Z","shell.execute_reply.started":"2025-04-30T02:33:24.196223Z","shell.execute_reply":"2025-04-30T02:33:24.201382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# try to push the dataset to hf (didn't work)\nfrom datasets import load_dataset, Dataset, Features, Value, Image\nfrom huggingface_hub import login\nimport numpy as np\nfrom PIL import Image as PILImage\nfrom io import BytesIO\nimport os\n\ndef bcf_generator(bcf_file, label_file, num_samples=None):\n    \"\"\"\n    Generator that yields samples one by one without loading entire file into memory\n    \"\"\"\n    # Read labels\n    with open(label_file, 'rb') as f:\n        labels = np.frombuffer(f.read(), dtype=np.uint32)\n    \n    # Open BCF file and keep it open for streaming\n    with open(bcf_file, 'rb') as f:\n        # Read header\n        num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n        \n        # Read all image sizes (small enough to fit in memory)\n        sizes_bytes = f.read(num_images * 8)\n        image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n        \n        # Calculate data start offset\n        data_start_offset = 8 + num_images * 8\n        \n        # Calculate cumulative offsets for seeking\n        offsets = np.zeros(num_images + 1, dtype=np.int64)\n        np.cumsum(image_sizes, out=offsets[1:])\n        \n        # Process only a subset if specified\n        process_count = min(num_samples, num_images) if num_samples else num_images\n        \n        # Yield samples one by one\n        for idx in range(process_count):\n            offset = offsets[idx]\n            size = image_sizes[idx]\n            \n            f.seek(data_start_offset + offset)\n            image_bytes = f.read(size)\n            \n            # Convert to PIL Image\n            try:\n                img = PILImage.open(BytesIO(image_bytes)).convert('L')\n                \n                yield {\n                    \"image\": img,\n                    \"label\": int(labels[idx])\n                }\n            except Exception as e:\n                print(f\"Error processing image {idx}: {e}\")\n                continue\n\n# Login to Hugging Face\nlogin()\n\n# Use streaming dataset approach\nprint(\"Creating streaming dataset...\")\n\n# Path to your BCF files\nbcf_train = '/kaggle/input/adobe-visual-font-recognition/train.bcf'\nlabel_train = '/kaggle/input/adobe-visual-font-recognition/train.label'\n\n# Define features\nfeatures = Features({\n    \"image\": Image(),\n    \"label\": Value(\"int32\")\n})\n\n# Create dataset from generator\ndataset = Dataset.from_generator(\n    generator=lambda: bcf_generator(bcf_train, label_train),\n    features=features\n)\n\n# Push dataset to hub with streaming enabled\ndataset.push_to_hub(\n    \"batmangiaicuuthegioi/VFRtrain\", \n    private=False,\n    max_shard_size=\"500MB\"  # Important: splits data into manageable chunks\n)\n\nprint(\"Dataset uploaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T02:35:17.989034Z","iopub.execute_input":"2025-04-30T02:35:17.989786Z","iopub.status.idle":"2025-04-30T02:37:57.409048Z","shell.execute_reply.started":"2025-04-30T02:35:17.989758Z","shell.execute_reply":"2025-04-30T02:37:57.407562Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"?BcfReader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read bcf file\nimport os\nimport numpy as np\nfrom PIL import Image\nfrom io import BytesIO\n\ndef extract_images_from_bcf(bcf_file, label_file, output_dir=\"/kaggle/working/\"):\n    # os.makedirs(output_dir, exist_ok=True)\n\n    # Step 1: Read label file\n    with open(label_file, 'rb') as f:\n        labels = np.frombuffer(f.read(), dtype=np.uint32)\n\n    # Step 2: Read image data from bcf file\n    with open(bcf_file, 'rb') as f:\n        num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n        image_sizes = np.frombuffer(f.read(num_images * 8), dtype=np.int64)\n        image_data = f.read()\n\n    assert len(labels) == num_images, \"Mismatch between labels and images.\"\n\n    # with open(bcf_file, 'rb') as f:\n    #   data = f.read()\n    \n    # Step 3: Extract and save images\n    offset = 0\n    image_arrays = []\n    for i in range(num_images):\n        size = image_sizes[i]\n        label = labels[i]\n        image_bytes = (image_data[offset:offset+size])\n\n        img = Image.open(BytesIO(image_bytes)).convert('RGB')\n\n        # Convert image to a numpy array (pixel-based array)\n        img_array = np.array(img)\n\n        # Append the image array and its label (if needed)\n        image_arrays.append((label, img_array))\n        \n        # Save image under output/<label>/img_xxxx.png\n        # label_dir = os.path.join(output_dir, f'{label:03d}')\n        # os.makedirs(label_dir, exist_ok=True)\n\n        # image_path = os.path.join(label_dir, f'img_{i:04d}.png')\n        # with open(image_path, 'wb') as out_img:\n        #     out_img.write(image_bytes)\n\n        offset += size\n    return image_arrays","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:53:31.871026Z","iopub.execute_input":"2025-05-09T15:53:31.871303Z","iopub.status.idle":"2025-05-09T15:53:31.879238Z","shell.execute_reply.started":"2025-05-09T15:53:31.871279Z","shell.execute_reply":"2025-05-09T15:53:31.8785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport random\nimport math # Needed for ceiling division\n\n# Helper function to extract patches (updated for grayscale)\ndef extract_patches(image_array, num_patch=3, patch_size=(105, 105)):\n    \"\"\"\n    Extracts a specified number of random patches from a single image array.\n    Handles both grayscale (2D) and color (3D) images.\n\n    Args:\n        image_array (np.ndarray): The input image (Height, Width) or (Height, Width, Channels).\n        num_patch (int): The number of patches to extract.\n        patch_size (tuple): The (height, width) of the patches.\n\n    Returns:\n        list[np.ndarray]: A list containing the extracted patch arrays.\n                          Patches will be 2D (H, W) if input is grayscale.\n                          Returns an empty list if image is smaller than patch size.\n    \"\"\"\n    patches = []\n    if image_array.ndim == 2: # Grayscale\n        h, w = image_array.shape\n        is_grayscale = True\n    elif image_array.ndim == 3: # Color\n        h, w, _ = image_array.shape\n        is_grayscale = False\n    else:\n        print(f\"Warning: Unexpected image array dimension: {image_array.ndim}. Skipping patch extraction.\")\n        return []\n\n    patch_h, patch_w = patch_size\n\n    # Check if image is large enough for at least one patch\n    if h < patch_h or w < patch_w:\n        # print(f\"Warning: Image shape ({h}, {w}) is smaller than patch size ({patch_h}, {patch_w}). Skipping patch extraction for this image.\")\n        return [] # Return empty list if image is too small\n\n    for _ in range(num_patch):\n        # Ensure random coordinates are within valid bounds\n        x = np.random.randint(0, w - patch_w + 1)\n        y = np.random.randint(0, h - patch_h + 1)\n        if is_grayscale:\n            patch = image_array[y:y + patch_h, x:x + patch_w] # Shape: (patch_h, patch_w)\n        else:\n             patch = image_array[y:y + patch_h, x:x + patch_w, :] # Shape: (patch_h, patch_w, C) - Kept for generality but not used in this specific request\n        patches.append(patch)\n    return patches\n\n# Custom Dataset for lazy-loading from BCF\nclass BCFImagePatchDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for loading images from a custom BCF file format lazily\n    and extracting patches on the fly. Loads images as grayscale.\n    \"\"\"\n    def __init__(self, bcf_file, label_file, num_patch=3, patch_size=(105, 105)):\n        \"\"\"\n        Initializes the dataset by reading metadata but not image data.\n\n        Args:\n            bcf_file (str): Path to the BCF file.\n            label_file (str): Path to the label file.\n            num_patch (int): Number of patches to extract per image.\n            patch_size (tuple): (height, width) of patches.\n        \"\"\"\n        self.bcf_file = bcf_file\n        self.label_file = label_file\n        self.num_patch = num_patch\n        self.patch_size = patch_size # Store patch_size for use in collate_fn reference\n\n        self.labels = None\n        self.num_images = 0\n        self.image_sizes = None\n        self.image_offsets = None\n        self.data_start_offset = 0 # Byte offset in BCF where actual image data begins\n\n        self._read_metadata()\n\n    def _read_metadata(self):\n        \"\"\"Reads labels and image size/offset information from the files.\"\"\"\n        try:\n            # Read label file\n            with open(self.label_file, 'rb') as f:\n                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n                print(f\"Read {len(self.labels)} labels.\")\n\n            # Read BCF header\n            with open(self.bcf_file, 'rb') as f:\n                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n                print(f\"BCF header indicates {self.num_images} images.\")\n\n                # Check for consistency\n                if len(self.labels) != self.num_images:\n                    raise ValueError(f\"Mismatch between number of labels ({len(self.labels)}) and images in BCF header ({self.num_images}).\")\n\n                # Read all image sizes\n                sizes_bytes = f.read(self.num_images * 8)\n                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n                print(f\"Read {len(self.image_sizes)} image sizes.\")\n\n                # Calculate the starting offset of the actual image data blob\n                self.data_start_offset = 8 + self.num_images * 8 # 8 bytes for num_images + 8 bytes per size\n\n                # Calculate cumulative offsets for seeking\n                # Offset[i] is the starting byte of image i relative to data_start_offset\n                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n                print(\"Calculated image offsets.\")\n\n        except FileNotFoundError as e:\n            print(f\"Error: File not found - {e}\")\n            raise\n        except Exception as e:\n            print(f\"Error reading metadata: {e}\")\n            raise\n\n    def __len__(self):\n        \"\"\"Returns the total number of images in the dataset.\"\"\"\n        return self.num_images\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Loads one image as grayscale, extracts patches, and returns patches with the label.\n\n        Args:\n            idx (int): The index of the image to retrieve.\n\n        Returns:\n            tuple: (list[np.ndarray], int): A tuple containing:\n                     - A list of NumPy arrays, each representing a patch (H, W).\n                     - The integer label for the image.\n               Returns ([], -1) if image reading or patch extraction fails.\n        \"\"\"\n        if idx >= self.num_images or idx < 0:\n            raise IndexError(f\"Index {idx} out of bounds for {self.num_images} images.\")\n\n        label = self.labels[idx]\n        offset = self.image_offsets[idx]\n        size = self.image_sizes[idx]\n\n        try:\n            # Open the BCF file, seek, read only the required bytes\n            with open(self.bcf_file, 'rb') as f:\n                f.seek(self.data_start_offset + offset)\n                image_bytes = f.read(size)\n\n            # Convert bytes to image (grayscale) and then to numpy array\n            # Use 'L' for grayscale conversion\n            img = Image.open(BytesIO(image_bytes)).convert('L')\n            img_array = np.array(img) # Shape: (H, W)\n\n            # Extract patches from this single grayscale image\n            patches = extract_patches(img_array, self.num_patch, self.patch_size)\n\n            return patches, label # Return list of patches and the single label\n\n        except FileNotFoundError:\n            print(f\"Error: BCF file not found during __getitem__ for index {idx}.\")\n            return [], -1 # Indicate error\n        except Exception as e:\n            print(f\"Error processing image index {idx}: {e}\")\n            return [], -1 # Indicate error\n\n\n# Custom collate function for the DataLoader (updated for grayscale)\ndef patch_collate_fn(batch, patch_size_tuple):\n    \"\"\"\n    Collates data from the BCFImagePatchDataset (handling grayscale).\n\n    Takes a batch of [(patches_list_img1, label1), (patches_list_img2, label2), ...],\n    flattens the patches, converts them to a tensor, adds a channel dimension,\n    normalizes, and returns a single batch tensor for patches and labels.\n\n    Args:\n        batch (list): A list of tuples, where each tuple is the output\n                      of BCFImagePatchDataset.__getitem__.\n        patch_size_tuple (tuple): The (height, width) of patches, needed for empty tensor shape.\n\n\n    Returns:\n        tuple: (torch.Tensor, torch.Tensor): A tuple containing:\n                 - Patches tensor (BatchSize * NumPatches, 1, Height, Width)\n                 - Labels tensor (BatchSize * NumPatches)\n    \"\"\"\n    all_patches = []\n    all_labels = []\n    valid_batch_items = 0\n\n    for item in batch:\n        patches, label = item\n        # Ensure item is valid (e.g., image wasn't too small, no read errors)\n        if patches and label != -1:\n             # Only add patches if the list is not empty\n            all_patches.extend(patches)\n            # Repeat the label for each patch extracted from the image\n            all_labels.extend([label] * len(patches))\n            valid_batch_items += 1\n        # else:\n            # Optionally print a warning if an item was skipped\n            # print(f\"Skipping item in collate_fn due to previous error or no patches.\")\n\n    # If no valid patches were collected in the batch (e.g., all images too small)\n    if not all_patches:\n        # Return empty tensors of appropriate type but 0 size in the batch dimension\n        # Shape for grayscale: (0, 1, H, W)\n        patch_h, patch_w = patch_size_tuple\n        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n\n    # Convert list of NumPy arrays (each H, W) to a single NumPy array\n    patches_np = np.array(all_patches) # Shape: (TotalPatches, H, W)\n\n    # Convert to PyTorch tensor, normalize\n    patches_tensor = torch.tensor(patches_np).float() / 255.0 # Shape: (TotalPatches, H, W)\n\n    # Add channel dimension: (TotalPatches, H, W) -> (TotalPatches, 1, H, W)\n    patches_tensor = patches_tensor.unsqueeze(1)\n\n    # Convert labels to PyTorch tensor\n    labels_tensor = torch.tensor(all_labels, dtype=torch.long) # Use long for classification labels\n\n    # print(f\"Collate - Input Batch Size: {len(batch)}, Valid Items: {valid_batch_items}, Output Patches Shape: {patches_tensor.shape}, Output Labels Shape: {labels_tensor.shape}\")\n\n    return patches_tensor, labels_tensor\n\n\n# --- Main Execution ---\n\n# Example usage:\nbcf_train = '/kaggle/input/deepfont-unlab/VFR_syn_train/train.bcf'\nbcf_val = '/kaggle/input/deepfont-unlab/VFR_syn_val/val.bcf'\nbcf_test = '/kaggle/input/deepfont-unlab/VFR_real_test/vfr_large.bcf'\n\nlabel_train = '/kaggle/input/deepfont-unlab/VFR_syn_train/train.label'\nlabel_val = '/kaggle/input/deepfont-unlab/VFR_syn_val/val.label'\nlabel_test = '/kaggle/input/deepfont-unlab/VFR_real_test/vfr_large.label'\n\nBATCH_SIZE = 1024 # Adjust as needed for your GPU memory\nNUM_PATCHES_PER_IMAGE = 1\nPATCH_SIZE = (105, 105) # Define patch size tuple\nNUM_WORKERS = 4 # Adjust based on your CPU cores, helps speed up loading\n\n# 1. Create the full dataset instance\ntry:\n    train_dataset = BCFImagePatchDataset(\n        bcf_file=bcf_train,\n        label_file=label_train,\n        num_patch=NUM_PATCHES_PER_IMAGE,\n        patch_size=PATCH_SIZE # Pass patch_size to dataset\n    )\n\n    val_dataset = BCFImagePatchDataset(\n        bcf_file=bcf_val,\n        label_file=label_val,\n        num_patch=NUM_PATCHES_PER_IMAGE,\n        patch_size=PATCH_SIZE # Pass patch_size to dataset\n    )\n\n    test_dataset = BCFImagePatchDataset(\n        bcf_file=bcf_test,\n        label_file=label_test,\n        num_patch=NUM_PATCHES_PER_IMAGE,\n        patch_size=PATCH_SIZE # Pass patch_size to dataset\n    )\n\n    # 2. Create indices for splitting\n    # Ensure labels were loaded before stratifying\n    if train_dataset.labels is None:\n         raise ValueError(\"Labels could not be loaded. Cannot stratify split.\")\n\n    # 4. Create DataLoaders using the custom collate function\n    # We need to pass the PATCH_SIZE to the collate function. functools.partial is good for this.\n    from functools import partial\n    collate_wrapper = partial(patch_collate_fn, patch_size_tuple=PATCH_SIZE)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        collate_fn=collate_wrapper, # Use the wrapper\n        pin_memory=True # Set to True if using GPU for faster data transfer\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        collate_fn=collate_wrapper, # Use the wrapper\n        pin_memory=True # Set to True if using GPU for faster data transfer\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        collate_fn=collate_wrapper, # Use the wrapper\n        pin_memory=True\n    )\n\n    # 5. Example loop through the train loader\n    print(\"\\nTesting DataLoader...\")\n    num_batches_to_test = 5\n    for i, (batch_patches, batch_labels) in enumerate(train_loader):\n        if batch_patches.numel() == 0: # Check if the batch is empty\n             print(f\"Batch {i+1}: Skipped (likely due to all images being too small or read errors)\")\n             continue\n\n        print(f\"Batch {i+1}: Patches shape: {batch_patches.shape}, Labels shape: {batch_labels.shape}\")\n        # Example: Check channel dimension is 1\n        if batch_patches.shape[1] != 1:\n             print(f\"Error: Unexpected channel dimension: {batch_patches.shape[1]}\")\n        # print(f\"Batch {i+1}: Labels: {batch_labels}\") # Optional: print labels\n\n        # --- Your training code would go here ---\n        # model(batch_patches) # Ensure your model expects input shape (B, 1, H, W)\n        # loss = criterion(outputs, batch_labels)\n        # ...\n        # ----------------------------------------\n\n        if i >= num_batches_to_test - 1:\n            break\n\n    print(\"\\nDataLoader setup complete and test loop finished.\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred during dataset/dataloader setup: {e}\")\n    import traceback\n    traceback.print_exc() # Print detailed traceback\n    # Depending on the error, you might want to investigate file paths,\n    # file formats, or permissions.\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:53:47.279323Z","iopub.execute_input":"2025-05-09T15:53:47.279885Z","iopub.status.idle":"2025-05-09T15:54:17.186313Z","shell.execute_reply.started":"2025-05-09T15:53:47.279862Z","shell.execute_reply":"2025-05-09T15:54:17.185422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = next(iter(train_loader))\na.shape\n# len(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:56:12.156035Z","iopub.execute_input":"2025-05-09T09:56:12.156647Z","iopub.status.idle":"2025-05-09T09:56:17.315785Z","shell.execute_reply.started":"2025-05-09T09:56:12.156622Z","shell.execute_reply":"2025-05-09T09:56:17.315067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# push to hf\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model\nimport torch\nimport torch.nn as nn\n\"\"\"\nFirst we train the SCAE model on the patches.\nIt contains two conv and 2 deconv layers.\n- conv1: 60x60, 64 filters, stride 1, padding 1 (on all four directions up down left right)\n- pool1: stride 2, kernel size 2, padding 0 (on all four directions up down left right)\n- conv2: 3x3, 128 filters, stride 1, padding 1 (on all four directions up down left right)\n- deconv1: (from 24x24x128 to 24x24x64), 64 filters, stride 1, padding 1, kernel size 3 (on all four directions up down left right)\n- unpool1: (from 24x24x64 to 48x48x64), 64 filters, stride 2, kernel size 2, padding 0 (on all four directions up down left right)\n- deconv2: (from 48x48x64 to 105x105x3), 3 filters, kernel size 60, stride 1, padding 1, dilation 0, output_padding 0 \n\"\"\"\n\nclass SCAE(nn.Module):\n    def __init__(self, normalization_type=\"batch_norm\", use_dropout=False, dropout_prob=0.3, activation=\"leaky_relu\"):\n        super(SCAE, self).__init__()\n\n        def norm_layer(num_features):\n            if normalization_type == \"batch_norm\":\n                return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\":\n                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\":\n                return nn.LayerNorm([num_features, 26, 26])  # nếu input là 26x26\n            else:\n                return nn.Identity()\n\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=60, stride=1, padding=1),\n            norm_layer(64),\n            activation_layer(),\n            dropout_layer(),\n\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            norm_layer(128),\n            activation_layer(),\n            dropout_layer(),\n\n            nn.MaxPool2d(2, 2)\n        )\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n            norm_layer(64),\n            activation_layer(),\n            dropout_layer(),\n\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ConvTranspose2d(64, 1, kernel_size=60, stride=1, padding=1),\n            activation_layer(),\n            \n        )\n\n    def forward(self, x):\n        for layer in self.encoder:\n            x = layer(x)\n            # print(x.shape)\n        for layer in self.decoder:\n            x = layer(x)\n            # print(x.shape)\n        return x\n        \n# test with the first patch\nmodel = SCAE()\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:02:55.723076Z","iopub.execute_input":"2025-05-09T16:02:55.723668Z","iopub.status.idle":"2025-05-09T16:02:55.749006Z","shell.execute_reply.started":"2025-05-09T16:02:55.723633Z","shell.execute_reply":"2025-05-09T16:02:55.748374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:54:16.518511Z","iopub.execute_input":"2025-05-09T09:54:16.51911Z","iopub.status.idle":"2025-05-09T09:54:16.523587Z","shell.execute_reply.started":"2025-05-09T09:54:16.519089Z","shell.execute_reply":"2025-05-09T09:54:16.522916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ndel model, optimizer, criterion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:07:02.847374Z","iopub.execute_input":"2025-05-09T16:07:02.84825Z","iopub.status.idle":"2025-05-09T16:07:02.922263Z","shell.execute_reply.started":"2025-05-09T16:07:02.848217Z","shell.execute_reply":"2025-05-09T16:07:02.921505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model = SCAE().to(device)\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    # Unpack the tuple yielded by the DataLoader\n    for patches, labels in tqdm(train_loader): # Unpack here\n        # Skip empty batches if any occurred\n        if patches.numel() == 0:\n            continue\n\n        patches = patches.to(device)\n        # Note: labels are loaded but not used for the autoencoder training\n        # labels = labels.to(device) # Optional: move labels to device if needed later\n\n        optimizer.zero_grad()\n        outputs = model(patches) # Use patches as input\n        loss = criterion(outputs, patches) # Use patches as target for reconstruction\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    # Avoid division by zero if train_loader is empty or only contained empty batches\n    torch.save(model.state_dict(), f\"/kaggle/working/checkpoint{i+3}\")\n    if len(train_loader) > 0:\n         avg_loss = running_loss / len(train_loader) # Or better: divide by number of non-empty batches processed\n         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n    else:\n         print(f\"Epoch [{epoch+1}/{num_epochs}], No batches processed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T21:41:51.464686Z","iopub.execute_input":"2025-05-09T21:41:51.464967Z","iopub.status.idle":"2025-05-10T01:01:24.021796Z","shell.execute_reply.started":"2025-05-09T21:41:51.464946Z","shell.execute_reply":"2025-05-10T01:01:24.020779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), f\"/kaggle/working/checkpoint{12+3}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T01:07:52.489759Z","iopub.execute_input":"2025-05-10T01:07:52.490477Z","iopub.status.idle":"2025-05-10T01:07:52.501888Z","shell.execute_reply.started":"2025-05-10T01:07:52.490446Z","shell.execute_reply":"2025-05-10T01:07:52.501201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:17:37.463492Z","iopub.execute_input":"2025-05-09T11:17:37.46378Z","iopub.status.idle":"2025-05-09T11:17:41.112171Z","shell.execute_reply.started":"2025-05-09T11:17:37.463751Z","shell.execute_reply":"2025-05-09T11:17:41.111178Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:55:42.027599Z","iopub.execute_input":"2025-05-09T15:55:42.028305Z","iopub.status.idle":"2025-05-09T15:55:42.501912Z","shell.execute_reply.started":"2025-05-09T15:55:42.028278Z","shell.execute_reply":"2025-05-09T15:55:42.501115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_repo = \"batmangiaicuuthegioi/SCAE\"  # Replace this with your repo\nfilename = \"2_epoch\"  # Replace with the actual filename (e.g., pytorch_model.bin)\n\n# Download the file\nmodel_path = hf_hub_download(repo_id=model_repo, filename=filename)\n\n# Check the path of the downloaded model\nprint(f\"Model downloaded to: {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:02:23.85932Z","iopub.execute_input":"2025-05-09T16:02:23.860133Z","iopub.status.idle":"2025-05-09T16:02:24.661041Z","shell.execute_reply.started":"2025-05-09T16:02:23.8601Z","shell.execute_reply":"2025-05-09T16:02:24.660499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SCAE()\nmodel.load_state_dict(torch.load(model_path,  weights_only=True))\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:07:12.57568Z","iopub.execute_input":"2025-05-09T16:07:12.576491Z","iopub.status.idle":"2025-05-09T16:07:12.597646Z","shell.execute_reply.started":"2025-05-09T16:07:12.576461Z","shell.execute_reply":"2025-05-09T16:07:12.597079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nimport torch\n\nmodel_name = \"batmangiaicuuthegioi/SCAE\"  # Replace with your repository name\n\n# Download the model checkpoint (this will download the .lfs model)\nmodel_path = hf_hub_download(repo_id=model_name, filename=\"2_epoch.bin\")  # or other checkpoint name\n\n# Load the model from the checkpoint\nmodel = torch.load(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:00:15.383586Z","iopub.execute_input":"2025-05-09T16:00:15.383916Z","iopub.status.idle":"2025-05-09T16:00:16.036111Z","shell.execute_reply.started":"2025-05-09T16:00:15.383894Z","shell.execute_reply":"2025-05-09T16:00:16.034904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = '/kaggle/working/SCAE_model'\ntorch.save(model.state_dict(), model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:18:27.233047Z","iopub.execute_input":"2025-05-09T11:18:27.233339Z","iopub.status.idle":"2025-05-09T11:18:27.246149Z","shell.execute_reply.started":"2025-05-09T11:18:27.233316Z","shell.execute_reply":"2025-05-09T11:18:27.245453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loaded_model = SCAE()\nloaded_model.load_state_dict(torch.load(\"/kaggle/working/SCAE_model\", weights_only=True))\nloaded_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:21:25.397965Z","iopub.execute_input":"2025-05-09T11:21:25.398801Z","iopub.status.idle":"2025-05-09T11:21:25.433805Z","shell.execute_reply.started":"2025-05-09T11:21:25.398774Z","shell.execute_reply":"2025-05-09T11:21:25.43327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForImageClassification, AutoTokenizer, pipeline\nfrom huggingface_hub import HfApi, HfFolder\nfrom pathlib import Path\n\nmodel_name = \"batmangiaicuuthegioi/SCAE\"  # Change this to your desired Hugging Face model name\nmodel_path = './SCAE_model'\n\n# Create the repository (first time only)\napi = HfApi()\napi.create_repo(model_name, exist_ok=True)\n\n# Push the model\napi.upload_folder(\n    folder_path=model_path,\n    path_in_repo=\".\",\n    repo_id=model_name,\n)\n\n# Optionally, you can push the tokenizer (if you have a tokenizer to upload)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your own tokenizer if applicable\ntokenizer.push_to_hub(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:19:07.301626Z","iopub.execute_input":"2025-05-09T11:19:07.301945Z","iopub.status.idle":"2025-05-09T11:19:24.646364Z","shell.execute_reply.started":"2025-05-09T11:19:07.301921Z","shell.execute_reply":"2025-05-09T11:19:24.645375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# delete all subfolders\nfrom pathlib import Path\nimport shutil\n\n# Set the path to the working directory\nworking_directory = Path('/kaggle/working/')\n\n# Loop over all subfolders and delete them\nfor subfolder in working_directory.iterdir():\n    if subfolder.is_dir():\n        shutil.rmtree(subfolder)\n        print(f\"Deleted: {subfolder}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !mkdir -p ~/.kaggle\n! mv /kaggle.json ~/.kaggle/\n# chmod 600 ~/.kaggle/kaggle.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib.pyplot import imshow\nimport matplotlib.cm as cm\nimport matplotlib.pylab as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport PIL\nfrom PIL import ImageFilter\nimport cv2\nimport itertools\nimport random\nimport keras\nimport imutils\nfrom imutils import paths\nimport os\nfrom keras import optimizers\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras import callbacks\nfrom keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D , UpSampling2D ,Conv2DTranspose\nfrom keras import backend as K\n\n%matplotlib inline","metadata":{"id":"hUkBRdY8ndhZ","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pil_image(img_path):\n    pil_im =PIL.Image.open(img_path).convert('L')\n    pil_im=pil_im.resize((105,105))\n    #imshow(np.asarray(pil_im))\n    return pil_im","metadata":{"id":"JHEynQv2ndhn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Augumentation Steps \n1) Noise\n2) Blur\n3) Perpective Rotation\n4) Shading\n5) Variable Character Spacing\n6) Variable Aspect Ratio","metadata":{"id":"1hbTCU2qndht"}},{"cell_type":"code","source":"def noise_image(pil_im):\n    # Adding Noise to image\n    img_array = np.asarray(pil_im)\n    mean = 0.0   # some constant\n    std = 5   # some constant (standard deviation)\n    noisy_img = img_array + np.random.normal(mean, std, img_array.shape)\n    noisy_img_clipped = np.clip(noisy_img, 0, 255)\n    noise_img = PIL.Image.fromarray(np.uint8(noisy_img_clipped)) # output\n    #imshow((noisy_img_clipped ).astype(np.uint8))\n    noise_img=noise_img.resize((105,105))\n    return noise_img","metadata":{"id":"MLCHbBKsndhv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def blur_image(pil_im):\n    #Adding Blur to image \n    blur_img = pil_im.filter(ImageFilter.GaussianBlur(radius=3)) # ouput\n    #imshow(blur_img)\n    blur_img=blur_img.resize((105,105))\n    return blur_img","metadata":{"id":"5TPvDBV5ndh2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def affine_rotation(img):\n    \n    #img=cv2.imread(img_path,0)\n    rows, columns = img.shape\n\n    point1 = np.float32([[10, 10], [30, 10], [10, 30]])\n    point2 = np.float32([[20, 15], [40, 10], [20, 40]])\n\n    A = cv2.getAffineTransform(point1, point2)\n\n    output = cv2.warpAffine(img, A, (columns, rows))\n    affine_img = PIL.Image.fromarray(np.uint8(output)) # affine rotated output\n    #imshow(output)\n    affine_img=affine_img.resize((105,105))\n    return affine_img\n   ","metadata":{"id":"CIDSvv7Qndh6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gradient_fill(image):\n    #image=cv2.imread(img_path,0)\n    laplacian = cv2.Laplacian(image,cv2.CV_64F)\n    laplacian = cv2.resize(laplacian, (105, 105))\n    return laplacian","metadata":{"id":"VCy6ReUNndh_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparing Dataset","metadata":{"id":"OBLjVHT9ndiF"}},{"cell_type":"code","source":"cd /","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path = \"/kaggle/input/font-patch/font_patch/\"\ndata=[]\nlabels=[]\nimagePaths = sorted(list(paths.list_images(data_path)))\nrandom.seed(42)\nrandom.shuffle(imagePaths)","metadata":{"id":"6hc1RAaVndiI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imagePaths","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conv_label(label):\n    if label == 'Lato':\n        return 0\n    elif label == 'Raleway':\n        return 1\n    elif label == 'Roboto':\n        return 2\n    elif label == 'Sansation':\n        return 3\n    elif label == 'Walkway':\n        return 4","metadata":{"id":"HYYzr_c1ndiN","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augument=[\"blur\",\"noise\",\"affine\",\"gradient\"]\na=itertools.combinations(augument, 4)\n\nfor i in list(a): \n    print(list(i))","metadata":{"id":"L5emmKqjd-3Q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counter=0\nfor imagePath in imagePaths:\n    label = imagePath.split(os.path.sep)[-2]\n    label = conv_label(label)\n    pil_img = pil_image(imagePath)\n    #imshow(pil_img)\n    \n    # Adding original image\n    org_img = img_to_array(pil_img)\n    #print(org_img.shape)\n    data.append(org_img)\n    labels.append(label)\n    \n    augument=[\"noise\",\"blur\",\"affine\",\"gradient\"]\n    for l in range(0,len(augument)):\n    \n        a=itertools.combinations(augument, l+1)\n\n        for i in list(a): \n            combinations=list(i)\n            temp_img = pil_img\n            for j in combinations:\n            \n                if j == 'noise':\n                    # Adding Noise image\n                    temp_img = noise_image(temp_img)\n                    \n                elif j == 'blur':\n                    # Adding Blur image\n                    temp_img = blur_image(temp_img)\n                    #imshow(blur_img)\n                    \n    \n                elif j == 'affine':\n                    open_cv_affine = np.array(pil_img)\n                    # Adding affine rotation image\n                    temp_img = affine_rotation(open_cv_affine)\n\n                elif j == 'gradient':\n                    open_cv_gradient = np.array(pil_img)\n                    # Adding gradient image\n                    temp_img = gradient_fill(open_cv_gradient)\n  \n            temp_img = img_to_array(temp_img)\n            data.append(temp_img)\n            labels.append(label)","metadata":{"id":"PIc22kLf4SAP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = np.asarray(data, dtype=\"float\") / 255.0\nlabels = np.array(labels)\nprint(\"Success\")\n# partition the data into training and testing splits using 75% of\n# the data for training and the remaining 25% for testing\n(trainX, testX, trainY, testY) = train_test_split(data,\n\tlabels, test_size=0.25, random_state=42)","metadata":{"id":"cFpIsgdHndit","outputId":"084a49bf-ee2d-4067-cbde-90d9b42bd8c6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert the labels from integers to vectors\ntrainY = to_categorical(trainY, num_classes=5)\ntestY = to_categorical(testY, num_classes=5)","metadata":{"id":"1NQr6OCQ_3qO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,horizontal_flip=True)","metadata":{"id":"9omeq7fqryGW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"K.set_image_data_format('channels_last')\n","metadata":{"id":"-vWihISP8kHV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model():\n  model=Sequential()\n\n  # Cu Layers \n  model.add(Conv2D(64, kernel_size=(48, 48), activation='relu', input_shape=(105,105,1)))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n\n  model.add(Conv2D(128, kernel_size=(24, 24), activation='relu'))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n\n  model.add(Conv2DTranspose(128, (24,24), strides = (2,2), activation = 'relu', padding='same', kernel_initializer='uniform'))\n  model.add(UpSampling2D(size=(2, 2)))\n\n  model.add(Conv2DTranspose(64, (12,12), strides = (2,2), activation = 'relu', padding='same', kernel_initializer='uniform'))\n  model.add(UpSampling2D(size=(2, 2)))\n\n  #Cs Layers\n  model.add(Conv2D(256, kernel_size=(12, 12), activation='relu'))\n\n  model.add(Conv2D(256, kernel_size=(12, 12), activation='relu'))\n\n  model.add(Conv2D(256, kernel_size=(12, 12), activation='relu'))\n\n  model.add(Flatten())\n\n  model.add(Dense(4096, activation='relu'))\n\n  model.add(Dropout(0.5))\n\n  model.add(Dense(4096,activation='relu'))\n\n  model.add(Dropout(0.5))\n\n  model.add(Dense(2383,activation='relu'))\n\n  model.add(Dense(5, activation='softmax'))\n \n  return model","metadata":{"id":"DpDdwzQguqWR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers.schedules import ExponentialDecay\nlr_schedule = ExponentialDecay(\n    initial_learning_rate=0.01,\n    decay_steps=10000,  # Number of steps before applying decay\n    decay_rate=0.9,     # Decay rate\n    staircase=True)\n\nbatch_size = 128\nepochs = 50\nmodel= create_model()\nsgd = optimizers.SGD(learning_rate=lr_schedule,\n    momentum=0.9,\n    nesterov=True)\nmodel.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])","metadata":{"id":"LSUkpdoI2J-M","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping=callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min')\n\nfilepath=\"top_model.keras\"\n\ncheckpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\ncallbacks_list = [early_stopping,checkpoint]","metadata":{"id":"IH8DclwlLkOw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(trainX, trainY,shuffle=True,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(testX, testY),callbacks=callbacks_list)","metadata":{"id":"ZfjlSwNt73XO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score = model.evaluate(testX, testY, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"QLtRqPzhLOUF","outputId":"7de4cd06-136d-424b-dd2d-6d1d85487384","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('top_model.h5')","metadata":{"id":"9oDaZS8LuWem","outputId":"c308968b-442f-49da-a69e-1a87bdb1cf27","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score = model.evaluate(testX, testY, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"id":"ltfB09zptlNN","outputId":"e023e99b-eb5a-449a-e08f-b124b3f7f284","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(img_path):\n    \n    # img_path=\"/kaggle/input/font-patch/font_patch/Roboto/0HMk9Wef_925.jpg\"\n    pil_im = PIL.Image.open(img_path).convert('L')\n    pil_im = blur_image(pil_im)\n    org_img = img_to_array(pil_im)\n\n    data=[]\n    data.append(org_img)\n    data = np.asarray(data, dtype=\"float\") / 255.0\n    \n    predictions = model.predict(data)  # Get the predicted probabilities\n    predicted_classes = np.argmax(predictions, axis=-1)\n    \n    label = rev_conv_label(int(predicted_classes[0]))\n    fig, ax = plt.subplots(1)\n    ax.imshow(pil_img, interpolation='nearest', cmap=cm.gray)\n    ax.text(5, 5, label , bbox={'facecolor': 'white', 'pad': 10})\n    plt.show()","metadata":{"id":"ov6w2Kmdv4dV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict(\"/kaggle/input/font-patch/font_patch/Roboto/0k_530.jpg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rev_conv_label(label):\n    if label == 0 :\n        return 'Lato'\n    elif label == 1:\n        return 'Raleway'\n    elif label == 2 :\n        return 'Roboto'\n    elif label == 3 :\n        return 'Sansation'\n    elif label == 4:\n        return 'Walkway'","metadata":{"id":"jN4su5FX3MzC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=[]\ndata.append(org_img)\ndata = np.asarray(data, dtype=\"float\") / 255.0","metadata":{"id":"q1yBSPTh0ooD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(data)  # Get the predicted probabilities\npredicted_classes = np.argmax(predictions, axis=-1)","metadata":{"id":"JR2YCKaaznhT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_classes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label = rev_conv_label(int(predicted_classes[0]))\nfig, ax = plt.subplots(1)\nax.imshow(pil_img, interpolation='nearest', cmap=cm.gray)\nax.text(5, 5, label , bbox={'facecolor': 'white', 'pad': 10})\nplt.show()","metadata":{"id":"SQjS-Iv80iLc","outputId":"75bc8aff-55d9-4675-bd36-96070ecfcf49","trusted":true},"outputs":[],"execution_count":null}]}