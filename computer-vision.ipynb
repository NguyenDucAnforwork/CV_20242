{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Font_Detect_Updated v1.ipynb","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11356819,"sourceType":"datasetVersion","datasetId":7107437},{"sourceId":11874818,"sourceType":"datasetVersion","datasetId":7462849},{"sourceId":11802691,"sourceType":"datasetVersion","datasetId":7359793,"isSourceIdPinned":true},{"sourceId":399921,"sourceType":"modelInstanceVersion","modelInstanceId":327270,"modelId":348151}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ndannnop/computer-vision?scriptVersionId=240710736\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pybcf pysam keras-layer-normalization","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport random\nimport math # Needed for ceiling division\n\nimport easyocr\nimport numpy as np\nimport os\nimport tempfile\nfrom PIL import Image\n\n# Global OCR reader for efficiency\n_ocr_reader = None\n\ndef get_ocr_reader(languages=[\"en\"]):\n    global _ocr_reader\n    if _ocr_reader is None:\n        _ocr_reader = easyocr.Reader(languages)\n    return _ocr_reader\n\ndef extract_patches(image_array, num_patch=3, patch_size=(105, 105), \n                    extract_text=True, min_text_coverage=0.3, max_attempts=20):\n    patch_h, patch_w = patch_size\n\n    # Determine if grayscale or color\n    if image_array.ndim == 2:\n        h, w = image_array.shape\n        is_grayscale = True\n    elif image_array.ndim == 3:\n        h, w, _ = image_array.shape\n        is_grayscale = False\n    else:\n        print(f\"Unexpected image shape: {image_array.shape}\")\n        return []\n\n    # === Step 1: Resize image to height = 105, maintain aspect ratio ===\n    scale_factor = patch_h / h\n    new_w = int(w * scale_factor)\n    if is_grayscale:\n        resized = cv2.resize(image_array, (new_w, patch_h), interpolation=cv2.INTER_LINEAR)\n    else:\n        resized = cv2.resize(image_array, (new_w, patch_h), interpolation=cv2.INTER_LINEAR)\n\n    # === Step 2: Check if width is enough for patch ===\n    if new_w < patch_w:\n        return []\n\n    # === Step 3: If not extracting text, return random crops ===\n    if not extract_text:\n        patches = []\n        for _ in range(num_patch):\n            x = np.random.randint(0, new_w - patch_w + 1)\n            if is_grayscale:\n                patch = resized[:, x:x + patch_w]\n            else:\n                patch = resized[:, x:x + patch_w, :]\n            patches.append(patch)\n        return patches\n\n    # === Step 4: Try to find text patches using OCR ===\n    reader = get_ocr_reader()\n    text_patches = []\n    attempts = 0\n\n    while len(text_patches) < num_patch and attempts < max_attempts:\n        x = np.random.randint(0, new_w - patch_w + 1)\n        if is_grayscale:\n            patch = resized[:, x:x + patch_w]\n        else:\n            patch = resized[:, x:x + patch_w, :]\n\n        # Save patch to temporary file for OCR\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:\n            tmp_path = tmp.name\n            patch_img = Image.fromarray(patch)\n            patch_img.save(tmp_path)\n\n        try:\n            ocr_results = reader.readtext(tmp_path)\n            os.unlink(tmp_path)\n\n            patch_area = patch_h * patch_w\n            text_area = 0\n            for bbox, text, conf in ocr_results:\n                if conf < 0.5:\n                    continue\n                bbox = [[int(p[0]), int(p[1])] for p in bbox]\n                min_x = max(0, min(p[0] for p in bbox))\n                max_x = min(patch_w, max(p[0] for p in bbox))\n                min_y = max(0, min(p[1] for p in bbox))\n                max_y = min(patch_h, max(p[1] for p in bbox))\n                if max_x > min_x and max_y > min_y:\n                    text_area += (max_x - min_x) * (max_y - min_y)\n\n            if text_area / patch_area >= min_text_coverage:\n                text_patches.append(patch)\n\n        except Exception as e:\n            print(f\"OCR error: {e}\")\n            try:\n                os.unlink(tmp_path)\n            except:\n                pass\n\n        attempts += 1\n\n    return text_patches\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:55:18.142009Z","iopub.execute_input":"2025-05-20T03:55:18.142296Z","iopub.status.idle":"2025-05-20T03:55:21.783151Z","shell.execute_reply.started":"2025-05-20T03:55:18.14227Z","shell.execute_reply":"2025-05-20T03:55:21.782338Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# augmentation functions\nimport random\nimport numpy as np\nimport cv2\nfrom PIL import Image, ImageFilter\n\nTARGET_SIZE = (105, 105)\n\ndef to_uint8(img: np.ndarray) -> np.ndarray:\n    return np.clip(img, 0, 255).astype(np.uint8)\n\ndef noise_image(img: np.ndarray, mean=0.0, std=3.0) -> np.ndarray:\n    \"\"\"Add Gaussian noise.\"\"\"\n    f = img.astype(np.float32)\n    n = np.random.normal(mean, std, f.shape).astype(np.float32)\n    return to_uint8(f + n)\n\ndef blur_image(img: np.ndarray, sigma_range=(0.5, 1.5)) -> np.ndarray:\n    \"\"\"\n    Apply a mild, randomly‐parameterized Gaussian blur.\n    \n    Args:\n        img (np.ndarray): Input image, either H×W or H×W×C, dtype uint8.\n        sigma_range (tuple): Min/max sigma for the blur kernel.\n    \n    Returns:\n        np.ndarray: Blurred image, same shape and dtype uint8.\n    \"\"\"\n    # Ensure float for convolution\n    f = img.astype(np.float32)\n    # Randomize sigma in the given range\n    sigma = random.uniform(*sigma_range)\n    # OpenCV: kernel size (0,0) triggers automatic size based on sigma\n    if f.ndim == 2:\n        blurred = cv2.GaussianBlur(f, ksize=(0, 0), sigmaX=sigma, sigmaY=sigma)\n    else:\n        # Split channels and blur each independently\n        channels = cv2.split(f)\n        channels = [cv2.GaussianBlur(ch, ksize=(0, 0), sigmaX=sigma, sigmaY=sigma)\n                    for ch in channels]\n        blurred = cv2.merge(channels)\n    # Restore uint8\n    return np.clip(blurred, 0, 255).astype(np.uint8)\n\ndef affine_rotation(img: np.ndarray, max_deg=10) -> np.ndarray:\n    \"\"\"Small random affine warp.\"\"\"\n    h, w = img.shape[:2]\n    # random shift on three points\n    src = np.float32([[0,0],[w-1,0],[0,h-1]])\n    dx = w * 0.05\n    dy = h * 0.05\n    dst = src + np.random.uniform([-dx, -dy], [dx, dy], src.shape).astype(np.float32)\n    M = cv2.getAffineTransform(src, dst)\n    warped = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n    return warped\n\ndef shading_gradient(img: np.ndarray) -> np.ndarray:\n    \"\"\"Multiply by a random horizontal or vertical linear illumination gradient.\"\"\"\n    h, w = img.shape[:2]\n    start, end = random.uniform(0.6,1.4), random.uniform(0.6,1.4)\n    if random.choice([True,False]):\n        # horizontal\n        grad = np.linspace(start, end, w, dtype=np.float32)[None,:]\n        mask = np.repeat(grad, h, axis=0)\n    else:\n        # vertical\n        grad = np.linspace(start, end, h, dtype=np.float32)[:,None]\n        mask = np.repeat(grad, w, axis=1)\n    if img.ndim==3:\n        mask = mask[:,:,None]\n    shaded = img.astype(np.float32) * mask\n    return to_uint8(shaded)\n\ndef variable_aspect_ratio_preprocess(img: np.ndarray) -> np.ndarray:\n    \"\"\"Squeeze width by a random factor in [5/6,7/6], then pad/crop to original.\"\"\"\n    h, w = img.shape[:2]\n    factor = random.uniform(5/6, 7/6)\n    new_w = max(1, int(w/factor))\n    resized = cv2.resize(img, (new_w, h), interpolation=cv2.INTER_LINEAR)\n    # pad or crop back to w x h\n    if new_w < w:\n        pad = ( (0,0), ( (w-new_w)//2, (w-new_w)-(w-new_w)//2 ) ) + ((0,0),) if img.ndim==3 else ( (0,0), ( (w-new_w)//2, (w-new_w)-(w-new_w)//2 ) )\n        resized = np.pad(resized, pad, mode='reflect')\n    else:\n        x0 = (new_w - w)//2\n        resized = resized[:, x0:x0+w]\n    return resized\n\ndef final_resize(img: np.ndarray, size=TARGET_SIZE) -> np.ndarray:\n    \"\"\"Resize to target patch size.\"\"\"\n    return cv2.resize(img, size, interpolation=cv2.INTER_LINEAR)\n\ndef augmentation_pipeline(img: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Apply a random subset of the six DeepFont augmentations:\n      - noise, blur, affine warp, shading, variable spacing (as AR squeeze)\n      - note: gradient_fill (Laplacian) dropped since shading covers illumination.\n    Finally, resize to TARGET_SIZE.\n    \"\"\"\n    # Ensure uint8\n    img = to_uint8(img)\n    # 1) variable aspect ratio\n    img = variable_aspect_ratio_preprocess(img)\n    # 2) choose 2–4 augmentations from the pool\n    pool = [\n        noise_image,\n        blur_image,\n        affine_rotation,\n        shading_gradient\n    ]\n    for fn in pool:\n        img = fn(img)\n    # 3) final resize\n    img = final_resize(img)\n    return img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:55:22.538229Z","iopub.execute_input":"2025-05-20T03:55:22.538704Z","iopub.status.idle":"2025-05-20T03:55:22.55489Z","shell.execute_reply.started":"2025-05-20T03:55:22.538679Z","shell.execute_reply":"2025-05-20T03:55:22.554069Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# image dataset\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom io import BytesIO\n# from datasets import Dataset\nfrom torch.utils.data import Dataset\n\nclass ImageDataset(Dataset):\n    \"\"\"\n    A dataset class that combines both .jpeg files and .bcf files into a single dataset.\n    This class can handle loading and patch extraction from both .jpeg and .bcf files.\n    \"\"\"\n    def __init__(self, jpeg_dir, bcf_file, label_file, testing=False, num_patch=3, patch_size=(105, 105), \n                 extract_text=False, min_text_coverage=0.3, max_attempts=20, ocr_languages=[\"en\"]):\n        \"\"\"\n        Initializes the dataset by loading both jpeg files and bcf files into one dataset.\n\n        Args:\n            jpeg_dir (str): Directory containing .jpeg files.\n            bcf_file (str): Path to the .bcf file.\n            label_file (str): Path to the label file corresponding to the .bcf file.\n            num_patch (int): Number of patches to extract per image.\n            patch_size (tuple): Tuple (height, width) for the size of the patches.\n            extract_text (bool): Whether to prioritize patches containing text\n            min_text_coverage (float): Minimum ratio of text area to patch area (0-1)\n            max_attempts (int): Maximum number of attempts to find text patches\n            ocr_languages (list): Languages for EasyOCR to detect\n        \"\"\"\n        self.jpeg_dir = jpeg_dir\n        self.bcf_file = bcf_file\n        self.label_file = label_file\n        self.testing = testing\n        self.num_patch = num_patch\n        self.patch_size = patch_size\n        self.extract_text = extract_text\n        self.min_text_coverage = min_text_coverage\n        self.max_attempts = max_attempts\n        self.ocr_languages = ocr_languages\n\n        # Initialize OCR reader if needed\n        if extract_text:\n            self.reader = get_ocr_reader(ocr_languages)\n\n        self.jpeg_data = []\n        self.bcf_data = []\n\n        # Load jpeg data\n        self._load_jpeg_data(jpeg_dir)\n\n        # Load bcf data\n        self._load_bcf_data(bcf_file, label_file)\n\n    def _extract_patches_test(self, img_array):\n        h, w = img_array.shape[:2]\n        target_h, target_w = self.patch_size\n        # 1) resize height\n        new_w = int(w * (target_h / h))\n        img = cv2.resize(img_array, (new_w, target_h), interpolation=cv2.INTER_LINEAR)\n        patches = []\n        for _scale in range(3):\n            factor = np.random.uniform(1.5, 3.5)\n            sw = max(1, int(new_w / factor))\n            squeezed = cv2.resize(img, (sw, target_h), interpolation=cv2.INTER_LINEAR)\n            # nếu width < target_w thì pad reflect, else crop giữa\n            if sw < target_w:\n                pad = target_w - sw\n                left = pad//2; right = pad-left\n                squeezed = np.pad(squeezed,\n                                  ((0,0),(left,right)) + ((0,0),)*(img.ndim-2),\n                                  mode='reflect')\n            else:\n                x0 = (sw - target_w)//2\n                squeezed = squeezed[:, x0:x0+target_w]\n            # crop 5 patch random\n            for _ in range(5):\n                x = np.random.randint(0, target_w - target_w + 1)\n                y = np.random.randint(0,      0  + 1)  # vì height==target_h\n                patch = squeezed[y:y+target_h, x:x+target_w] if img.ndim==2 \\\n                        else squeezed[y:y+target_h, x:x+target_w, :]\n                patches.append(patch)\n        return patches\n\n    def _load_jpeg_data(self, jpeg_dir):\n        \"\"\"Loads the .jpeg files from the specified directory.\"\"\"\n        if not os.path.exists(jpeg_dir):\n            print(f\"Warning: JPEG directory {jpeg_dir} does not exist.\")\n            return\n            \n        image_filenames = [f for f in os.listdir(jpeg_dir) if f.lower().endswith(('.jpeg', '.jpg'))]\n        self.jpeg_data = [(os.path.join(jpeg_dir, f), 0) for f in image_filenames]  # Assuming label is 0 for .jpeg files\n        print(f\"Loaded {len(self.jpeg_data)} .jpeg images.\")\n\n    def _load_bcf_data(self, bcf_file, label_file):\n        \"\"\"Loads the .bcf file and the associated label file.\"\"\"\n        if not (os.path.exists(bcf_file) and os.path.exists(label_file)):\n            print(f\"Warning: BCF file {bcf_file} or label file {label_file} does not exist.\")\n            return\n            \n        try:\n            with open(label_file, 'rb') as f:\n                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n                print(f\"Loaded {len(self.labels)} labels from {label_file}.\")\n\n            with open(bcf_file, 'rb') as f:\n                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n                print(f\"Loaded {self.num_images} images from {bcf_file}.\")\n\n                sizes_bytes = f.read(self.num_images * 8)\n                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n\n                self.data_start_offset = 8 + self.num_images * 8\n                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n\n                for idx in range(self.num_images):\n                    self.bcf_data.append((idx, self.labels[idx]))\n                \n            print(f\"Loaded {len(self.bcf_data)} .bcf images.\")\n        except Exception as e:\n            print(f\"Error loading .bcf data: {e}\")\n\n    def __len__(self):\n        \"\"\"Returns the total number of images in the combined dataset.\"\"\"\n        return len(self.jpeg_data) + len(self.bcf_data)\n\n    def _extract_patches(self, img_array):\n        \"\"\"Helper function to extract patches from an image.\"\"\"\n        return extract_patches(\n            img_array, \n            num_patch=self.num_patch, \n            patch_size=self.patch_size,\n            extract_text=self.extract_text, \n            min_text_coverage=self.min_text_coverage,\n            max_attempts=self.max_attempts\n        )\n\n    def __getitem__(self, idx):\n        \"\"\"Fetches one item with robust error handling for corrupted images.\"\"\"\n        # Handle case where idx is a list (batch loading)\n        if isinstance(idx, list):\n            # Handle batch indices properly\n            results = []\n            labels = []\n            for single_idx in idx:\n                try:\n                    patches, label = self.__getitem__(single_idx)  # Call recursively with single index\n                    if patches and label != -1:\n                        results.append(patches)\n                        labels.append(label)\n                except Exception as e:\n                    print(f\"Error processing index {single_idx}: {e}\")\n                    # Skip this item on error\n            \n            # Return whatever valid items we were able to get\n            return results, labels\n        \n        # Original single-item loading logic\n        max_retries = 3  # Try a few times before giving up on an index\n        \n        for retry in range(max_retries):\n            try:\n                if idx < len(self.jpeg_data):\n                    # JPEG image with error handling\n                    img_path, label = self.jpeg_data[idx]\n                    try:\n                        with warnings.catch_warnings():\n                            warnings.simplefilter(\"ignore\")  # Ignore PIL warnings\n                            img = Image.open(img_path)\n                            img.verify()  # Verify image is not corrupted\n                        \n                        # Re-open since verify() closes the file\n                        img = Image.open(img_path).convert('L')\n                        img_array = np.array(img)\n                        patches = self._extract_patches(img_array)\n                        # patches = [augmentation_pipeline(patch) for patch in patches]\n                        \n                        # Clean memory\n                        del img, img_array\n                        \n                        return patches, label\n                    \n                    except (OSError, IOError, ValueError) as e:\n                        # Image is corrupted, return empty list\n                        print(f\"Warning: Corrupt image at {img_path}: {e}\")\n                        return [], -1\n                        \n                else:\n                    # BCF image with error handling\n                    bcf_idx = idx - len(self.jpeg_data)\n                    if bcf_idx >= len(self.bcf_data):\n                        return [], -1\n                        \n                    label = self.bcf_data[bcf_idx][1]\n                    offset = self.image_offsets[bcf_idx]\n                    size = self.image_sizes[bcf_idx]\n                    \n                    try:\n                        with open(self.bcf_file, 'rb') as f:\n                            f.seek(self.data_start_offset + offset)\n                            image_bytes = f.read(size)\n                        \n                        # Use BytesIO to catch corruption\n                        buffer = BytesIO(image_bytes)\n                        img = Image.open(buffer)\n                        img.verify()  # Verify it's valid\n                        \n                        # Re-open since verify() closes the file\n                        buffer.seek(0)\n                        img = Image.open(buffer).convert('L')\n                        img_array = np.array(img)\n                        \n                        if self.testing:\n                            patches = self._extract_patches_test(img_array)\n                        else:\n                            patches = self._extract_patches(img_array)\n                            patches = [augmentation_pipeline(patch) for patch in patches]\n                        \n                        # Clean memory\n                        del img, img_array, buffer, image_bytes\n                        \n                        return patches, label\n                    \n                    except (OSError, IOError, ValueError) as e:\n                        print(f\"Warning: Corrupt BCF image at index {bcf_idx}: {e}\")\n                        return [], -1\n                        \n            except Exception as e:\n                print(f\"Unexpected error processing idx {idx}: {e}\")\n            \n            # If we got here, there was an issue with this index - try a different one\n            # Important: increment as an integer, not trying to add to a list\n            if retry < max_retries - 1:  # Only increment if we have retries left\n                idx = (int(idx) + 1) % len(self)\n        \n        # If all retries failed, return empty\n        return [], -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:55:25.923526Z","iopub.execute_input":"2025-05-20T03:55:25.924488Z","iopub.status.idle":"2025-05-20T03:55:25.94613Z","shell.execute_reply.started":"2025-05-20T03:55:25.924464Z","shell.execute_reply":"2025-05-20T03:55:25.945486Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# memory_efficient_patch_collate_fn\nimport gc\nimport warnings\nfrom functools import partial\n\n# Add this memory-efficient patch collate function\ndef memory_efficient_patch_collate_fn(batch, patch_size_tuple):\n    \"\"\"\n    Memory-efficient version of patch_collate_fn that processes one patch at a time\n    and includes robust error handling.\n    \"\"\"\n    import gc  # Import inside function for worker processes\n    \n    all_patches = []\n    all_labels = []\n    valid_batch_items = 0\n\n    # Process one item at a time to avoid large memory allocations\n    for item in batch:\n        patches, label = item\n        # Ensure item is valid\n        if patches and label != -1:\n            # Process patches one by one\n            for patch in patches:\n                all_patches.append(patch)\n                all_labels.append(label)\n            valid_batch_items += 1\n    \n    # Periodically force garbage collection\n    if len(all_patches) > 100:\n        gc.collect()\n    \n    # Empty batch handling\n    if not all_patches:\n        patch_h, patch_w = patch_size_tuple\n        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n\n    # Process in smaller chunks to reduce peak memory usage\n    max_chunk_size = 64  # Adjust based on your GPU memory\n    num_patches = len(all_patches)\n    patches_tensor_list = []\n    \n    for i in range(0, num_patches, max_chunk_size):\n        chunk = all_patches[i:i+max_chunk_size]\n        # Convert to NumPy array\n        chunk_np = np.stack(chunk)\n        # Convert to tensor, normalize and add channel dimension\n        chunk_tensor = torch.from_numpy(chunk_np).float() / 255.0\n        chunk_tensor = chunk_tensor.unsqueeze(1)\n        patches_tensor_list.append(chunk_tensor)\n        \n        # Clear variables to free memory\n        del chunk, chunk_np\n    \n    # Concatenate chunks\n    patches_tensor = torch.cat(patches_tensor_list, dim=0)\n    labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n    \n    # Clean up\n    del patches_tensor_list, all_patches, all_labels\n    gc.collect()\n    \n    return patches_tensor, labels_tensor\n\n# Add this function to create optimized DataLoaders\nimport torch\nfrom torch.utils.data import DataLoader\nfrom functools import partial\n\ndef create_optimized_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper error handling, avoiding HuggingFace datasets compatibility issues.\n    \n    Args:\n        dataset: The image dataset instance\n        batch_size: Batch size for training\n        num_workers: Number of worker processes\n        val_split: Validation split ratio (0-1)\n        \n    Returns:\n        tuple: (train_loader, val_loader)\n    \"\"\"\n    from torch.utils.data import DataLoader, Subset\n    import numpy as np\n    \n    # Calculate split sizes\n    dataset_size = len(dataset)\n    indices = np.arange(dataset_size)\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets - this avoids Hugging Face's __getitems__ implementation\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    # Custom collate function with error handling\n    def safe_collate(batch):\n        # Filter out empty or invalid items\n        valid_batch = [(patches, label) for patches, label in batch if patches and label != -1]\n        \n        if not valid_batch:\n            # Return empty tensors if no valid items\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n        \n        # Process valid items\n        all_patches = []\n        all_labels = []\n        \n        for patches, label in valid_batch:\n            if isinstance(patches, list) and patches:\n                all_patches.extend(patches)\n                all_labels.extend([label] * len(patches))\n        \n        if not all_patches:\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n            \n        # Convert to PyTorch tensors\n        try:\n            patches_np = np.array(all_patches)\n            patches_tensor = torch.tensor(patches_np, dtype=torch.float) / 255.0\n            \n            # Add channel dimension if needed\n            if len(patches_tensor.shape) == 3:  # (B, H, W)\n                patches_tensor = patches_tensor.unsqueeze(1)  # -> (B, 1, H, W)\n                \n            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n            return patches_tensor, labels_tensor\n        except Exception as e:\n            print(f\"Error in collate function: {e}\")\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n    \n    # Create DataLoaders with minimal worker configuration for stability\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-20T03:55:29.492707Z","iopub.execute_input":"2025-05-20T03:55:29.493011Z","iopub.status.idle":"2025-05-20T03:55:29.506732Z","shell.execute_reply.started":"2025-05-20T03:55:29.492986Z","shell.execute_reply":"2025-05-20T03:55:29.505878Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# collate functions\nimport torch\nimport numpy as np\n\ndef test_collate_fn(batch):\n    \"\"\"\n    batch: list of tuples (patches_list, label)\n      - patches_list: list of P numpy arrays of shape (H, W) or (H, W, C)\n      - label: int\n    Returns:\n      - images: Tensor of shape (B, P, C, H, W)\n      - labels: Tensor of shape (B,)\n    \"\"\"\n    images = []\n    labels = []\n    for patches_list, label in batch:\n        patch_tensors = []\n        for patch in patches_list:\n            # patch is a numpy array\n            arr = patch\n            # grayscale or RGB?\n            if arr.ndim == 2:\n                # (H, W) → (1, H, W)\n                t = torch.from_numpy(arr).unsqueeze(0)\n            else:\n                # (H, W, C) → (C, H, W)\n                t = torch.from_numpy(arr).permute(2, 0, 1)\n            # normalize to [0,1] float\n            t = t.float().div(255.0)\n            patch_tensors.append(t)\n        # stack P patches → (P, C, H, W)\n        images.append(torch.stack(patch_tensors, dim=0))\n        labels.append(label)\n    # now batch them → (B, P, C, H, W) and (B,)\n    images = torch.stack(images, dim=0)\n    labels = torch.tensor(labels, dtype=torch.long)\n    return images, labels\n    \ndef train_collate_fn(batch):\n    \"\"\"\n    batch: list of (patches_list, label) where\n      patches_list: list of P numpy arrays, each H×W or H×W×C\n      label: int\n    Returns:\n      images: Tensor of shape (B*P, C, H, W)\n      labels: Tensor of shape (B*P,)\n    \"\"\"\n    imgs = []\n    lbls = []\n    for patches, label in batch:\n        for patch in patches:\n            arr = patch\n            # to torch tensor: (H,W)→(1,H,W), (H,W,C)→(C,H,W)\n            if arr.ndim == 2:\n                t = torch.from_numpy(arr).unsqueeze(0)\n            else:\n                t = torch.from_numpy(arr).permute(2, 0, 1)\n            # normalize to [0,1]\n            imgs.append(t.float().div(255.0))\n            lbls.append(label)\n    images = torch.stack(imgs, dim=0)\n    labels = torch.tensor(lbls, dtype=torch.long)\n    return images, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:55:32.966841Z","iopub.execute_input":"2025-05-20T03:55:32.967135Z","iopub.status.idle":"2025-05-20T03:55:32.974563Z","shell.execute_reply.started":"2025-05-20T03:55:32.96711Z","shell.execute_reply":"2025-05-20T03:55:32.973642Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# save and load dataset\nimport pickle\n\ndef save_dataset(dataset, filepath: str):\n    \"\"\"\n    Serialize and save a CombinedImageDataset to disk.\n    \"\"\"\n    with open(filepath, 'wb') as f:\n        pickle.dump(dataset, f)\n    print(f\"Dataset saved to {filepath!r}\")\n\ndef load_dataset(filepath: str):\n    \"\"\"\n    Load a pickled CombinedImageDataset from disk.\n    \"\"\"\n    with open(filepath, 'rb') as f:\n        dataset = pickle.load(f)\n    print(f\"Dataset loaded from {filepath!r}\")\n    return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:55:36.506262Z","iopub.execute_input":"2025-05-20T03:55:36.506878Z","iopub.status.idle":"2025-05-20T03:55:36.511488Z","shell.execute_reply.started":"2025-05-20T03:55:36.506851Z","shell.execute_reply":"2025-05-20T03:55:36.510674Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Visualization some samples from the combined dataset \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom PIL import Image, ImageFile\nfrom io import BytesIO\nimport os\n\ndef visualize_simple_images_and_patches(dataset, num_images=2, seed=None):\n    # Allow loading of truncated images\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n    \n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Find valid images (with patches)\n    valid_indices = []\n    attempts = 0\n    max_attempts = min(len(dataset) * 2, 100)  # Limit search attempts\n    \n    while len(valid_indices) < num_images and attempts < max_attempts:\n        idx = random.randint(0, len(dataset) - 1)\n        if idx not in valid_indices:  # Avoid duplicates\n            try:\n                patches, label = dataset[idx]\n                if patches and len(patches) > 0:\n                    valid_indices.append(idx)\n            except Exception as e:\n                print(f\"Error loading index {idx}: {e}\")\n            attempts += 1\n    \n    # If we couldn't find enough valid images\n    if len(valid_indices) < num_images:\n        print(f\"Warning: Could only find {len(valid_indices)} valid images with patches\")\n        if len(valid_indices) == 0:\n            print(\"No valid images found. Check your dataset.\")\n            return\n    \n    # Create figure with enough space for all elements\n    fig, axes = plt.subplots(len(valid_indices), 4, figsize=(16, 5 * len(valid_indices)))\n    \n    # If only one image is requested, make axes indexable as 2D\n    if len(valid_indices) == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i, idx in enumerate(valid_indices):\n        try:\n            # Get item directly from dataset\n            patches, label = dataset[idx]\n            \n            # Get the original full image\n            img_array = None\n            \n            if hasattr(dataset, 'jpeg_data') and idx < len(dataset.jpeg_data):\n                # From jpeg_data\n                img_path, _ = dataset.jpeg_data[idx]\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {os.path.basename(img_path)}\"\n                \n            elif hasattr(dataset, 'image_filenames') and not hasattr(dataset, 'num_images'):\n                # From BCFImagePatchDataset with JPEG source\n                img_path = os.path.join(dataset.data_source, dataset.image_filenames[idx])\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {dataset.image_filenames[idx]}\"\n                \n            else:\n                # From BCF file (either CombinedImageDataset or BCFImagePatchDataset)\n                if hasattr(dataset, 'bcf_data'):\n                    # CombinedImageDataset\n                    bcf_idx = idx - len(dataset.jpeg_data)\n                    if bcf_idx < 0 or bcf_idx >= len(dataset.bcf_data):\n                        print(f\"Invalid BCF index: {bcf_idx}\")\n                        continue\n                        \n                    offset = dataset.image_offsets[bcf_idx]\n                    size = dataset.image_sizes[bcf_idx]\n                    data_file = dataset.bcf_file\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {bcf_idx})\"\n                else:\n                    # BCFImagePatchDataset\n                    offset = dataset.image_offsets[idx]\n                    size = dataset.image_sizes[idx]\n                    data_file = dataset.data_source\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {idx})\"\n                \n                with open(data_file, 'rb') as f:\n                    f.seek(data_start + offset)\n                    image_bytes = f.read(size)\n                img = Image.open(BytesIO(image_bytes)).convert('L')\n                img_array = np.array(img)\n            \n            # Plot original image if we successfully loaded it\n            if img_array is not None:\n                axes[i, 0].imshow(img_array, cmap='gray')\n                axes[i, 0].set_title(f\"Original Image\\nLabel: {label}\\nSource: {source}\")\n                axes[i, 0].axis('off')\n            else:\n                axes[i, 0].text(0.5, 0.5, \"Image loading failed\", ha='center', va='center')\n                axes[i, 0].axis('off')\n            \n            # Plot the patches - ensure we have patches to display\n            if patches and len(patches) > 0:\n                for j in range(3):\n                    if j < len(patches):\n                        patch = patches[j]\n                        axes[i, j+1].imshow(patch, cmap='gray')\n                        axes[i, j+1].set_title(f\"Patch {j+1}\\nShape: {patch.shape}\")\n                    else:\n                        # No more patches to display\n                        axes[i, j+1].text(0.5, 0.5, \"No patch\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            else:\n                # No patches for this image\n                for j in range(3):\n                    axes[i, j+1].text(0.5, 0.5, \"No patches extracted\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            \n        except Exception as e:\n            print(f\"Error processing index {idx}: {e}\")\n            # Create error message in subplot\n            for j in range(4):\n                axes[i, j].text(0.5, 0.5, f\"Error: {str(e)[:50]}...\", ha='center', va='center')\n                axes[i, j].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return the indices we used (helpful for debugging)\n    return valid_indices\n\n# Example usage:\nvisualize_simple_images_and_patches(synthetic_dataset.dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"synthetic_dataset = load_dataset(\"/kaggle/working/synthetic_dataset.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(synthetic_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport gc\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\n\n# Clean memory before starting\ngc.collect()\ntorch.cuda.empty_cache()\n\njpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\nbcf_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf\"\nlabel_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label\"\n\n# Create dataset with smaller patch size and fewer patches per image\nsynthetic_dataset = ImageDataset(\n    jpeg_dir=\"jpeg_dir\",\n    bcf_file=bcf_file,\n    label_file=label_file,\n    # testing=True,\n    num_patch=3,  # Number of patches per image\n)\n# save_dataset(test_dataset, \"/kaggle/working/test_datasets.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:55:43.342452Z","iopub.execute_input":"2025-05-20T03:55:43.343208Z","iopub.status.idle":"2025-05-20T03:55:43.63012Z","shell.execute_reply.started":"2025-05-20T03:55:43.343182Z","shell.execute_reply":"2025-05-20T03:55:43.629292Z"}},"outputs":[{"name":"stdout","text":"Warning: JPEG directory jpeg_dir does not exist.\nLoaded 200000 labels from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label.\nLoaded 200000 images from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf.\nLoaded 200000 .bcf images.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"save_dataset(synthetic_dataset, \"/kaggle/working/synthetic_dataset.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:56:30.903059Z","iopub.execute_input":"2025-05-20T03:56:30.903763Z","iopub.status.idle":"2025-05-20T03:56:31.426383Z","shell.execute_reply.started":"2025-05-20T03:56:30.903737Z","shell.execute_reply":"2025-05-20T03:56:31.425476Z"}},"outputs":[{"name":"stdout","text":"Dataset saved to '/kaggle/working/synthetic_dataset.pkl'\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n# combined_dataset = load_dataset(\"/kaggle/working/combined_dataset.pkl\")\n\ndataset = synthetic_dataset\n# 1. Decide split sizes\ntotal = len(dataset)\ntrain_size = int(0.8 * total)\nval_size   = total - train_size\n\n# 2. Split with a fixed seed for reproducibility\ntrain_dataset, val_dataset = random_split(\n    dataset,\n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\nprint(f\"Train samples: {len(train_dataset)},  Eval samples: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:58:12.274281Z","iopub.execute_input":"2025-05-20T03:58:12.274926Z","iopub.status.idle":"2025-05-20T03:58:12.291849Z","shell.execute_reply.started":"2025-05-20T03:58:12.274898Z","shell.execute_reply":"2025-05-20T03:58:12.290981Z"}},"outputs":[{"name":"stdout","text":"Train samples: 160000,  Eval samples: 40000\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset,\n    batch_size=128,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n    collate_fn=train_collate_fn # test_collate_fn\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=64,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True,\n    collate_fn=train_collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:58:14.004607Z","iopub.execute_input":"2025-05-20T03:58:14.004902Z","iopub.status.idle":"2025-05-20T03:58:14.009731Z","shell.execute_reply.started":"2025-05-20T03:58:14.004878Z","shell.execute_reply":"2025-05-20T03:58:14.00899Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"for batch in val_loader:\n    images, labels = batch\n    print(f\"Batch size: {images.size()}, Labels size: {labels.size()}\")\n    break  # Just check the first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:58:18.725666Z","iopub.execute_input":"2025-05-20T03:58:18.725961Z","iopub.status.idle":"2025-05-20T03:58:20.915831Z","shell.execute_reply.started":"2025-05-20T03:58:18.72594Z","shell.execute_reply":"2025-05-20T03:58:20.914888Z"}},"outputs":[{"name":"stdout","text":"Batch size: torch.Size([192, 1, 105, 105]), Labels size: torch.Size([192])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import Subset\n\nds = synthetic_dataset\n# 1) Build a single labels array for the whole dataset\njpeg_labels = np.array([lbl for _, lbl in ds.jpeg_data], dtype=int)\nbcf_labels  = ds.labels           # already an np.ndarray\nall_labels  = np.concatenate((jpeg_labels, bcf_labels), axis=0)\n\n# 2) Count frequencies and find the two rarest labels\ncounts     = np.bincount(all_labels)\npresent    = np.nonzero(counts)[0]\nrarest_two = present[np.argsort(counts[present])[:2]]\nprint(\"Dropping labels:\", rarest_two.tolist())\n\n# 3) Make one boolean mask over all samples\nmask_keep  = (all_labels != rarest_two[0]) & (all_labels != rarest_two[1])\n\n# 4) Get the indices to keep (this is fast C‐side)\nkeep_idx   = np.nonzero(mask_keep)[0]\n\n# 5) Wrap in a Subset without iterating the dataset\nfiltered = Subset(ds, keep_idx.tolist())\n\n# 6) If you need the remaining unique labels, compute them via numpy\nremaining_labels = present[~np.isin(present, rarest_two)]\nprint(f\"Kept {len(filtered)} samples. Remaining labels ({len(remaining_labels)}): {remaining_labels}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(combined_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# synthetic_dataset = load_dataset(\"/kaggle/input/font-datasets/synthetic_dataset.pkl\")\nfiltered = synthetic_dataset\nunique_labels = np.unique(filtered.labels)\n\nprint(f\"Number of unique labels: {unique_labels.size}\")\nprint(f\"Labels: {unique_labels}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:56:02.401877Z","iopub.execute_input":"2025-05-20T03:56:02.402692Z","iopub.status.idle":"2025-05-20T03:56:02.408573Z","shell.execute_reply.started":"2025-05-20T03:56:02.402662Z","shell.execute_reply":"2025-05-20T03:56:02.407811Z"}},"outputs":[{"name":"stdout","text":"Number of unique labels: 200\nLabels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n 198 199]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# SCAE","metadata":{}},{"cell_type":"code","source":"# train_memory_efficient_model\n\nimport torch.cuda.amp as amp\nimport gc\nimport torch\nimport torch.nn as nn\nimport os\nimport gc\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\n\ndef train_memory_efficient_model(model, train_loader, val_loader=None, \n                                num_epochs=5, learning_rate=0.0001,\n                                checkpoint_dir=\"/kaggle/working/\"):\n    \"\"\"\n    Memory-efficient training function for SCAE model.\n    \"\"\"\n    # Ensure checkpoint directory exists\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Setup device and optimization tools\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Training on {device} with {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n    print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n    print(f\"Memory reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Set up mixed precision training\n    scaler = amp.GradScaler()\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n    \n    # Track best model\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    try:\n        for epoch in range(num_epochs):\n            # Clean memory before each epoch\n            gc.collect()\n            torch.cuda.empty_cache()\n            \n            # TRAINING PHASE\n            model.train()\n            running_loss = 0.0\n            valid_batches = 0\n            \n            # Use tqdm for progress tracking\n            pbar = tqdm(train_loader)\n            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n            \n            for batch_idx, (patches, _) in enumerate(pbar):\n                # Skip empty batches\n                if patches.numel() == 0:\n                    continue\n                \n                # Move data to device\n                patches = patches.to(device, non_blocking=True)\n                \n                # Mixed precision forward pass\n                with amp.autocast():\n                    outputs = model(patches)\n                    loss = criterion(outputs, patches)\n                \n                # Backward pass with gradient scaling\n                optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                \n                # Update metrics\n                running_loss += loss.item()\n                valid_batches += 1\n                \n                # Update progress bar\n                pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n                \n                # Aggressive memory cleanup every few batches\n                if batch_idx % 10 == 0:\n                    del outputs, loss, patches\n                    gc.collect()\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n            \n            # Calculate epoch metrics\n            if valid_batches > 0:\n                train_loss = running_loss / valid_batches\n                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}\")\n            else:\n                print(f\"Epoch {epoch+1}/{num_epochs}, No valid batches!\")\n                continue\n                \n            # Save checkpoint every epoch\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': train_loss,\n            }, f\"{checkpoint_dir}/model_epoch_{epoch+1}.pt\")\n            \n            # VALIDATION PHASE\n            if val_loader:\n                val_loss = validate_memory_efficient(model, val_loader, criterion, device)\n                scheduler.step(val_loss)\n                \n                # Early stopping logic\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    patience_counter = 0\n                    torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pt\")\n                    print(f\"New best model saved with val_loss: {val_loss:.6f}\")\n                else:\n                    patience_counter += 1\n                    if patience_counter >= 3:  # Adjust patience as needed\n                        print(\"Early stopping triggered!\")\n                        break\n    \n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        # Save emergency checkpoint\n        torch.save(model.state_dict(), f\"{checkpoint_dir}/emergency_model.pt\")\n        raise\n        \n    return model\n\ndef validate_memory_efficient(model, val_loader, criterion, device):\n    \"\"\"Memory-efficient validation function.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    valid_batches = 0\n    \n    with torch.no_grad():\n        pbar = tqdm(val_loader)\n        pbar.set_description(f\"Validating\")\n        \n        for patches, _ in pbar:\n            if patches.numel() == 0:\n                continue\n                \n            patches = patches.to(device, non_blocking=True)\n            \n            # Using mixed precision even for validation\n            with amp.autocast():\n                outputs = model(patches)\n                loss = criterion(outputs, patches)\n                \n            running_loss += loss.item()\n            valid_batches += 1\n            \n            # Update progress bar\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n            \n            # Clean up\n            del outputs, patches, loss\n    \n    if valid_batches > 0:\n        val_loss = running_loss / valid_batches\n        print(f\"Validation Loss: {val_loss:.6f}\")\n        return val_loss\n    else:\n        print(\"No valid validation batches!\")\n        return float('inf')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SCAE\nimport torch.nn as nn\nclass SCAE(nn.Module):\n    def __init__(self, normalization_type=\"batch_norm\", use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n        super(SCAE, self).__init__()\n\n        def norm_layer(num_features):\n            if normalization_type == \"batch_norm\":\n                return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\":\n                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\":\n                return nn.LayerNorm([num_features, 12, 12])  # Updated for 12x12 feature maps\n            else:\n                return nn.Identity()\n\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n\n        # Encoder: Input 105x105 -> Output 12x12\n        self.encoder = nn.Sequential(\n            # Layer 1: 105x105 -> 48x48\n            nn.Conv2d(1, 64, kernel_size=11, stride=2, padding=0),\n            norm_layer(64),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 2: 48x48 -> 24x24\n            nn.MaxPool2d(2, 2),\n            \n            # Layer 3: 24x24 -> 24x24\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            norm_layer(128),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 4: 24x24 -> 12x12 (added to get 12x12 output)\n            nn.MaxPool2d(2, 2)\n        )\n        \n        # Decoder: Input 12x12 -> Output 105x105\n        self.decoder = nn.Sequential(\n            # Layer 1: 12x12 -> 24x24\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n            norm_layer(64),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 2: 24x24 -> 48x48\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n            norm_layer(32),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 3: 48x48 -> 105x105 (with precise output size)\n            nn.ConvTranspose2d(32, 1, kernel_size=14, stride=2, padding=2, output_padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Pass through encoder\n        if x.size(1) == 3:\n            # Use standard RGB to grayscale conversion: 0.299*R + 0.587*G + 0.114*B\n            x = 0.299 * x[:, 0:1] + 0.587 * x[:, 1:2] + 0.114 * x[:, 2:3]\n        for layer in self.encoder:\n            x = layer(x)\n            # print(x.shape)\n\n        for layer in self.decoder:\n            x = layer(x)\n            # print(x.shape)\n            \n        return x","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-20T03:57:53.112188Z","iopub.execute_input":"2025-05-20T03:57:53.112524Z","iopub.status.idle":"2025-05-20T03:57:53.122782Z","shell.execute_reply.started":"2025-05-20T03:57:53.112499Z","shell.execute_reply":"2025-05-20T03:57:53.122139Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"del model, optimizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create model and train with memory optimization\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SCAE().to(device)\ntrained_model = train_memory_efficient_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=5,\n    learning_rate=0.0001\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load checkpoints\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 1) Re-instantiate your model\npretrained_scae = SCAE().to(device)\n\n# 2) Load the checkpoint dict\nckpt = torch.load(\"/kaggle/working/model_epoch_5.pt\", weights_only=True)\n\n# 3) Pull out and load the actual weights\npretrained_scae.load_state_dict(ckpt[\"model_state_dict\"])\n\n# 4) (Optional) if you saved epoch or optimizer state too:\n# start_epoch = ckpt[\"epoch\"] + 1\n# optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n\npretrained_scae.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:57:57.606689Z","iopub.execute_input":"2025-05-20T03:57:57.60698Z","iopub.status.idle":"2025-05-20T03:57:57.746229Z","shell.execute_reply.started":"2025-05-20T03:57:57.606959Z","shell.execute_reply":"2025-05-20T03:57:57.745412Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"SCAE(\n  (encoder): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(2, 2))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Identity()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): ReLU(inplace=True)\n    (8): Identity()\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (decoder): Sequential(\n    (0): Upsample(scale_factor=2.0, mode='nearest')\n    (1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU(inplace=True)\n    (4): Identity()\n    (5): Upsample(scale_factor=2.0, mode='nearest')\n    (6): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): Identity()\n    (10): ConvTranspose2d(32, 1, kernel_size=(14, 14), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n    (11): Sigmoid()\n  )\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"evaluate_reconstruction(model, val_loader, device, num_samples=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_latent_space(model, val_loader, device, max_samples=1000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluation\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n\n# ——————————————————————————————————————————————\n# 1) Reconstruction evaluation\n# ——————————————————————————————————————————————\n\ndef evaluate_reconstruction(model, dataloader, device, num_samples=10, save_path=None):\n    \"\"\"\n    Compute MSE, SSIM, PSNR between inputs and auto‐encoder reconstructions.\n    Also visualize num_samples side‐by‐side.\n    Expects dataloader that yields (imgs, _), where imgs is (B,1,H,W) in [0,1].\n    \"\"\"\n    model.eval()\n    total_mse = total_ssim = total_psnr = 0.0\n    seen = 0\n    vis_in, vis_re = [], []\n\n    with torch.no_grad():\n        for imgs, _ in dataloader:\n            imgs = imgs.to(device)                  # (B,1,105,105)\n            recons = model(imgs)                    # (B,1,105,105)\n\n            # batch MSE\n            mse_batch = torch.mean((recons - imgs) ** 2).item()\n            total_mse += mse_batch * imgs.size(0)\n\n            # to numpy [0,1]\n            inp_np  = imgs.cpu().squeeze(1).numpy()\n            rec_np  = recons.cpu().squeeze(1).numpy()\n            B = inp_np.shape[0]\n\n            for i in range(B):\n                if seen >= num_samples: break\n\n                im = inp_np[i]\n                rc = rec_np[i]\n                total_ssim += ssim(im, rc, data_range=1.0)\n                total_psnr += psnr(im, rc, data_range=1.0)\n\n                vis_in .append(imgs [i].cpu())\n                vis_re .append(recons[i].cpu())\n                seen += 1\n\n            if seen >= num_samples:\n                break\n\n    N = len(dataloader.dataset) if hasattr(dataloader.dataset, \"__len__\") else seen\n    avg_mse  = total_mse / N\n    avg_ssim = total_ssim / seen\n    avg_psnr = total_psnr / seen\n\n    print(f\"Reconstruction → MSE: {avg_mse:.4f}, SSIM: {avg_ssim:.4f}, PSNR: {avg_psnr:.2f} dB\")\n\n    if vis_in:\n        # interleave originals and reconstructions\n        grid = make_grid([*vis_in[:seen], *vis_re[:seen]],\n                         nrow= seen,\n                         normalize=True, pad_value=1)\n        plt.figure(figsize=(seen*2, 4))\n        plt.imshow(grid.permute(1,2,0).numpy())\n        plt.axis('off')\n        plt.title(\"Originals (top) vs Reconstructions (bottom)\")\n        if save_path:\n            plt.savefig(f\"{save_path}/recon.png\", bbox_inches=\"tight\")\n        plt.show()\n\n    return {\"mse\":avg_mse, \"ssim\":avg_ssim, \"psnr\":avg_psnr}\n\n\n# ——————————————————————————————————————————————\n# 2) Latent extraction & generation\n# ——————————————————————————————————————————————\n\ndef extract_latent_features(model, x):\n    \"\"\"\n    Runs x through the encoder only.\n    x: (B,1,105,105)\n    returns: z (B,128,12,12)\n    \"\"\"\n    return model.encoder(x)\n\ndef generate_from_latent(model, z):\n    \"\"\"\n    Runs z through the decoder only.\n    z: (B,128,12,12)\n    returns: recon (B,1,105,105)\n    \"\"\"\n    return model.decoder(z)\n\n\n# ——————————————————————————————————————————————\n# 3) t-SNE visualization of latent space\n# ——————————————————————————————————————————————\n\ndef visualize_latent_space(model, dataloader, device, max_samples=500, save_path=None):\n    \"\"\"\n    Collects up to max_samples latent vectors, runs t-SNE, and plots.\n    Expects dataloader yielding (imgs, labels) with imgs in [0,1].\n    \"\"\"\n    from sklearn.manifold import TSNE\n\n    model.eval()\n    zs, ys = [], []\n    with torch.no_grad():\n        for imgs, labels in dataloader:\n            if len(zs) >= max_samples: break\n            imgs = imgs.to(device)\n            z = extract_latent_features(model, imgs)         # (B,128,12,12)\n            zflat = z.view(z.size(0), -1).cpu().numpy()      # (B, 128*12*12)\n            zs.append(zflat)\n            ys.append(labels.numpy())\n        zs = np.vstack(zs)[:max_samples]\n        ys = np.concatenate(ys)[:max_samples]\n\n    tsne = TSNE(n_components=2, random_state=42)\n    Z2 = tsne.fit_transform(zs)\n\n    plt.figure(figsize=(8,6))\n    plt.scatter(Z2[:,0], Z2[:,1], c=ys, cmap=\"tab10\", s=5, alpha=0.7)\n    plt.colorbar(label=\"Class\")\n    plt.title(\"t-SNE of SCAE Latent Space\")\n    if save_path:\n        plt.savefig(f\"{save_path}/tsne.png\", bbox_inches=\"tight\")\n    plt.show()\n\n    return Z2, ys\n\n\n# ——————————————————————————————————————————————\n# 4) Latent interpolation\n# ——————————————————————————————————————————————\n\ndef interpolate_latent_space(model, dataloader, device, steps=10, save_path=None):\n    \"\"\"\n    Linearly interpolate between two latent codes and decode them.\n    \"\"\"\n    model.eval()\n    imgs = None\n    # grab first two distinct samples\n    with torch.no_grad():\n        for x, _ in dataloader:\n            if x.size(0) >= 2:\n                imgs = x[:2].to(device)\n                break\n    if imgs is None:\n        print(\"Not enough samples for interpolation\"); return\n\n    z1 = extract_latent_features(model, imgs[0:1])   # (1,128,12,12)\n    z2 = extract_latent_features(model, imgs[1:2])\n\n    interps = []\n    alphas = np.linspace(0,1,steps)\n    for a in alphas:\n        z = a*z1 + (1-a)*z2\n        recon = generate_from_latent(model, z)        # (1,1,105,105)\n        interps.append(recon.cpu())\n\n    # build grid: [orig1, *interps, orig2]\n    all_ = torch.cat([imgs[0:1].cpu(), *interps, imgs[1:2].cpu()], dim=0)\n    grid = make_grid(all_, nrow=steps+2, normalize=True, pad_value=1)\n\n    plt.figure(figsize=(12,3))\n    plt.imshow(grid.permute(1,2,0).numpy())\n    plt.axis(\"off\")\n    plt.title(\"Latent Interpolation\")\n    if save_path:\n        plt.savefig(f\"{save_path}/interp.png\", bbox_inches=\"tight\")\n    plt.show()\n\n    return interps\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/evaluation_results.zip /kaggle/working/evaluation_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Font classifier","metadata":{"id":"1hbTCU2qndht"}},{"cell_type":"code","source":"# FontClassifier\nclass FontClassifier(nn.Module):\n    def __init__(self, pretrained_scae, num_classes=200, normalization_type=\"batch_norm\", \n                 use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n        super().__init__()\n        self.pretrained_scae = pretrained_scae  # Use pretrained SCAE encoder\n        \n        # Define helper functions for creating layers\n        def norm_layer(num_features, spatial_size=None):\n            if normalization_type == \"batch_norm\":\n                return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\":\n                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\" and spatial_size is not None:\n                return nn.LayerNorm([num_features, spatial_size, spatial_size])\n            else:\n                return nn.Identity()\n\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n        \n        # CNN head after the SCAE encoder\n        # SCAE encoder output is 128 x 26 x 26\n        self.cnn_head = nn.Sequential(\n            # Conv layer 4\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Out: 256 x 12 x 12\n            norm_layer(256, 12),\n            activation_layer(),\n            \n            # Conv layer 5\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Out: 256 x 12 x 12\n            norm_layer(256, 13),\n            activation_layer(),\n            dropout_layer()\n        )\n        \n        # Flatten layer\n        self.flatten = nn.Flatten()\n        \n        # Fully connected layers\n        # Input size is 256 * 12 * 12 = 43,264\n        self.fully_connected = nn.Sequential(\n            nn.Linear(256 * 12 * 12, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_prob if use_dropout else 0),\n            \n            nn.Linear(4096, 2048),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_prob if use_dropout else 0),\n            \n            nn.Linear(2048, num_classes),\n            # nn.Softmax(dim=1) no softmax here bro, crossentropy does the softmax automatically\n        )\n\n    def forward(self, x):\n        # Use the encoder part of SCAE\n        x = self.pretrained_scae.encoder(x)\n        # Continue with additional CNN layers\n        x = self.cnn_head(x)\n        \n        # Flatten and apply fully connected layers\n        x = self.flatten(x)\n        x = self.fully_connected(x)\n        \n        return x\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:58:36.393484Z","iopub.execute_input":"2025-05-20T03:58:36.394143Z","iopub.status.idle":"2025-05-20T03:58:36.403376Z","shell.execute_reply.started":"2025-05-20T03:58:36.394111Z","shell.execute_reply":"2025-05-20T03:58:36.402769Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"classifier = FontClassifier(pretrained_scae, num_classes=200).to(device)\n\nfor batch in train_loader:\n    print(batch[0].shape)\n    print(classifier(batch[0].to(device)).shape)\n    # show_images_in_grid(batch.permute(0, 2, 3, 1).numpy(), titles=[f'Patch {i+1}' for i in range(len(batch))], cols=4)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:58:41.402367Z","iopub.execute_input":"2025-05-20T03:58:41.403296Z","iopub.status.idle":"2025-05-20T03:58:46.67149Z","shell.execute_reply.started":"2025-05-20T03:58:41.403268Z","shell.execute_reply":"2025-05-20T03:58:46.670756Z"}},"outputs":[{"name":"stdout","text":"torch.Size([384, 1, 105, 105])\ntorch.Size([384, 200])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# train\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nimport os\n\ndef train(\n    model,\n    train_loader,\n    val_loader,\n    optimizer,\n    criterion,\n    scheduler=None,\n    device=\"cuda\",\n    num_epochs=5,\n    early_stopping_patience=None,\n    checkpoint_path=None,\n    use_amp=False\n):\n    \"\"\"\n    Generic training loop for any (batch_size,1,105,105) ➔ (batch_size,num_classes) model.\n    \n    Args:\n        model:         nn.Module that maps inputs ➔ logits\n        train_loader:  DataLoader for training\n        val_loader:    DataLoader for validation\n        optimizer:     torch.optim.Optimizer\n        criterion:     loss function (e.g. nn.CrossEntropyLoss())\n        scheduler:     learning-rate scheduler (optional)\n        device:        'cuda' or 'cpu'\n        num_epochs:    number of epochs\n        early_stopping_patience: stop if no val-loss improvement for this many epochs (optional)\n        checkpoint_path: path to save best model state_dict (optional)\n        use_amp:       whether to use mixed precision (bool)\n    \n    Returns:\n        model:         best model (weights restored from checkpoint if provided)\n        history:       dict with lists: train_loss, train_acc, val_loss, val_acc\n    \"\"\"\n    model = model.to(device)\n    scaler = GradScaler() if use_amp and device != \"cpu\" else None\n\n    best_val_loss = float(\"inf\")\n    epochs_no_improve = 0\n\n    history = {\n        \"train_loss\": [], \"train_acc\": [],\n        \"val_loss\":   [], \"val_acc\":   []\n    }\n\n    for epoch in range(1, num_epochs+1):\n        # ——— Training ————————————————————————————————\n        model.train()\n        running_loss = 0.0\n        running_correct = 0\n        running_total = 0\n\n        pbar = tqdm(train_loader, desc=f\"[Epoch {epoch}/{num_epochs}] Train\", leave=False)\n        for inputs, labels in pbar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            if use_amp:\n                with autocast():\n                    logits = model(inputs)\n                    loss = criterion(logits, labels)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                logits = model(inputs)\n                loss = criterion(logits, labels)\n                loss.backward()\n                optimizer.step()\n\n            # metrics\n            running_loss += loss.item() * labels.size(0)\n            preds = logits.argmax(dim=1)\n            running_correct += (preds == labels).sum().item()\n            running_total   += labels.size(0)\n            pbar.set_postfix(loss=running_loss/running_total, acc=100.*running_correct/running_total)\n\n        train_loss = running_loss / running_total\n        train_acc  = 100. * running_correct / running_total\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n\n        # ——— Validation ———————————————————————————————\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total   = 0\n\n        with torch.no_grad():\n            pbar = tqdm(val_loader, desc=f\"[Epoch {epoch}/{num_epochs}] Val  \", leave=False)\n            for inputs, labels in pbar:\n                inputs, labels = inputs.to(device), labels.to(device)\n                if use_amp:\n                    with autocast():\n                        logits = model(inputs)\n                        loss = criterion(logits, labels)\n                else:\n                    logits = model(inputs)\n                    loss = criterion(logits, labels)\n\n                val_loss += loss.item() * labels.size(0)\n                preds = logits.argmax(dim=1)\n                val_correct += (preds == labels).sum().item()\n                val_total   += labels.size(0)\n                pbar.set_postfix(val_loss=val_loss/val_total, val_acc=100.*val_correct/val_total)\n\n        val_loss_epoch = val_loss / val_total\n        val_acc_epoch  = 100. * val_correct / val_total\n        history[\"val_loss\"].append(val_loss_epoch)\n        history[\"val_acc\"].append(val_acc_epoch)\n\n        # ——— Scheduler step ————————————————————————————\n        if scheduler is not None:\n            # if ReduceLROnPlateau, pass val_loss\n            if hasattr(scheduler, \"step\") and scheduler.__class__.__name__ == \"ReduceLROnPlateau\":\n                scheduler.step(val_loss_epoch)\n            else:\n                scheduler.step()\n\n        # ——— Checkpoint & Early Stopping —————————————————————\n        if checkpoint_path is not None and val_loss_epoch < best_val_loss:\n            best_val_loss = val_loss_epoch\n            torch.save(model.state_dict(), checkpoint_path)\n            epochs_no_improve = 0\n            print(f\"→ New best model saved (val_loss={best_val_loss:.4f})\")\n        elif early_stopping_patience is not None:\n            epochs_no_improve += 1\n            if epochs_no_improve >= early_stopping_patience:\n                print(f\"→ Early stopping after {epoch} epochs without improvement.\")\n                break\n\n        # summary print\n        print(\n            f\"Epoch {epoch}/{num_epochs} \"\n            f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n            f\"Val: loss={val_loss_epoch:.4f}, acc={val_acc_epoch:.2f}%\"\n        )\n\n    # reload best weights if checkpoint was used\n    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    return model, history\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-20T03:58:47.817826Z","iopub.execute_input":"2025-05-20T03:58:47.818487Z","iopub.status.idle":"2025-05-20T03:58:47.892021Z","shell.execute_reply.started":"2025-05-20T03:58:47.818456Z","shell.execute_reply":"2025-05-20T03:58:47.891368Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Evaluation\ndef evaluate(model, test_loader, device='cuda', use_amp=False):\n    \"\"\"\n    Testing-phase: aggregates 15 patches per sample, computes loss + top1/top5 metrics.\n    Returns (top1_error, top5_error).\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    model.eval()\n    total_loss = 0.0\n    total_samples = 0\n    correct1 = 0\n    correct5 = 0\n\n    test_pbar = tqdm(\n        total=len(test_loader),\n        desc=\"Testing\",\n        unit=\"batch\",\n        leave=True,\n        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n    )\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            # inputs: [B, P, C, H, W]\n            B, P, C, H, W = inputs.shape\n            inputs = inputs.view(B*P, C, H, W).to(device)\n            labels = labels.to(device)\n\n            # Forward all patches\n            if use_amp and device != 'cpu':\n                with autocast():\n                    logits = model(inputs)\n            else:\n                logits = model(inputs)\n\n            # Reshape + average over patches → [B, num_classes]\n            num_classes = logits.size(1)\n            avg_logits = logits.view(B, P, num_classes).mean(dim=1)\n\n            # Compute loss on averaged logits\n            loss = criterion(avg_logits, labels)\n            total_loss += loss.item() * B\n\n            # Top-1\n            _, pred1 = avg_logits.max(1)\n            correct1 += pred1.eq(labels).sum().item()\n\n            # Top-5\n            _, pred5 = avg_logits.topk(5, dim=1, largest=True, sorted=True)\n            correct5 += (pred5 == labels.view(-1, 1)).any(dim=1).sum().item()\n\n            total_samples += B\n\n            test_pbar.set_postfix({\n                'loss':    f\"{loss.item():.4f}\",\n                'top1_acc': f\"{100.*correct1/total_samples:.2f}%\",\n                'top5_acc': f\"{100.*correct5/total_samples:.2f}%\"\n            })\n            test_pbar.update()\n\n    test_pbar.close()\n\n    avg_loss = total_loss / total_samples\n    top1_acc = 100. * correct1 / total_samples\n    top5_acc = 100. * correct5 / total_samples\n\n    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n    print(f\"Top-1 Accuracy: {top1_acc:.2f}% | Top-1 Error: {100.-top1_acc:.2f}%\")\n    print(f\"Top-5 Accuracy: {top5_acc:.2f}% | Top-5 Error: {100.-top5_acc:.2f}%\")\n\n    return 100. - top1_acc, 100. - top5_acc\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- setup outside ----\nmodel     = FontClassifier(pretrained_scae, num_classes=200).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = torch.nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n\n# ---- call train() ----\nbest_model, history = train(\n    model,\n    train_loader,\n    val_loader,\n    optimizer,\n    criterion,\n    scheduler=scheduler,\n    device='cuda',\n    num_epochs=5,\n    early_stopping_patience=7,\n    checkpoint_path=\"/kaggle/working/classifier_checkpoints/best_font_model.pt\",\n    use_amp=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:59:30.859999Z","iopub.execute_input":"2025-05-20T03:59:30.860335Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"[Epoch 1/5] Train:   0%|          | 0/1250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91ce7588420f4475bc5cfdcebc8d6133"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/classifier_checkpoints","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## HENet","metadata":{}},{"cell_type":"code","source":"top1_error, top5_error = evaluate_model_optimized(\n    trained_model, \n    test_font_loader, \n    device=device,\n    use_amp=True\n)\n\nprint(f\"Final Results:\")\nprint(f\"Top-1 Error: {top1_error:.2f}%\")\nprint(f\"Top-5 Error: {top5_error:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HEBlock + HENet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport numpy as np\n\nclass HEBlock(nn.Module):\n    \"\"\"\n    Optimized HE (Hide and Enhance) Block implementation.\n    Vectorized implementation to eliminate slow Python loops.\n    \"\"\"\n    def __init__(self, beta=0.5):\n        \"\"\"\n        Args:\n            beta: weight of mask (default: 0.5 as recommended in the paper)\n        \"\"\"\n        super(HEBlock, self).__init__()\n        self.beta = beta\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: input feature map of shape (batch_size, C, H, W)\n        Returns:\n            Modified feature map with suppressed maximum activations\n        \"\"\"\n        if not self.training:  # Only apply during training\n            return x\n        \n        # Get shape information\n        batch_size, channels, h, w = x.size()\n        \n        # Find maximum values for each channel in each sample in batch\n        # Shape: [batch_size, channels, 1, 1]\n        max_vals = x.view(batch_size, channels, -1).max(dim=2)[0].view(batch_size, channels, 1, 1)\n        \n        # Create masks where the value equals the max value\n        # Broadcasting handles the comparison efficiently\n        mask = (x == max_vals).float()\n        \n        # Apply the beta factor to maximum values using the mask\n        # This is a vectorized operation that replaces the nested loops\n        output = torch.where(mask == 1, self.beta * x, x)\n        \n        return output\n\n\nclass HENet(nn.Module):\n    \"\"\"\n    Optimized HENet implementation for font recognition.\n    \"\"\"\n    def __init__(self, num_classes=2383, beta=0.5, use_amp=True):\n        \"\"\"\n        Args:\n            num_classes: Number of font classes (default: 2383)\n            beta: Weight for the HE Block mask (default: 0.5)\n            use_amp: Whether to use Automatic Mixed Precision (default: True)\n        \"\"\"\n        super(HENet, self).__init__()\n        \n        # Track whether to use mixed precision\n        self.use_amp = use_amp\n        \n        # Load pretrained ResNet18 - efficient backbone architecture\n        self.resnet = models.resnet18(pretrained=True)\n        \n        # Modify the first convolutional layer to accept grayscale input\n        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Remove the final fully connected layer to use as feature extractor\n        self.features = nn.Sequential(*list(self.resnet.children())[:-2])\n        \n        # 1x1 convolution to match the number of classes (more efficient than FC for large number of classes)\n        self.conv_final = nn.Conv2d(512, num_classes, kernel_size=1)\n        \n        # Optimized HE Block\n        self.he_block = HEBlock(beta=beta)\n        \n        # Global average pooling for efficient dimensionality reduction\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n    \n    def forward(self, x):\n        # Use AMP if specified (faster computation with minimal accuracy loss)\n        with torch.cuda.amp.autocast() if self.use_amp and torch.cuda.is_available() else torch.no_grad():\n            # Feature extraction using ResNet backbone\n            x = self.features(x)\n            \n            # 1x1 convolution to get class-specific activation maps\n            x = self.conv_final(x)\n            \n            # Apply HE Block during training (now optimized)\n            x = self.he_block(x)\n            \n            # Global average pooling and flatten\n            x = self.avgpool(x)\n            x = torch.flatten(x, 1)\n            \n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# optimized_train_eval.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm.auto import tqdm\nimport time\nimport os\nfrom datetime import timedelta\nimport numpy as np\nimport math\n\ndef train_model_optimized(model, train_loader, val_loader, num_epochs=100, \n                          device='cuda', use_amp=True, use_compile=False,\n                          gradient_accumulation_steps=4, save_dir='checkpoints'):\n    \"\"\"\n    Optimized training function with support for:\n    - Automatic Mixed Precision (AMP)\n    - Gradient accumulation\n    - Detailed monitoring\n    - Model checkpointing\n    - Compatibility with older GPUs\n    \"\"\"\n    # Create directory for checkpoints\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Check GPU compatibility for torch.compile\n    if use_compile and hasattr(torch, 'compile'):\n        # Only enable on supported hardware (CUDA capability >= 7.0)\n        if torch.cuda.is_available():\n            device_cap = torch.cuda.get_device_capability(torch.cuda.current_device())\n            if device_cap[0] < 7:\n                use_compile = False\n                print(f\"GPU CUDA capability {device_cap[0]}.{device_cap[1]} is too old for torch.compile(). Disabling.\")\n            else:\n                model = torch.compile(model)\n                print(\"Using torch.compile() to optimize model execution\")\n    \n    # Loss function, optimizer and scheduler\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=0.01,             # base learning rate from the paper\n        momentum=0.9,        # typical momentum\n        weight_decay=5e-4    # small L2 regularization\n    )\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=0.1,          \n        total_steps=math.ceil(len(train_loader) / gradient_accumulation_steps) * num_epochs,  # Use math.ceil instead of //\n        pct_start=0.2,       \n        anneal_strategy='cos',\n        div_factor=10,       \n        final_div_factor=1e4 \n    )\n    \n    # Initialize AMP scaler if using AMP\n    scaler = GradScaler() if use_amp and device != 'cpu' else None\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # For tracking best model\n    best_val_error = float('inf')\n    best_model_state = None\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        epoch_start = time.time()\n        \n        # === TRAINING PHASE ===\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Progress bar for training\n        train_pbar = tqdm(\n            total=len(train_loader),\n            desc=f\"Training\",\n            unit=\"batch\",\n            leave=True,\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n        )\n        \n        # Track batch-level metrics\n        batch_times = []\n        \n        # Zero gradients at the start of epoch\n        optimizer.zero_grad()\n        \n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            batch_start = time.time()\n            \n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass with AMP if enabled\n            step_count = 0\n            total_steps = math.ceil(len(train_loader) / gradient_accumulation_steps) * num_epochs\n            if use_amp and device != 'cpu':\n                with autocast():\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    # Scale loss by gradient accumulation steps\n                    loss = loss / gradient_accumulation_steps\n                \n                # Backward pass with AMP\n                scaler.scale(loss).backward()\n                \n                # Step optimizer every gradient_accumulation_steps\n                if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n                    # Unscale gradients for proper gradient clipping\n                    scaler.unscale_(optimizer)\n                    \n                    # Clip gradients to prevent exploding gradients\n                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    \n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                    if step_count < total_steps:\n                        scheduler.step()\n                        step_count += 1\n            else:\n                # Standard forward pass\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                # Scale loss by gradient accumulation steps\n                loss = loss / gradient_accumulation_steps\n                \n                # Standard backward pass\n                loss.backward()\n                \n                # Step optimizer every gradient_accumulation_steps\n                if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n                    # Clip gradients to prevent exploding gradients\n                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    \n                    optimizer.step()\n                    optimizer.zero_grad()\n                    if step_count < total_steps:\n                        scheduler.step()\n                        step_count += 1\n            \n            # Calculate batch statistics\n            batch_loss = loss.item() * gradient_accumulation_steps\n            running_loss += batch_loss\n            _, predicted = outputs.max(1)\n            batch_correct = predicted.eq(labels).sum().item()\n            batch_size = labels.size(0)\n            total += batch_size\n            correct += batch_correct\n            \n            # Measure batch time\n            batch_end = time.time()\n            batch_time = batch_end - batch_start\n            batch_times.append(batch_time)\n            \n            # Update progress bar with detailed metrics\n            batch_acc = 100. * batch_correct / batch_size\n            current_lr = optimizer.param_groups[0]['lr']\n            \n            train_pbar.set_postfix({\n                'loss': f\"{batch_loss:.4f}\",\n                'acc': f\"{batch_acc:.2f}%\",\n                'lr': f\"{current_lr:.6f}\",\n                'time': f\"{batch_time:.3f}s\"\n            })\n            train_pbar.update()\n        torch.save(model.state_dict(), f'/kaggle/working/checkpoints/henet_final_model_epoch_{epoch+1}.pt')\n        \n        train_pbar.close()\n        \n        # Calculate training statistics\n        train_loss = running_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        train_error = 100. - train_acc\n        avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n        \n        # === VALIDATION PHASE ===\n        val_metrics = validate_model(model, val_loader, criterion, device, use_amp)\n        val_loss = val_metrics['val_loss']\n        \n        # Print epoch summary (now only loss)\n        epoch_time = time.time() - epoch_start\n\n        # Print epoch summary\n        print(f\"Epoch {epoch+1}/{num_epochs} completed in {timedelta(seconds=int(epoch_time))}\")\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Train Error: {train_error:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f}\")\n        \n        # Save best model\n        if val_loss < best_val_error:\n             best_val_error = val_loss\n             best_model_state = {\n                 'epoch': epoch + 1,\n                 'model_state_dict': model.state_dict(),\n                 'optimizer_state_dict': optimizer.state_dict(),\n                 'scheduler_state_dict': scheduler.state_dict(),\n                 'val_loss': val_loss,\n                 'train_error': train_error,\n             }\n             save_path = os.path.join(save_dir, f\"best_model_epoch{epoch+1}_val{val_loss:.4f}.pt\")\n        \n        # Free up memory\n        torch.cuda.empty_cache()\n    \n    # Restore best model if available\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state['model_state_dict'])\n        print(f\"Restored best model with validation error: {best_val_error:.2f}%\")\n    \n    return model\n\ndef validate_model(model, val_loader, criterion, device, use_amp=False):\n    \"\"\"\n    Evaluation-phase (during training): only average loss.\n    Returns: {'val_loss': float}\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_samples = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n            # inputs: [B, P, C, H, W] or [B, C, H, W]\n            B = labels.size(0)\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # flatten patches if needed\n            if inputs.ndim == 5:\n                B, P, C, H, W = inputs.shape\n                inputs = inputs.view(B*P, C, H, W)\n\n            # forward\n            if use_amp and device != 'cpu':\n                with autocast():\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels.repeat_interleave(inputs.size(0)//B))\n            else:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels.repeat_interleave(inputs.size(0)//B))\n\n            total_loss += loss.item() * B\n            total_samples += B\n\n    return {'val_loss': total_loss / total_samples}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# main.py\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport os\nimport gc\n\n# Set device and enable deterministic mode for reproducibility\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Print GPU info\nif torch.cuda.is_available():\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    device_cap = torch.cuda.get_device_capability(0)\n    print(f\"CUDA Capability: {device_cap[0]}.{device_cap[1]}\")\n    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n\n# Clean up memory before starting\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Hyperparameters\nnum_classes = 2383\nbatch_size = 64  # Base batch size\nnum_epochs = 10\nbeta = 0.5  # HE Block mask weight\n\n# Create the model with optimized HEBlock\nmodel = HENet(num_classes=num_classes, beta=beta)\n\n# Configure DataLoader with optimal settings\ntrain_loader = train_font_loader\nval_loader = val_font_loader\ntest_loader = test_font_loader\n\n# Train the model with optimizations\ntrained_model = train_model_optimized(\n    model, \n    train_loader, \n    val_loader, \n    num_epochs=num_epochs, \n    device=device,\n    use_amp=True,          # Enable Mixed Precision\n    use_compile=False,     # Disable torch.compile for P100\n    gradient_accumulation_steps=2,  # Effective batch size = 64 * 4 = 256\n    save_dir='/kaggle/working/checkpoints'\n)\n\n# Save the final trained model\n# torch.save(trained_model.state_dict(), '/kaggle/working/checkpoints/henet_final_model.pt')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top1_error, top5_error = evaluate_model_optimized(\n    trained_model, \n    test_font_loader, \n    device=device,\n    use_amp=True\n)\n\nprint(f\"Final Results:\")\nprint(f\"Top-1 Error: {top1_error:.2f}%\")\nprint(f\"Top-5 Error: {top5_error:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}