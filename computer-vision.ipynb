{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Font_Detect_Updated v1.ipynb","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11356819,"sourceType":"datasetVersion","datasetId":7107437},{"sourceId":11786582,"sourceType":"datasetVersion","datasetId":7359793,"isSourceIdPinned":true},{"sourceId":399921,"sourceType":"modelInstanceVersion","modelInstanceId":327270,"modelId":348151}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ndannnop/computer-vision?scriptVersionId=240659003\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pybcf pysam keras-layer-normalization","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport random\nimport math # Needed for ceiling division\n    \nclass BCFImagePatchDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for loading either images from a BCF file or from a folder of JPEG files,\n    and extracting patches on the fly. Loads images as grayscale.\n    \"\"\"\n    def __init__(self, data_source, label_file=None, num_patch=3, patch_size=(105, 105)):\n        \"\"\"\n        Initializes the dataset. Can handle both BCF files and JPEG images.\n\n        Args:\n            data_source (str): Path to the BCF file or directory containing JPEG images.\n            label_file (str): Path to the label file (required only for BCF files).\n            num_patch (int): Number of patches to extract per image.\n            patch_size (tuple): (height, width) of patches.\n        \"\"\"\n        self.data_source = data_source\n        self.label_file = label_file\n        self.num_patch = num_patch\n        self.patch_size = patch_size\n\n        self.labels = None\n        self.image_filenames = []\n\n        # Determine whether the dataset is based on BCF or JPEG files\n        if data_source.endswith('.bcf'):\n            self._read_bcf_metadata()\n        else:\n            self._read_image_filenames()\n\n    def _read_bcf_metadata(self):\n        \"\"\"Reads labels and image size/offset information from BCF files.\"\"\"\n        try:\n            # Read label file\n            if not self.label_file:\n                raise ValueError(\"Label file is required for BCF data source.\")\n            with open(self.label_file, 'rb') as f:\n                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n                print(f\"Read {len(self.labels)} labels.\")\n\n            # Read BCF header\n            with open(self.data_source, 'rb') as f:\n                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n                print(f\"BCF header indicates {self.num_images} images.\")\n\n                if len(self.labels) != self.num_images:\n                    raise ValueError(f\"Mismatch between number of labels ({len(self.labels)}) and images in BCF header ({self.num_images}).\")\n\n                # Read all image sizes\n                sizes_bytes = f.read(self.num_images * 8)\n                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n                print(f\"Read {len(self.image_sizes)} image sizes.\")\n\n                # Calculate the starting offset of the actual image data blob\n                self.data_start_offset = 8 + self.num_images * 8\n                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n                print(\"Calculated image offsets.\")\n\n        except FileNotFoundError as e:\n            print(f\"Error: File not found - {e}\")\n            raise\n        except Exception as e:\n            print(f\"Error reading metadata: {e}\")\n            raise\n\n    def _read_image_filenames(self):\n        \"\"\"Reads image filenames from a folder (only for JPEG images).\"\"\"\n        try:\n            # List all JPEG images in the folder\n            self.image_filenames = [f for f in os.listdir(self.data_source) if f.endswith('.jpeg') or f.endswith('.jpg')]\n            print(f\"Found {len(self.image_filenames)} JPEG images.\")\n\n            if len(self.image_filenames) == 0:\n                raise ValueError(\"No JPEG images found in the specified folder.\")\n        except FileNotFoundError as e:\n            print(f\"Error: Folder not found - {e}\")\n            raise\n        except Exception as e:\n            print(f\"Error reading filenames: {e}\")\n            raise\n\n    def __len__(self):\n        \"\"\"Returns the total number of images in the dataset.\"\"\"\n        if hasattr(self, 'num_images'):\n            return self.num_images  # For BCF files\n        return len(self.image_filenames)  # For JPEG images\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Loads one image as grayscale, extracts patches, and returns patches with the label.\n    \n        Args:\n            idx (int): The index of the image to retrieve.\n    \n        Returns:\n            tuple: (list[np.ndarray], int): A tuple containing:\n                     - A list of NumPy arrays, each representing a patch (H, W).\n                     - The integer label for the image (or 0 for JPEG).\n                   Returns ([], -1) if image reading or patch extraction fails.\n        \"\"\"\n        if hasattr(self, 'num_images'):  # BCF source\n            if idx >= self.num_images or idx < 0:\n                raise IndexError(f\"Index {idx} out of bounds for {self.num_images} images.\")\n    \n            label = self.labels[idx]\n            offset = self.image_offsets[idx]\n            size = self.image_sizes[idx]\n    \n            try:\n                with open(self.data_source, 'rb') as f:\n                    f.seek(self.data_start_offset + offset)\n                    image_bytes = f.read(size)\n    \n                img = Image.open(BytesIO(image_bytes)).convert('L')\n                img_array = np.array(img)\n                patches = extract_patches(img_array, self.num_patch, self.patch_size)\n    \n                return patches, label\n    \n            except Exception as e:\n                print(f\"Error processing image index {idx}: {e}\")\n                return [], -1  # Indicate error\n    \n        else:  # JPEG source\n            if idx >= len(self.image_filenames) or idx < 0:\n                raise IndexError(f\"Index {idx} out of bounds for {len(self.image_filenames)} images.\")\n    \n            image_filename = self.image_filenames[idx]\n            image_path = os.path.join(self.data_source, image_filename)\n    \n            try:\n                img = Image.open(image_path).convert('L')\n                img_array = np.array(img)\n                patches = extract_patches(img_array, self.num_patch, self.patch_size)\n    \n                return patches, 0\n    \n            except Exception as e:\n                print(f\"Error processing image index {idx}: {e}\")\n                return [], -1  # Indicate error\n\ndef patch_collate_fn(batch, patch_size_tuple):\n    \"\"\"\n    Collates data from the BCFImagePatchDataset (handling grayscale) or JPEG-based dataset.\n\n    Takes a batch of [(patches_list_img1, label1), (patches_list_img2, label2), ...],\n    flattens the patches, converts them to a tensor, adds a channel dimension,\n    normalizes, and returns a single batch tensor for patches and labels.\n\n    Args:\n        batch (list): A list of tuples, where each tuple is the output\n                      of BCFImagePatchDataset.__getitem__.\n        patch_size_tuple (tuple): The (height, width) of patches, needed for empty tensor shape.\n\n    Returns:\n        tuple: (torch.Tensor, torch.Tensor): A tuple containing:\n                 - Patches tensor (BatchSize * NumPatches, 1, Height, Width)\n                 - Labels tensor (BatchSize * NumPatches)\n    \"\"\"\n    all_patches = []\n    all_labels = []\n    valid_batch_items = 0\n\n    for item in batch:\n        patches, label = item\n        # Ensure item is valid (e.g., image wasn't too small, no read errors)\n        if patches and label != -1:\n            # Only add patches if the list is not empty\n            all_patches.extend(patches)\n            # Repeat the label for each patch extracted from the image\n            all_labels.extend([label] * len(patches))\n            valid_batch_items += 1\n\n    # If no valid patches were collected in the batch (e.g., all images too small)\n    if not all_patches:\n        # Return empty tensors of appropriate type but 0 size in the batch dimension\n        patch_h, patch_w = patch_size_tuple\n        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n\n    # Convert list of NumPy arrays (each H, W) to a single NumPy array\n    patches_np = np.array(all_patches)  # Shape: (TotalPatches, H, W)\n\n    # Convert to PyTorch tensor, normalize\n    patches_tensor = torch.tensor(patches_np).float() / 255.0  # Shape: (TotalPatches, H, W)\n\n    # Add channel dimension: (TotalPatches, H, W) -> (TotalPatches, 1, H, W)\n    patches_tensor = patches_tensor.unsqueeze(1)\n\n    # Convert labels to PyTorch tensor\n    labels_tensor = torch.tensor(all_labels, dtype=torch.long)  # Use long for classification labels\n\n    return patches_tensor, labels_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:42:05.136223Z","iopub.execute_input":"2025-05-19T13:42:05.136651Z","iopub.status.idle":"2025-05-19T13:42:12.103476Z","shell.execute_reply.started":"2025-05-19T13:42:05.136622Z","shell.execute_reply":"2025-05-19T13:42:12.10162Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import easyocr\nimport numpy as np\nimport os\nimport tempfile\nfrom PIL import Image\n\n# Global OCR reader for efficiency\n_ocr_reader = None\n\ndef get_ocr_reader(languages=[\"en\"]):\n    global _ocr_reader\n    if _ocr_reader is None:\n        _ocr_reader = easyocr.Reader(languages)\n    return _ocr_reader\n\ndef extract_patches(image_array, num_patch=3, patch_size=(105, 105), \n                    extract_text=True, min_text_coverage=0.3, max_attempts=20):\n    patch_h, patch_w = patch_size\n\n    # Determine if grayscale or color\n    if image_array.ndim == 2:\n        h, w = image_array.shape\n        is_grayscale = True\n    elif image_array.ndim == 3:\n        h, w, _ = image_array.shape\n        is_grayscale = False\n    else:\n        print(f\"Unexpected image shape: {image_array.shape}\")\n        return []\n\n    # === Step 1: Resize image to height = 105, maintain aspect ratio ===\n    scale_factor = patch_h / h\n    new_w = int(w * scale_factor)\n    if is_grayscale:\n        resized = cv2.resize(image_array, (new_w, patch_h), interpolation=cv2.INTER_LINEAR)\n    else:\n        resized = cv2.resize(image_array, (new_w, patch_h), interpolation=cv2.INTER_LINEAR)\n\n    # === Step 2: Check if width is enough for patch ===\n    if new_w < patch_w:\n        return []\n\n    # === Step 3: If not extracting text, return random crops ===\n    if not extract_text:\n        patches = []\n        for _ in range(num_patch):\n            x = np.random.randint(0, new_w - patch_w + 1)\n            if is_grayscale:\n                patch = resized[:, x:x + patch_w]\n            else:\n                patch = resized[:, x:x + patch_w, :]\n            patches.append(patch)\n        return patches\n\n    # === Step 4: Try to find text patches using OCR ===\n    reader = get_ocr_reader()\n    text_patches = []\n    attempts = 0\n\n    while len(text_patches) < num_patch and attempts < max_attempts:\n        x = np.random.randint(0, new_w - patch_w + 1)\n        if is_grayscale:\n            patch = resized[:, x:x + patch_w]\n        else:\n            patch = resized[:, x:x + patch_w, :]\n\n        # Save patch to temporary file for OCR\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:\n            tmp_path = tmp.name\n            patch_img = Image.fromarray(patch)\n            patch_img.save(tmp_path)\n\n        try:\n            ocr_results = reader.readtext(tmp_path)\n            os.unlink(tmp_path)\n\n            patch_area = patch_h * patch_w\n            text_area = 0\n            for bbox, text, conf in ocr_results:\n                if conf < 0.5:\n                    continue\n                bbox = [[int(p[0]), int(p[1])] for p in bbox]\n                min_x = max(0, min(p[0] for p in bbox))\n                max_x = min(patch_w, max(p[0] for p in bbox))\n                min_y = max(0, min(p[1] for p in bbox))\n                max_y = min(patch_h, max(p[1] for p in bbox))\n                if max_x > min_x and max_y > min_y:\n                    text_area += (max_x - min_x) * (max_y - min_y)\n\n            if text_area / patch_area >= min_text_coverage:\n                text_patches.append(patch)\n\n        except Exception as e:\n            print(f\"OCR error: {e}\")\n            try:\n                os.unlink(tmp_path)\n            except:\n                pass\n\n        attempts += 1\n\n    return text_patches\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-19T13:42:17.604715Z","iopub.execute_input":"2025-05-19T13:42:17.605772Z","iopub.status.idle":"2025-05-19T13:42:23.980967Z","shell.execute_reply.started":"2025-05-19T13:42:17.605738Z","shell.execute_reply":"2025-05-19T13:42:23.97989Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# augmentation functions\nimport random\nimport numpy as np\nimport cv2\nfrom PIL import Image, ImageFilter\n\nTARGET_SIZE = (105, 105)\n\ndef to_uint8(img: np.ndarray) -> np.ndarray:\n    return np.clip(img, 0, 255).astype(np.uint8)\n\ndef noise_image(img: np.ndarray, mean=0.0, std=3.0) -> np.ndarray:\n    \"\"\"Add Gaussian noise.\"\"\"\n    f = img.astype(np.float32)\n    n = np.random.normal(mean, std, f.shape).astype(np.float32)\n    return to_uint8(f + n)\n\ndef blur_image(img: np.ndarray, sigma_range=(0.5, 1.5)) -> np.ndarray:\n    \"\"\"\n    Apply a mild, randomly‐parameterized Gaussian blur.\n    \n    Args:\n        img (np.ndarray): Input image, either H×W or H×W×C, dtype uint8.\n        sigma_range (tuple): Min/max sigma for the blur kernel.\n    \n    Returns:\n        np.ndarray: Blurred image, same shape and dtype uint8.\n    \"\"\"\n    # Ensure float for convolution\n    f = img.astype(np.float32)\n    # Randomize sigma in the given range\n    sigma = random.uniform(*sigma_range)\n    # OpenCV: kernel size (0,0) triggers automatic size based on sigma\n    if f.ndim == 2:\n        blurred = cv2.GaussianBlur(f, ksize=(0, 0), sigmaX=sigma, sigmaY=sigma)\n    else:\n        # Split channels and blur each independently\n        channels = cv2.split(f)\n        channels = [cv2.GaussianBlur(ch, ksize=(0, 0), sigmaX=sigma, sigmaY=sigma)\n                    for ch in channels]\n        blurred = cv2.merge(channels)\n    # Restore uint8\n    return np.clip(blurred, 0, 255).astype(np.uint8)\n\ndef affine_rotation(img: np.ndarray, max_deg=10) -> np.ndarray:\n    \"\"\"Small random affine warp.\"\"\"\n    h, w = img.shape[:2]\n    # random shift on three points\n    src = np.float32([[0,0],[w-1,0],[0,h-1]])\n    dx = w * 0.05\n    dy = h * 0.05\n    dst = src + np.random.uniform([-dx, -dy], [dx, dy], src.shape).astype(np.float32)\n    M = cv2.getAffineTransform(src, dst)\n    warped = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n    return warped\n\ndef shading_gradient(img: np.ndarray) -> np.ndarray:\n    \"\"\"Multiply by a random horizontal or vertical linear illumination gradient.\"\"\"\n    h, w = img.shape[:2]\n    start, end = random.uniform(0.6,1.4), random.uniform(0.6,1.4)\n    if random.choice([True,False]):\n        # horizontal\n        grad = np.linspace(start, end, w, dtype=np.float32)[None,:]\n        mask = np.repeat(grad, h, axis=0)\n    else:\n        # vertical\n        grad = np.linspace(start, end, h, dtype=np.float32)[:,None]\n        mask = np.repeat(grad, w, axis=1)\n    if img.ndim==3:\n        mask = mask[:,:,None]\n    shaded = img.astype(np.float32) * mask\n    return to_uint8(shaded)\n\ndef variable_aspect_ratio_preprocess(img: np.ndarray) -> np.ndarray:\n    \"\"\"Squeeze width by a random factor in [5/6,7/6], then pad/crop to original.\"\"\"\n    h, w = img.shape[:2]\n    factor = random.uniform(5/6, 7/6)\n    new_w = max(1, int(w/factor))\n    resized = cv2.resize(img, (new_w, h), interpolation=cv2.INTER_LINEAR)\n    # pad or crop back to w x h\n    if new_w < w:\n        pad = ( (0,0), ( (w-new_w)//2, (w-new_w)-(w-new_w)//2 ) ) + ((0,0),) if img.ndim==3 else ( (0,0), ( (w-new_w)//2, (w-new_w)-(w-new_w)//2 ) )\n        resized = np.pad(resized, pad, mode='reflect')\n    else:\n        x0 = (new_w - w)//2\n        resized = resized[:, x0:x0+w]\n    return resized\n\ndef final_resize(img: np.ndarray, size=TARGET_SIZE) -> np.ndarray:\n    \"\"\"Resize to target patch size.\"\"\"\n    return cv2.resize(img, size, interpolation=cv2.INTER_LINEAR)\n\ndef augmentation_pipeline(img: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Apply a random subset of the six DeepFont augmentations:\n      - noise, blur, affine warp, shading, variable spacing (as AR squeeze)\n      - note: gradient_fill (Laplacian) dropped since shading covers illumination.\n    Finally, resize to TARGET_SIZE.\n    \"\"\"\n    # Ensure uint8\n    img = to_uint8(img)\n    # 1) variable aspect ratio\n    img = variable_aspect_ratio_preprocess(img)\n    # 2) choose 2–4 augmentations from the pool\n    pool = [\n        noise_image,\n        blur_image,\n        affine_rotation,\n        shading_gradient\n    ]\n    for fn in pool:\n        img = fn(img)\n    # 3) final resize\n    img = final_resize(img)\n    return img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:40:25.181132Z","iopub.execute_input":"2025-05-19T15:40:25.18156Z","iopub.status.idle":"2025-05-19T15:40:25.204252Z","shell.execute_reply.started":"2025-05-19T15:40:25.181536Z","shell.execute_reply":"2025-05-19T15:40:25.202948Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"# augmentation functions\nfrom PIL import ImageFilter, Image\nimport random\nimport numpy as np\nimport cv2\n\ndef noise_image(np_img, mean=0, std=1):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n    img_array = np_img.astype(np.float32)\n    noise = np.random.normal(mean, std, img_array.shape)\n    noisy_img = img_array + noise\n    noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n    return cv2.resize(noisy_img, (105, 105))\n\ndef blur_image(np_img):\n    if isinstance(np_img, np.ndarray):\n        np_img = Image.fromarray(np_img.astype('uint8'))\n    blur_img = np_img.filter(ImageFilter.GaussianBlur(radius=1))\n    blur_img = blur_img.resize((105, 105))\n    return np.array(blur_img)\n\ndef affine_rotation(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    if len(np_img.shape) == 2:\n        np_img = np.expand_dims(np_img, axis=-1)\n\n    rows, cols = np_img.shape[:2]\n    src_pts = np.float32([[0, 0], [cols - 1, 0], [0, rows - 1]])\n    max_shift = 0.05\n    dst_pts = src_pts + np.random.uniform(-max_shift * cols, max_shift * cols, src_pts.shape).astype(np.float32)\n\n    A = cv2.getAffineTransform(src_pts, dst_pts)\n    warped = cv2.warpAffine(np_img, A, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n\n    if warped.ndim == 3 and warped.shape[-1] == 1:\n        warped = warped[:, :, 0]\n\n    warped = cv2.resize(warped, (105, 105))\n    return warped\n\ndef gradient_fill(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if len(np_img.shape) == 3:\n        gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = np_img\n\n    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n    abs_lap = np.absolute(laplacian) * 0.5\n    lap_uint8 = np.uint8(np.clip(abs_lap, 0, 255))\n    lap_resized = cv2.resize(lap_uint8, (105, 105))\n    return lap_resized\n\ndef shading_gradient(np_img, horizontal=None, start=None, end=None):\n    \"\"\"\n    Apply a gradient illumination shading to an image background.\n\n    Args:\n        np_img (np.ndarray or PIL.Image): Input grayscale or RGB image.\n        horizontal (bool): If True, horizontal gradient; if False, vertical; random if None.\n        start (float): Starting illumination factor (0.6–1.4); random if None.\n        end (float): Ending illumination factor (0.6–1.4); random if None.\n\n    Returns:\n        np.ndarray: Shaded image, dtype uint8.\n    \"\"\"\n    # Convert PIL to numpy\n    if isinstance(np_img, Image.Image):\n        img = np.array(np_img).astype(np.float32)\n    else:\n        img = np_img.astype(np.float32)\n\n    # Determine shape\n    if img.ndim == 2:\n        h, w = img.shape\n        channels = 1\n    else:\n        h, w, channels = img.shape\n\n    # Randomize orientation and factors\n    if horizontal is None:\n        horizontal = random.choice([True, False])\n    if start is None:\n        start = random.uniform(0.6, 1.4)\n    if end is None:\n        end = random.uniform(0.6, 1.4)\n\n    # Build gradient mask\n    if horizontal:\n        grad = np.linspace(start, end, w, dtype=np.float32)[None, :]\n        mask = np.repeat(grad, h, axis=0)\n    else:\n        grad = np.linspace(start, end, h, dtype=np.float32)[:, None]\n        mask = np.repeat(grad, w, axis=1)\n\n    # Expand to channels\n    if channels > 1:\n        mask = mask[:, :, None]\n\n    # Apply and clip\n    shaded = img * mask\n    shaded = np.clip(shaded, 0, 255).astype(np.uint8)\n    return shaded\n\ndef variable_aspect_ratio_preprocess(np_img, target_size=(105, 105)):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    if len(np_img.shape) == 3:\n        h, w, c = np_img.shape\n    else:\n        h, w = np_img.shape\n        c = None\n\n    scale_ratio = np.random.uniform(0.95, 1.05)\n    new_width = int(w * scale_ratio)\n\n    resized = cv2.resize(np_img, (new_width, h), interpolation=cv2.INTER_LINEAR)\n    final = cv2.resize(resized, target_size, interpolation=cv2.INTER_LINEAR)\n    return final\n\ndef augmentation_pipeline(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    img = variable_aspect_ratio_preprocess(np_img)\n\n    augmentations = [\n        lambda x: noise_image(x)\n        # lambda x: blur_image(x),\n        # lambda x: affine_rotation(x),\n        # lambda x: shading_gradient(x)\n        # lambda x: gradient_fill(x)\n    ]\n\n    num_aug = random.randint(1, 3)\n    selected_augs = random.sample(augmentations, num_aug)\n    img = augmentations(img)\n    # for aug in selected_augs:\n    #     img = aug(img)\n    #     if isinstance(img, Image.Image):\n    #         img = np.array(img)\n    #     if img.dtype != np.uint8:\n    #         img = np.clip(img, 0, 255).astype(np.uint8)\n\n    return img","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-19T15:01:59.006703Z","iopub.execute_input":"2025-05-19T15:01:59.007138Z","iopub.status.idle":"2025-05-19T15:01:59.032333Z","shell.execute_reply.started":"2025-05-19T15:01:59.007111Z","shell.execute_reply":"2025-05-19T15:01:59.030934Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# combined dataset\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom io import BytesIO\n# from datasets import Dataset\nfrom torch.utils.data import Dataset\n\nclass CombinedImageDataset(Dataset):\n    \"\"\"\n    A dataset class that combines both .jpeg files and .bcf files into a single dataset.\n    This class can handle loading and patch extraction from both .jpeg and .bcf files.\n    \"\"\"\n    def __init__(self, jpeg_dir, bcf_file, label_file, testing=False, num_patch=3, patch_size=(105, 105), \n                 extract_text=False, min_text_coverage=0.3, max_attempts=20, ocr_languages=[\"en\"]):\n        \"\"\"\n        Initializes the dataset by loading both jpeg files and bcf files into one dataset.\n\n        Args:\n            jpeg_dir (str): Directory containing .jpeg files.\n            bcf_file (str): Path to the .bcf file.\n            label_file (str): Path to the label file corresponding to the .bcf file.\n            num_patch (int): Number of patches to extract per image.\n            patch_size (tuple): Tuple (height, width) for the size of the patches.\n            extract_text (bool): Whether to prioritize patches containing text\n            min_text_coverage (float): Minimum ratio of text area to patch area (0-1)\n            max_attempts (int): Maximum number of attempts to find text patches\n            ocr_languages (list): Languages for EasyOCR to detect\n        \"\"\"\n        self.jpeg_dir = jpeg_dir\n        self.bcf_file = bcf_file\n        self.label_file = label_file\n        self.testing = testing\n        self.num_patch = num_patch\n        self.patch_size = patch_size\n        self.extract_text = extract_text\n        self.min_text_coverage = min_text_coverage\n        self.max_attempts = max_attempts\n        self.ocr_languages = ocr_languages\n\n        # Initialize OCR reader if needed\n        if extract_text:\n            self.reader = get_ocr_reader(ocr_languages)\n\n        self.jpeg_data = []\n        self.bcf_data = []\n\n        # Load jpeg data\n        self._load_jpeg_data(jpeg_dir)\n\n        # Load bcf data\n        self._load_bcf_data(bcf_file, label_file)\n\n    def _extract_patches_test(self, img_array):\n        h, w = img_array.shape[:2]\n        target_h, target_w = self.patch_size\n        # 1) resize height\n        new_w = int(w * (target_h / h))\n        img = cv2.resize(img_array, (new_w, target_h), interpolation=cv2.INTER_LINEAR)\n        patches = []\n        for _scale in range(3):\n            factor = np.random.uniform(1.5, 3.5)\n            sw = max(1, int(new_w / factor))\n            squeezed = cv2.resize(img, (sw, target_h), interpolation=cv2.INTER_LINEAR)\n            # nếu width < target_w thì pad reflect, else crop giữa\n            if sw < target_w:\n                pad = target_w - sw\n                left = pad//2; right = pad-left\n                squeezed = np.pad(squeezed,\n                                  ((0,0),(left,right)) + ((0,0),)*(img.ndim-2),\n                                  mode='reflect')\n            else:\n                x0 = (sw - target_w)//2\n                squeezed = squeezed[:, x0:x0+target_w]\n            # crop 5 patch random\n            for _ in range(5):\n                x = np.random.randint(0, target_w - target_w + 1)\n                y = np.random.randint(0,      0  + 1)  # vì height==target_h\n                patch = squeezed[y:y+target_h, x:x+target_w] if img.ndim==2 \\\n                        else squeezed[y:y+target_h, x:x+target_w, :]\n                patches.append(patch)\n        return patches\n\n    def _load_jpeg_data(self, jpeg_dir):\n        \"\"\"Loads the .jpeg files from the specified directory.\"\"\"\n        if not os.path.exists(jpeg_dir):\n            print(f\"Warning: JPEG directory {jpeg_dir} does not exist.\")\n            return\n            \n        image_filenames = [f for f in os.listdir(jpeg_dir) if f.lower().endswith(('.jpeg', '.jpg'))]\n        self.jpeg_data = [(os.path.join(jpeg_dir, f), 0) for f in image_filenames]  # Assuming label is 0 for .jpeg files\n        print(f\"Loaded {len(self.jpeg_data)} .jpeg images.\")\n\n    def _load_bcf_data(self, bcf_file, label_file):\n        \"\"\"Loads the .bcf file and the associated label file.\"\"\"\n        if not (os.path.exists(bcf_file) and os.path.exists(label_file)):\n            print(f\"Warning: BCF file {bcf_file} or label file {label_file} does not exist.\")\n            return\n            \n        try:\n            with open(label_file, 'rb') as f:\n                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n                print(f\"Loaded {len(self.labels)} labels from {label_file}.\")\n\n            with open(bcf_file, 'rb') as f:\n                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n                print(f\"Loaded {self.num_images} images from {bcf_file}.\")\n\n                sizes_bytes = f.read(self.num_images * 8)\n                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n\n                self.data_start_offset = 8 + self.num_images * 8\n                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n\n                for idx in range(self.num_images):\n                    self.bcf_data.append((idx, self.labels[idx]))\n                \n            print(f\"Loaded {len(self.bcf_data)} .bcf images.\")\n        except Exception as e:\n            print(f\"Error loading .bcf data: {e}\")\n\n    def __len__(self):\n        \"\"\"Returns the total number of images in the combined dataset.\"\"\"\n        return len(self.jpeg_data) + len(self.bcf_data)\n\n    def _extract_patches(self, img_array):\n        \"\"\"Helper function to extract patches from an image.\"\"\"\n        return extract_patches(\n            img_array, \n            num_patch=self.num_patch, \n            patch_size=self.patch_size,\n            extract_text=self.extract_text, \n            min_text_coverage=self.min_text_coverage,\n            max_attempts=self.max_attempts\n        )\n\n    def __getitem__(self, idx):\n        \"\"\"Fetches one item with robust error handling for corrupted images.\"\"\"\n        # Handle case where idx is a list (batch loading)\n        if isinstance(idx, list):\n            # Handle batch indices properly\n            results = []\n            labels = []\n            for single_idx in idx:\n                try:\n                    patches, label = self.__getitem__(single_idx)  # Call recursively with single index\n                    if patches and label != -1:\n                        results.append(patches)\n                        labels.append(label)\n                except Exception as e:\n                    print(f\"Error processing index {single_idx}: {e}\")\n                    # Skip this item on error\n            \n            # Return whatever valid items we were able to get\n            return results, labels\n        \n        # Original single-item loading logic\n        max_retries = 3  # Try a few times before giving up on an index\n        \n        for retry in range(max_retries):\n            try:\n                if idx < len(self.jpeg_data):\n                    # JPEG image with error handling\n                    img_path, label = self.jpeg_data[idx]\n                    try:\n                        with warnings.catch_warnings():\n                            warnings.simplefilter(\"ignore\")  # Ignore PIL warnings\n                            img = Image.open(img_path)\n                            img.verify()  # Verify image is not corrupted\n                        \n                        # Re-open since verify() closes the file\n                        img = Image.open(img_path).convert('L')\n                        img_array = np.array(img)\n                        patches = self._extract_patches(img_array)\n                        # patches = [augmentation_pipeline(patch) for patch in patches]\n                        \n                        # Clean memory\n                        del img, img_array\n                        \n                        return patches, label\n                    \n                    except (OSError, IOError, ValueError) as e:\n                        # Image is corrupted, return empty list\n                        print(f\"Warning: Corrupt image at {img_path}: {e}\")\n                        return [], -1\n                        \n                else:\n                    # BCF image with error handling\n                    bcf_idx = idx - len(self.jpeg_data)\n                    if bcf_idx >= len(self.bcf_data):\n                        return [], -1\n                        \n                    label = self.bcf_data[bcf_idx][1]\n                    offset = self.image_offsets[bcf_idx]\n                    size = self.image_sizes[bcf_idx]\n                    \n                    try:\n                        with open(self.bcf_file, 'rb') as f:\n                            f.seek(self.data_start_offset + offset)\n                            image_bytes = f.read(size)\n                        \n                        # Use BytesIO to catch corruption\n                        buffer = BytesIO(image_bytes)\n                        img = Image.open(buffer)\n                        img.verify()  # Verify it's valid\n                        \n                        # Re-open since verify() closes the file\n                        buffer.seek(0)\n                        img = Image.open(buffer).convert('L')\n                        img_array = np.array(img)\n                        \n                        if self.testing:\n                            patches = self._extract_patches_test(img_array)\n                        else:\n                            patches = self._extract_patches(img_array)\n                            patches = [augmentation_pipeline(patch) for patch in patches]\n                        \n                        # Clean memory\n                        del img, img_array, buffer, image_bytes\n                        \n                        return patches, label\n                    \n                    except (OSError, IOError, ValueError) as e:\n                        print(f\"Warning: Corrupt BCF image at index {bcf_idx}: {e}\")\n                        return [], -1\n                        \n            except Exception as e:\n                print(f\"Unexpected error processing idx {idx}: {e}\")\n            \n            # If we got here, there was an issue with this index - try a different one\n            # Important: increment as an integer, not trying to add to a list\n            if retry < max_retries - 1:  # Only increment if we have retries left\n                idx = (int(idx) + 1) % len(self)\n        \n        # If all retries failed, return empty\n        return [], -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:25:21.728051Z","iopub.execute_input":"2025-05-19T16:25:21.72845Z","iopub.status.idle":"2025-05-19T16:25:21.765234Z","shell.execute_reply.started":"2025-05-19T16:25:21.728424Z","shell.execute_reply":"2025-05-19T16:25:21.763907Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"# memory_efficient_patch_collate_fn\nimport gc\nimport warnings\nfrom functools import partial\n\n# Add this memory-efficient patch collate function\ndef memory_efficient_patch_collate_fn(batch, patch_size_tuple):\n    \"\"\"\n    Memory-efficient version of patch_collate_fn that processes one patch at a time\n    and includes robust error handling.\n    \"\"\"\n    import gc  # Import inside function for worker processes\n    \n    all_patches = []\n    all_labels = []\n    valid_batch_items = 0\n\n    # Process one item at a time to avoid large memory allocations\n    for item in batch:\n        patches, label = item\n        # Ensure item is valid\n        if patches and label != -1:\n            # Process patches one by one\n            for patch in patches:\n                all_patches.append(patch)\n                all_labels.append(label)\n            valid_batch_items += 1\n    \n    # Periodically force garbage collection\n    if len(all_patches) > 100:\n        gc.collect()\n    \n    # Empty batch handling\n    if not all_patches:\n        patch_h, patch_w = patch_size_tuple\n        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n\n    # Process in smaller chunks to reduce peak memory usage\n    max_chunk_size = 64  # Adjust based on your GPU memory\n    num_patches = len(all_patches)\n    patches_tensor_list = []\n    \n    for i in range(0, num_patches, max_chunk_size):\n        chunk = all_patches[i:i+max_chunk_size]\n        # Convert to NumPy array\n        chunk_np = np.stack(chunk)\n        # Convert to tensor, normalize and add channel dimension\n        chunk_tensor = torch.from_numpy(chunk_np).float() / 255.0\n        chunk_tensor = chunk_tensor.unsqueeze(1)\n        patches_tensor_list.append(chunk_tensor)\n        \n        # Clear variables to free memory\n        del chunk, chunk_np\n    \n    # Concatenate chunks\n    patches_tensor = torch.cat(patches_tensor_list, dim=0)\n    labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n    \n    # Clean up\n    del patches_tensor_list, all_patches, all_labels\n    gc.collect()\n    \n    return patches_tensor, labels_tensor\n\n# Add this function to create optimized DataLoaders\nimport torch\nfrom torch.utils.data import DataLoader\nfrom functools import partial\n\ndef create_optimized_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper error handling, avoiding HuggingFace datasets compatibility issues.\n    \n    Args:\n        dataset: The image dataset instance\n        batch_size: Batch size for training\n        num_workers: Number of worker processes\n        val_split: Validation split ratio (0-1)\n        \n    Returns:\n        tuple: (train_loader, val_loader)\n    \"\"\"\n    from torch.utils.data import DataLoader, Subset\n    import numpy as np\n    \n    # Calculate split sizes\n    dataset_size = len(dataset)\n    indices = np.arange(dataset_size)\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets - this avoids Hugging Face's __getitems__ implementation\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    # Custom collate function with error handling\n    def safe_collate(batch):\n        # Filter out empty or invalid items\n        valid_batch = [(patches, label) for patches, label in batch if patches and label != -1]\n        \n        if not valid_batch:\n            # Return empty tensors if no valid items\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n        \n        # Process valid items\n        all_patches = []\n        all_labels = []\n        \n        for patches, label in valid_batch:\n            if isinstance(patches, list) and patches:\n                all_patches.extend(patches)\n                all_labels.extend([label] * len(patches))\n        \n        if not all_patches:\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n            \n        # Convert to PyTorch tensors\n        try:\n            patches_np = np.array(all_patches)\n            patches_tensor = torch.tensor(patches_np, dtype=torch.float) / 255.0\n            \n            # Add channel dimension if needed\n            if len(patches_tensor.shape) == 3:  # (B, H, W)\n                patches_tensor = patches_tensor.unsqueeze(1)  # -> (B, 1, H, W)\n                \n            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n            return patches_tensor, labels_tensor\n        except Exception as e:\n            print(f\"Error in collate function: {e}\")\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n    \n    # Create DataLoaders with minimal worker configuration for stability\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-19T14:04:37.126218Z","iopub.execute_input":"2025-05-19T14:04:37.127122Z","iopub.status.idle":"2025-05-19T14:04:37.154604Z","shell.execute_reply.started":"2025-05-19T14:04:37.127081Z","shell.execute_reply":"2025-05-19T14:04:37.151929Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# data wrapper\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport gc\n\n\nclass DatasetWrapper(Dataset):\n    \"\"\"\n    A wrapper for your dataset to ensure compatibility with DataLoader\n    \"\"\"\n    def __init__(self, original_dataset):\n        self.dataset = original_dataset\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # Get a single item by index, handling both direct dataset access\n        # and access through Subset indices\n        try:\n            # Handle if we're accessing through a Subset\n            if hasattr(self.dataset, 'dataset') and hasattr(self.dataset, 'indices'):\n                original_idx = self.dataset.indices[idx]\n                return self.dataset.dataset[original_idx]\n            # Normal access\n            return self.dataset[idx]\n        except Exception as e:\n            print(f\"Error accessing item {idx}: {e}\")\n            # Return a placeholder for invalid items\n            return [], -1\n\n\ndef create_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper handling for HuggingFace datasets.\n    \"\"\"\n    # Ensure the dataset is properly wrapped\n    wrapped_dataset = DatasetWrapper(dataset)\n    \n    # Calculate split sizes\n    dataset_size = len(wrapped_dataset)\n    indices = list(range(dataset_size))\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets\n    train_dataset = Subset(wrapped_dataset, train_indices)\n    val_dataset = Subset(wrapped_dataset, val_indices)\n    \n    # Custom collate function\n    def custom_collate_fn(batch):\n        # Filter out invalid items\n        batch = [(img, label) for img, label in batch if img is not None and len(img) > 0 and label != -1]\n        \n        if not batch:\n            # Return empty tensors with appropriate dimensions\n            return torch.empty((0, 3, 105, 105), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n        \n        # Extract images and labels\n        images, labels = zip(*batch)\n        \n        # Convert to tensors\n        images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in images])\n        labels_tensor = torch.tensor(labels, dtype=torch.long)\n        \n        # Normalize images if needed\n        if images_tensor.max() > 1.0:\n            images_tensor = images_tensor / 255.0\n            \n        return images_tensor, labels_tensor\n    \n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=custom_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=custom_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    return train_loader, val_loader\n\n\n# For datasets where you're dealing with patches\ndef create_patch_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1, patch_size=(105, 105)):\n    \"\"\"\n    Creates DataLoaders specifically for patch-based datasets where each item\n    may contain multiple patches.\n    \"\"\"\n    # Ensure the dataset is properly wrapped\n    wrapped_dataset = DatasetWrapper(dataset)\n    \n    # Calculate split sizes\n    dataset_size = len(wrapped_dataset)\n    indices = list(range(dataset_size))\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets\n    train_dataset = Subset(wrapped_dataset, train_indices)\n    val_dataset = Subset(wrapped_dataset, val_indices)\n    \n    # Memory efficient collate function for patches\n    def patch_collate_fn(batch):\n        all_patches = []\n        all_labels = []\n        \n        # Process one batch item at a time\n        for patches, label in batch:\n            if patches is not None and len(patches) > 0 and label != -1:\n                # Handle both single patches and lists of patches\n                if isinstance(patches, list):\n                    all_patches.extend(patches)\n                    all_labels.extend([label] * len(patches))\n                else:\n                    all_patches.append(patches)\n                    all_labels.append(label)\n        \n        # Return empty tensors if batch is empty\n        if not all_patches:\n            return torch.empty((0, 3, patch_size[0], patch_size[1]), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n            \n        try:\n            # Process in chunks to reduce memory usage\n            max_chunk_size = min(64, len(all_patches))\n            patches_tensors = []\n            \n            for i in range(0, len(all_patches), max_chunk_size):\n                chunk = all_patches[i:i+max_chunk_size]\n                chunk_tensor = torch.stack([torch.tensor(p, dtype=torch.float) for p in chunk])\n                \n                # Normalize if needed\n                if chunk_tensor.max() > 1.0:\n                    chunk_tensor = chunk_tensor / 255.0\n                    \n                patches_tensors.append(chunk_tensor)\n                \n            # Combine chunks\n            patches_tensor = torch.cat(patches_tensors, dim=0).unsqueeze(1)\n            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n            \n            # Clean up to save memory\n            del patches_tensors, all_patches, all_labels\n            gc.collect()\n            \n            return patches_tensor, labels_tensor\n        except Exception as e:\n            print(f\"Error in collate function: {e}\")\n            return torch.empty((0, 3, patch_size[0], patch_size[1]), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n    \n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=patch_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=patch_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    return train_loader, val_loader\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-19T14:04:42.021922Z","iopub.execute_input":"2025-05-19T14:04:42.022261Z","iopub.status.idle":"2025-05-19T14:04:42.052055Z","shell.execute_reply.started":"2025-05-19T14:04:42.022235Z","shell.execute_reply":"2025-05-19T14:04:42.050115Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Check all elements of combined_dataset for shape consistency\ndef check_shapes(dataset):\n    \"\"\"\n    Check if all patches in the dataset have the same shape.\n    \n    Args:\n        dataset: The dataset instance to check.\n        \n    Returns:\n        bool: True if all patches have the same shape, False otherwise.\n    \"\"\"\n    first_shape = None\n    for idx in range(len(dataset)):\n        patches, _ = dataset[idx]\n        if not patches:\n            continue  # Skip empty patches\n        current_shape = patches[0].shape\n        if first_shape is None:\n            first_shape = current_shape\n        elif current_shape != first_shape:\n            print(f\"Shape mismatch at index {idx}: {current_shape} != {first_shape}\")\n            return False\n    return True \ncheck_shapes(combined_dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load dataset -> create dataloader -> training script\nimport torch\nimport torch.nn as nn\nimport os\nimport gc\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\n\n# Clean memory before starting\ngc.collect()\ntorch.cuda.empty_cache()\n\njpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\nbcf_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf\"\nlabel_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label\"\n\n# Print GPU info\nif torch.cuda.is_available():\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n    print(f\"Available memory: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n\n# Create dataset with smaller patch size and fewer patches per image\ncombined_dataset = CombinedImageDataset(\n    jpeg_dir=jpeg_dir,\n    bcf_file=bcf_file,\n    label_file=label_file,\n    num_patch=3,  # Number of patches per image\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:58:12.486363Z","iopub.execute_input":"2025-05-19T15:58:12.486795Z","iopub.status.idle":"2025-05-19T15:58:15.613668Z","shell.execute_reply.started":"2025-05-19T15:58:12.486771Z","shell.execute_reply":"2025-05-19T15:58:15.612479Z"}},"outputs":[{"name":"stdout","text":"Loaded 82389 .jpeg images.\nLoaded 202000 labels from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label.\nLoaded 202000 images from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf.\nLoaded 202000 .bcf images.\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"import pickle\n\ndef save_dataset(dataset: CombinedImageDataset, filepath: str):\n    \"\"\"\n    Serialize and save a CombinedImageDataset to disk.\n    \"\"\"\n    with open(filepath, 'wb') as f:\n        pickle.dump(dataset, f)\n    print(f\"Dataset saved to {filepath!r}\")\n\ndef load_dataset(filepath: str) -> CombinedImageDataset:\n    \"\"\"\n    Load a pickled CombinedImageDataset from disk.\n    \"\"\"\n    with open(filepath, 'rb') as f:\n        dataset = pickle.load(f)\n    print(f\"Dataset loaded from {filepath!r}\")\n    return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:43:47.436274Z","iopub.execute_input":"2025-05-19T15:43:47.43663Z","iopub.status.idle":"2025-05-19T15:43:47.443112Z","shell.execute_reply.started":"2025-05-19T15:43:47.436609Z","shell.execute_reply":"2025-05-19T15:43:47.441912Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"real_test_dataset = CombinedImageDataset(\n    jpeg_dir=\"jpeg_dir\",\n    bcf_file=\"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\",\n    label_file=\"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\",\n    testing=True,\n    num_patch=3,  # Number of patches per image\n)\nsave_dataset(real_test_dataset, \"/kaggle/working/real_test_dataset.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:26:38.876516Z","iopub.execute_input":"2025-05-19T16:26:38.876913Z","iopub.status.idle":"2025-05-19T16:26:38.898365Z","shell.execute_reply.started":"2025-05-19T16:26:38.876884Z","shell.execute_reply":"2025-05-19T16:26:38.897273Z"}},"outputs":[{"name":"stdout","text":"Warning: JPEG directory jpeg_dir does not exist.\nLoaded 3202 labels from /kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label.\nLoaded 3202 images from /kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf.\nLoaded 3202 .bcf images.\nDataset saved to '/kaggle/working/real_test_dataset.pkl'\n","output_type":"stream"}],"execution_count":129},{"cell_type":"code","source":"dataset = load_dataset(\"/kaggle/working/real_test_dataset.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:30:05.7173Z","iopub.execute_input":"2025-05-19T16:30:05.718882Z","iopub.status.idle":"2025-05-19T16:30:05.747997Z","shell.execute_reply.started":"2025-05-19T16:30:05.718809Z","shell.execute_reply":"2025-05-19T16:30:05.74662Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded from '/kaggle/working/real_test_dataset.pkl'\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"combined_dataset = CombinedImageDataset(\n    jpeg_dir=\"\",\n    bcf_file=bcf_file,\n    label_file=label_file,\n    num_patch=3,  # Number of patches per image\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:52:31.99525Z","iopub.execute_input":"2025-05-19T15:52:31.9956Z","iopub.status.idle":"2025-05-19T15:52:32.054206Z","shell.execute_reply.started":"2025-05-19T15:52:31.995567Z","shell.execute_reply":"2025-05-19T15:52:32.052934Z"}},"outputs":[{"name":"stdout","text":"Warning: JPEG directory  does not exist.\nLoaded 202000 labels from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label.\nLoaded 202000 images from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf.\nLoaded 202000 .bcf images.\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"len(combined_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:25:49.263691Z","iopub.execute_input":"2025-05-19T15:25:49.26404Z","iopub.status.idle":"2025-05-19T15:25:49.270509Z","shell.execute_reply.started":"2025-05-19T15:25:49.264017Z","shell.execute_reply":"2025-05-19T15:25:49.269643Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"82389"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"# Create memory-optimized dataloaders with smaller batch size\ntrain_loader, val_loader = create_patch_dataloaders(\n    combined_dataset,\n    batch_size=64,\n    num_workers=2,\n    val_split=0.1,\n    patch_size=(105, 105)\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!touch /kaggle/working/dataloaders.pkl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport torch\nfrom torch.utils.data import DataLoader, Subset\n\ndef save_dataloaders(train_loader, val_loader, filename='dataloaders.pkl'):\n    \"\"\"\n    Save dataset indices and DataLoader configurations to a pickle file.\n    \n    Args:\n        train_loader: Training DataLoader\n        val_loader: Validation DataLoader\n        filename: Name of the output pickle file\n    \"\"\"\n    # Extract the dataset and indices from the loaders\n    if isinstance(train_loader.dataset, Subset):\n        train_indices = train_loader.dataset.indices\n        dataset = train_loader.dataset.dataset  # Get the original dataset\n    else:\n        train_indices = list(range(len(train_loader.dataset)))\n        dataset = train_loader.dataset\n        \n    if isinstance(val_loader.dataset, Subset):\n        val_indices = val_loader.dataset.indices\n    else:\n        val_indices = list(range(len(val_loader.dataset)))\n    \n    # Extract DataLoader configurations - with defaults for attributes that might not be accessible\n    train_config = {\n        'batch_size': getattr(train_loader, 'batch_size', 64),\n        'shuffle': True,  # Default to True for training loader\n        'num_workers': getattr(train_loader, 'num_workers', 0),\n        'pin_memory': getattr(train_loader, 'pin_memory', False),\n        'drop_last': getattr(train_loader, 'drop_last', False),\n    }\n    \n    val_config = {\n        'batch_size': getattr(val_loader, 'batch_size', 64),\n        'shuffle': False,  # Default to False for validation loader\n        'num_workers': getattr(val_loader, 'num_workers', 0),\n        'pin_memory': getattr(val_loader, 'pin_memory', False),\n        'drop_last': getattr(val_loader, 'drop_last', False),\n    }\n    \n    # Try to get collate_fn name safely\n    collate_fn_name = None\n    if hasattr(train_loader, 'collate_fn') and train_loader.collate_fn is not None:\n        if hasattr(train_loader.collate_fn, '__name__'):\n            collate_fn_name = train_loader.collate_fn.__name__\n    \n    # Save dataset, indices, and configs to a file\n    save_data = {\n        'dataset': dataset,\n        'train_indices': train_indices,\n        'val_indices': val_indices,\n        'train_config': train_config,\n        'val_config': val_config,\n        'collate_fn_name': collate_fn_name\n    }\n    \n    with open(filename, 'wb') as f:\n        pickle.dump(save_data, f)\n    \n    print(f\"DataLoader configurations saved to {filename}\")\n\ndef load_dataloaders(filename='dataloaders.pkl', custom_collate_fn=None):\n    \"\"\"\n    Load dataset and recreate DataLoaders from a pickle file.\n    \n    Args:\n        filename: Name of the pickle file to load\n        custom_collate_fn: Custom collate function if needed\n        \n    Returns:\n        tuple: (train_loader, val_loader)\n    \"\"\"\n    with open(filename, 'rb') as f:\n        saved_data = pickle.load(f)\n    \n    dataset = saved_data['dataset']\n    train_indices = saved_data['train_indices']\n    val_indices = saved_data['val_indices']\n    train_config = saved_data['train_config']\n    val_config = saved_data['val_config']\n    \n    # Use provided collate_fn or None\n    collate_fn = custom_collate_fn\n    \n    # Create the subsets\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    # Recreate the DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        collate_fn=collate_fn,\n        **train_config\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        collate_fn=collate_fn,\n        **val_config\n    )\n    \n    print(f\"DataLoaders successfully loaded from {filename}\")\n    return train_loader, val_loader","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save your DataLoaders\nsave_dataloaders(train_loader, val_loader, '/kaggle/working/dataloaders.pkl')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load our dataloaders\nfrom functools import partial\nmemory_efficient_collate = partial(memory_efficient_patch_collate_fn, patch_size_tuple=(105, 105))\ntrain_loader, val_loader = load_dataloaders(\n    '/kaggle/working/dataloaders.pkl', \n    custom_collate_fn=memory_efficient_collate\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization some samples from the combined dataset \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom PIL import Image, ImageFile\nfrom io import BytesIO\nimport os\n\ndef visualize_simple_images_and_patches(dataset, num_images=2, seed=None):\n    # Allow loading of truncated images\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n    \n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Find valid images (with patches)\n    valid_indices = []\n    attempts = 0\n    max_attempts = min(len(dataset) * 2, 100)  # Limit search attempts\n    \n    while len(valid_indices) < num_images and attempts < max_attempts:\n        idx = random.randint(0, len(dataset) - 1)\n        if idx not in valid_indices:  # Avoid duplicates\n            try:\n                patches, label = dataset[idx]\n                if patches and len(patches) > 0:\n                    valid_indices.append(idx)\n            except Exception as e:\n                print(f\"Error loading index {idx}: {e}\")\n            attempts += 1\n    \n    # If we couldn't find enough valid images\n    if len(valid_indices) < num_images:\n        print(f\"Warning: Could only find {len(valid_indices)} valid images with patches\")\n        if len(valid_indices) == 0:\n            print(\"No valid images found. Check your dataset.\")\n            return\n    \n    # Create figure with enough space for all elements\n    fig, axes = plt.subplots(len(valid_indices), 4, figsize=(16, 5 * len(valid_indices)))\n    \n    # If only one image is requested, make axes indexable as 2D\n    if len(valid_indices) == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i, idx in enumerate(valid_indices):\n        try:\n            # Get item directly from dataset\n            patches, label = dataset[idx]\n            \n            # Get the original full image\n            img_array = None\n            \n            if hasattr(dataset, 'jpeg_data') and idx < len(dataset.jpeg_data):\n                # From jpeg_data\n                img_path, _ = dataset.jpeg_data[idx]\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {os.path.basename(img_path)}\"\n                \n            elif hasattr(dataset, 'image_filenames') and not hasattr(dataset, 'num_images'):\n                # From BCFImagePatchDataset with JPEG source\n                img_path = os.path.join(dataset.data_source, dataset.image_filenames[idx])\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {dataset.image_filenames[idx]}\"\n                \n            else:\n                # From BCF file (either CombinedImageDataset or BCFImagePatchDataset)\n                if hasattr(dataset, 'bcf_data'):\n                    # CombinedImageDataset\n                    bcf_idx = idx - len(dataset.jpeg_data)\n                    if bcf_idx < 0 or bcf_idx >= len(dataset.bcf_data):\n                        print(f\"Invalid BCF index: {bcf_idx}\")\n                        continue\n                        \n                    offset = dataset.image_offsets[bcf_idx]\n                    size = dataset.image_sizes[bcf_idx]\n                    data_file = dataset.bcf_file\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {bcf_idx})\"\n                else:\n                    # BCFImagePatchDataset\n                    offset = dataset.image_offsets[idx]\n                    size = dataset.image_sizes[idx]\n                    data_file = dataset.data_source\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {idx})\"\n                \n                with open(data_file, 'rb') as f:\n                    f.seek(data_start + offset)\n                    image_bytes = f.read(size)\n                img = Image.open(BytesIO(image_bytes)).convert('L')\n                img_array = np.array(img)\n            \n            # Plot original image if we successfully loaded it\n            if img_array is not None:\n                axes[i, 0].imshow(img_array, cmap='gray')\n                axes[i, 0].set_title(f\"Original Image\\nLabel: {label}\\nSource: {source}\")\n                axes[i, 0].axis('off')\n            else:\n                axes[i, 0].text(0.5, 0.5, \"Image loading failed\", ha='center', va='center')\n                axes[i, 0].axis('off')\n            \n            # Plot the patches - ensure we have patches to display\n            if patches and len(patches) > 0:\n                for j in range(3):\n                    if j < len(patches):\n                        patch = patches[j]\n                        axes[i, j+1].imshow(patch, cmap='gray')\n                        axes[i, j+1].set_title(f\"Patch {j+1}\\nShape: {patch.shape}\")\n                    else:\n                        # No more patches to display\n                        axes[i, j+1].text(0.5, 0.5, \"No patch\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            else:\n                # No patches for this image\n                for j in range(3):\n                    axes[i, j+1].text(0.5, 0.5, \"No patches extracted\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            \n        except Exception as e:\n            print(f\"Error processing index {idx}: {e}\")\n            # Create error message in subplot\n            for j in range(4):\n                axes[i, j].text(0.5, 0.5, f\"Error: {str(e)[:50]}...\", ha='center', va='center')\n                axes[i, j].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return the indices we used (helpful for debugging)\n    return valid_indices\n\n# Example usage:\nvisualize_simple_images_and_patches(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:30:40.266224Z","iopub.execute_input":"2025-05-19T16:30:40.266544Z","iopub.status.idle":"2025-05-19T16:30:41.059267Z","shell.execute_reply.started":"2025-05-19T16:30:40.266512Z","shell.execute_reply":"2025-05-19T16:30:41.058176Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1600x1000 with 8 Axes>","image/png":"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\n"},"metadata":{}},{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"[1486, 1180]"},"metadata":{}}],"execution_count":136},{"cell_type":"code","source":"for batch in train_loader:\n    images, labels = batch\n    print(f\"Batch size: {images.size()}, Labels size: {labels.size()}\")\n    break  # Just check the first batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_memory_efficient_model\n\nimport torch.cuda.amp as amp\nimport gc\n\ndef train_memory_efficient_model(model, train_loader, val_loader=None, \n                                num_epochs=5, learning_rate=0.0001,\n                                checkpoint_dir=\"/kaggle/working/\"):\n    \"\"\"\n    Memory-efficient training function for SCAE model.\n    \"\"\"\n    # Ensure checkpoint directory exists\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Setup device and optimization tools\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Training on {device} with {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n    print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n    print(f\"Memory reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Set up mixed precision training\n    scaler = amp.GradScaler()\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n    \n    # Track best model\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    try:\n        for epoch in range(num_epochs):\n            # Clean memory before each epoch\n            gc.collect()\n            torch.cuda.empty_cache()\n            \n            # TRAINING PHASE\n            model.train()\n            running_loss = 0.0\n            valid_batches = 0\n            \n            # Use tqdm for progress tracking\n            pbar = tqdm(train_loader)\n            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n            \n            for batch_idx, (patches, _) in enumerate(pbar):\n                # Skip empty batches\n                if patches.numel() == 0:\n                    continue\n                \n                # Move data to device\n                patches = patches.to(device, non_blocking=True)\n                \n                # Mixed precision forward pass\n                with amp.autocast():\n                    outputs = model(patches)\n                    loss = criterion(outputs, patches)\n                \n                # Backward pass with gradient scaling\n                optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                \n                # Update metrics\n                running_loss += loss.item()\n                valid_batches += 1\n                \n                # Update progress bar\n                pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n                \n                # Aggressive memory cleanup every few batches\n                if batch_idx % 10 == 0:\n                    del outputs, loss, patches\n                    gc.collect()\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n            \n            # Calculate epoch metrics\n            if valid_batches > 0:\n                train_loss = running_loss / valid_batches\n                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}\")\n            else:\n                print(f\"Epoch {epoch+1}/{num_epochs}, No valid batches!\")\n                continue\n                \n            # Save checkpoint every epoch\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': train_loss,\n            }, f\"{checkpoint_dir}/model_epoch_{epoch+1}.pt\")\n            \n            # VALIDATION PHASE\n            if val_loader:\n                val_loss = validate_memory_efficient(model, val_loader, criterion, device)\n                scheduler.step(val_loss)\n                \n                # Early stopping logic\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    patience_counter = 0\n                    torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pt\")\n                    print(f\"New best model saved with val_loss: {val_loss:.6f}\")\n                else:\n                    patience_counter += 1\n                    if patience_counter >= 3:  # Adjust patience as needed\n                        print(\"Early stopping triggered!\")\n                        break\n    \n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        # Save emergency checkpoint\n        torch.save(model.state_dict(), f\"{checkpoint_dir}/emergency_model.pt\")\n        raise\n        \n    return model\n\ndef validate_memory_efficient(model, val_loader, criterion, device):\n    \"\"\"Memory-efficient validation function.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    valid_batches = 0\n    \n    with torch.no_grad():\n        pbar = tqdm(val_loader)\n        pbar.set_description(f\"Validating\")\n        \n        for patches, _ in pbar:\n            if patches.numel() == 0:\n                continue\n                \n            patches = patches.to(device, non_blocking=True)\n            \n            # Using mixed precision even for validation\n            with amp.autocast():\n                outputs = model(patches)\n                loss = criterion(outputs, patches)\n                \n            running_loss += loss.item()\n            valid_batches += 1\n            \n            # Update progress bar\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n            \n            # Clean up\n            del outputs, patches, loss\n    \n    if valid_batches > 0:\n        val_loss = running_loss / valid_batches\n        print(f\"Validation Loss: {val_loss:.6f}\")\n        return val_loss\n    else:\n        print(\"No valid validation batches!\")\n        return float('inf')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SCAE\nimport torch.nn as nn\nclass SCAE(nn.Module):\n    def __init__(self, normalization_type=\"batch_norm\", use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n        super(SCAE, self).__init__()\n\n        def norm_layer(num_features):\n            if normalization_type == \"batch_norm\":\n                return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\":\n                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\":\n                return nn.LayerNorm([num_features, 12, 12])  # Updated for 12x12 feature maps\n            else:\n                return nn.Identity()\n\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n\n        # Encoder: Input 105x105 -> Output 12x12\n        self.encoder = nn.Sequential(\n            # Layer 1: 105x105 -> 48x48\n            nn.Conv2d(1, 64, kernel_size=11, stride=2, padding=0),\n            norm_layer(64),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 2: 48x48 -> 24x24\n            nn.MaxPool2d(2, 2),\n            \n            # Layer 3: 24x24 -> 24x24\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            norm_layer(128),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 4: 24x24 -> 12x12 (added to get 12x12 output)\n            nn.MaxPool2d(2, 2)\n        )\n        \n        # Decoder: Input 12x12 -> Output 105x105\n        self.decoder = nn.Sequential(\n            # Layer 1: 12x12 -> 24x24\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n            norm_layer(64),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 2: 24x24 -> 48x48\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n            norm_layer(32),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 3: 48x48 -> 105x105 (with precise output size)\n            nn.ConvTranspose2d(32, 1, kernel_size=14, stride=2, padding=2, output_padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Pass through encoder\n        if x.size(1) == 3:\n            # Use standard RGB to grayscale conversion: 0.299*R + 0.587*G + 0.114*B\n            x = 0.299 * x[:, 0:1] + 0.587 * x[:, 1:2] + 0.114 * x[:, 2:3]\n        for layer in self.encoder:\n            x = layer(x)\n            # print(x.shape)\n\n        for layer in self.decoder:\n            x = layer(x)\n            # print(x.shape)\n            \n        return x","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nscae = SCAE().to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(scae(sample[0].to(device)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SCAE (Tu's version)\nimport torch\nimport torch.nn as nn\nclass SCAE(nn.Module):\n    def __init__(self, normalization_type=\"batch_norm\", use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n        super(SCAE, self).__init__()\n        def norm_layer(num_features):\n            if normalization_type == \"batch_norm\": return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\": return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\": return nn.LayerNorm([num_features, 48, 48])\n            else: return nn.Identity()\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n\n        # Encoder: conv1 → pool1 → conv2\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=11, stride=2, padding=0)  # 105x105 → 48x48\n        self.norm1 = norm_layer(64)\n        self.act1 = activation_layer()\n        self.drop1 = dropout_layer()\n        self.pool1 = nn.MaxPool2d(2, 2, return_indices=True)                # 48x48 → 24x24\n\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) # 24x24 → 24x24\n        self.norm2 = norm_layer(128)\n        self.act2 = activation_layer()\n        self.drop2 = dropout_layer()\n\n        # Decoder: deconv1 → unpool1 → deconv2\n        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1) # 24x24 → 24x24\n        self.norm3 = norm_layer(64)\n        self.act3 = activation_layer()\n        self.drop3 = dropout_layer()\n        self.unpool1 = nn.MaxUnpool2d(2, 2)                                           # 24x24 → 48x48\n\n        self.deconv2 = nn.ConvTranspose2d(64, 1, kernel_size=11, stride=2, padding=0) # 48x48 → 105x105\n        # No normalization/activation after last layer for output\n        self.final_act = nn.Sigmoid()\n\n    def forward(self, x):\n        # Encoder\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.act1(x)\n        x = self.drop1(x)\n        x, indices = self.pool1(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.act2(x)\n        x = self.drop2(x)\n        # Decoder\n        x = self.deconv1(x)\n        x = self.norm3(x)\n        x = self.act3(x)\n        x = self.drop3(x)\n        x = self.unpool1(x, indices, output_size=torch.Size([x.size(0), x.size(1), 48, 48]))\n        x = self.deconv2(x)\n        x = self.final_act(x)\n        return x\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SCAE().to(device)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del model, optimizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create model and train with memory optimization\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SCAE().to(device)\ntrained_model = train_memory_efficient_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=5,\n    learning_rate=0.0001\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/checkpoint\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom functools import partial\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nimport os\nimport gc\n\n\ndef clean_gpu_memory():\n    \"\"\"Clean up GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef evaluate_reconstruction(model, dataloader, device, num_samples=10, save_path=None):\n    \"\"\"\n    Evaluate reconstruction quality with metrics (SSIM, PSNR, MSE) \n    and visualize original vs reconstructed images\n    \"\"\"\n    model.eval()\n    \n    # Initialize metrics\n    total_mse = 0\n    total_ssim = 0\n    total_psnr = 0\n    count = 0\n    \n    # Get a batch of images for visualization\n    vis_images = []\n    vis_recons = []\n    \n    with torch.no_grad():\n        # Get one batch for metrics and visualization\n        for images, _ in dataloader:\n            if images.numel() == 0:\n                continue\n                \n            images = images.to(device)\n            reconstructions = model(images)\n            \n            # Calculate MSE\n            mse = torch.mean((reconstructions - images) ** 2).item()\n            total_mse += mse\n            \n            # Convert tensors to numpy for SSIM and PSNR calculation\n            images_np = images.cpu().numpy()\n            recons_np = reconstructions.cpu().numpy()\n            \n            # Calculate metrics for each image in the batch\n            batch_size = images_np.shape[0]\n            for i in range(min(batch_size, num_samples - count)):\n                # Get single image (remove batch and channel dimensions)\n                img = np.squeeze(images_np[i])\n                recon = np.squeeze(recons_np[i])\n                \n                # Calculate SSIM (structural similarity index)\n                ssim_val = ssim(img, recon, data_range=1.0)\n                total_ssim += ssim_val\n                \n                # Calculate PSNR (peak signal-to-noise ratio)\n                psnr_val = psnr(img, recon, data_range=1.0)\n                total_psnr += psnr_val\n                \n                # Save images for visualization\n                if count < num_samples:\n                    vis_images.append(images[i])\n                    vis_recons.append(reconstructions[i])\n                \n                count += 1\n                \n                if count >= num_samples:\n                    break\n            \n            if count >= num_samples:\n                break\n    \n    # Calculate averages\n    avg_mse = total_mse / (count // batch_size + 1)\n    avg_ssim = total_ssim / count\n    avg_psnr = total_psnr / count\n    \n    # Print metrics\n    print(f\"Reconstruction Metrics:\")\n    print(f\"  Average MSE: {avg_mse:.4f}\")\n    print(f\"  Average SSIM: {avg_ssim:.4f} (higher is better, max 1.0)\")\n    print(f\"  Average PSNR: {avg_psnr:.2f} dB (higher is better)\")\n    \n    # Visualize original vs reconstructed images\n    if len(vis_images) > 0:\n        # Combine original and reconstructed images for side-by-side comparison\n        vis_combined = []\n        for img, recon in zip(vis_images, vis_recons):\n            vis_combined.extend([img, recon])\n        \n        # Create a grid of images\n        grid = make_grid(vis_combined, nrow=2, normalize=True, pad_value=1)\n        plt.figure(figsize=(12, num_samples*2))\n        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n        plt.axis('off')\n        plt.title('Original (left) vs Reconstructed (right)')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'reconstruction_comparison.png'), bbox_inches='tight')\n            print(f\"Saved reconstruction comparison to {save_path}\")\n        plt.show()\n    \n    return {\n        'mse': avg_mse,\n        'ssim': avg_ssim,\n        'psnr': avg_psnr\n    }\n\ndef evaluate_classification(model, dataloader, device, num_classes, save_path=None):\n    \"\"\"\n    Evaluate classification performance if your SCAE includes classification capability\n    \"\"\"\n    model.eval()\n    \n    # Check if model has a classify method or classification head\n    if not hasattr(model, 'classify') and not hasattr(model, 'classification_head'):\n        print(\"Model doesn't appear to have classification capability\")\n        return None\n        \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in dataloader:\n            if images.numel() == 0:\n                continue\n                \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # Get predictions - adapt this to your model's API\n            try:\n                if hasattr(model, 'classify'):\n                    preds = model.classify(images)\n                else:\n                    # Assume model returns (reconstructions, classifications) if called with return_classifications=True\n                    _, preds = model(images, return_classifications=True)\n                \n                # Convert to class indices\n                _, predicted = torch.max(preds.data, 1)\n                \n                # Store predictions and labels\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                \n            except Exception as e:\n                print(f\"Error during classification evaluation: {e}\")\n                return None\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Classification Accuracy: {accuracy:.4f}\")\n    \n    # Create confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    \n    if save_path:\n        plt.savefig(os.path.join(save_path, 'confusion_matrix.png'), bbox_inches='tight')\n    plt.show()\n    \n    return {\n        'accuracy': accuracy,\n        'confusion_matrix': cm\n    }\n\ndef visualize_latent_space(model, dataloader, device, save_path=None):\n    \"\"\"\n    Visualize the latent space of the autoencoder using t-SNE\n    \"\"\"\n    try:\n        from sklearn.manifold import TSNE\n        \n        # Get encoder output for a batch of images\n        model.eval()\n        latent_vectors = []\n        labels = []\n        \n        with torch.no_grad():\n            for images, batch_labels in dataloader:\n                if images.numel() == 0:\n                    continue\n                    \n                images = images.to(device)\n                \n                # Get latent vectors - adapt this to your model's API\n                if hasattr(model, 'encode'):\n                    latent = model.encode(images)\n                else:\n                    # Try to extract the latent representation from your model\n                    # This depends on your model's architecture\n                    x = images\n                    for layer in model.encoder:\n                        x = layer(x)\n                    latent = x\n                \n                # Flatten the latent vectors\n                batch_size = images.size(0)\n                latent_flat = latent.view(batch_size, -1).cpu().numpy()\n                \n                latent_vectors.append(latent_flat)\n                labels.append(batch_labels.numpy())\n                \n                if len(latent_vectors) * batch_size >= 1000:  # Limit number of points for t-SNE\n                    break\n        \n        # Concatenate batches\n        latent_vectors = np.vstack(latent_vectors)\n        labels = np.concatenate(labels)\n        \n        # Apply t-SNE\n        tsne = TSNE(n_components=2, random_state=42)\n        latent_tsne = tsne.fit_transform(latent_vectors)\n        \n        # Plot t-SNE results\n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\n        plt.colorbar(scatter, label='Font Class')\n        plt.title('t-SNE Visualization of Latent Space')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_tsne.png'), bbox_inches='tight')\n        plt.show()\n        \n        return latent_tsne, labels\n        \n    except Exception as e:\n        print(f\"Error in latent space visualization: {e}\")\n        return None, None\n\ndef generate_samples_from_latent(model, num_samples=10, latent_dim=128, device=None, save_path=None):\n    \"\"\"\n    Generate new images by sampling from the latent space (like in VAE)\n    \"\"\"\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model.eval()\n    \n    try:\n        # Generate random latent vectors\n        z = torch.randn(num_samples, latent_dim).to(device)\n        \n        # Generate images\n        with torch.no_grad():\n            if hasattr(model, 'decode'):\n                generated = model.decode(z)\n            else:\n                print(\"Model doesn't have a decode method. Cannot generate samples.\")\n                return None\n        \n        # Visualize generated images\n        grid = make_grid(generated.cpu(), nrow=5, normalize=True, pad_value=1)\n        plt.figure(figsize=(12, 6))\n        plt.imshow(grid.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.title('Generated Samples from Random Latent Vectors')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'generated_samples.png'), bbox_inches='tight')\n        plt.show()\n        \n        return generated\n        \n    except Exception as e:\n        print(f\"Error generating samples from latent space: {e}\")\n        print(f\"Your model might not support the VAE-style generation.\")\n        return None\n\ndef interpolate_latent_space(model, dataloader, device, steps=10, save_path=None):\n    \"\"\"\n    Interpolate between two points in latent space and decode them\n    \"\"\"\n    model.eval()\n    \n    try:\n        # Get two images from the dataset\n        for images, _ in dataloader:\n            if images.shape[0] >= 2 and images.numel() > 0:\n                break\n        else:\n            print(\"Couldn't find suitable images for interpolation\")\n            return None\n            \n        # Select two images\n        img1 = images[0:1].to(device)\n        img2 = images[1:2].to(device)\n        \n        # Get latent representations\n        with torch.no_grad():\n            if hasattr(model, 'encode'):\n                z1 = model.encode(img1)\n                z2 = model.encode(img2)\n            else:\n                # Try to extract the latent representation\n                x1 = img1\n                x2 = img2\n                for layer in model.encoder:\n                    x1 = layer(x1)\n                    x2 = layer(x2)\n                z1 = x1\n                z2 = x2\n        \n        # Interpolate between the two latent vectors\n        interpolations = []\n        with torch.no_grad():\n            for alpha in np.linspace(0, 1, steps):\n                z_interp = alpha * z1 + (1 - alpha) * z2\n                \n                # Decode the interpolated latent vector\n                if hasattr(model, 'decode'):\n                    decoded = model.decode(z_interp)\n                else:\n                    decoded = model.decoder(z_interp)\n                \n                interpolations.append(decoded)\n        \n        # Combine original images and interpolation\n        all_images = [img1.cpu()]\n        all_images.extend([interp.cpu() for interp in interpolations])\n        all_images.append(img2.cpu())\n        \n        # Create a grid\n        grid = make_grid(torch.cat(all_images), nrow=steps+2, normalize=True, pad_value=1)\n        plt.figure(figsize=(15, 4))\n        plt.imshow(grid.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.title('Latent Space Interpolation')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_interpolation.png'), bbox_inches='tight')\n        plt.show()\n        \n        return interpolations\n        \n    except Exception as e:\n        print(f\"Error during latent space interpolation: {e}\")\n        return None\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create output directory for visualizations\n    save_dir = \"/kaggle/working/evaluation_results\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define paths\n    model_path = \"/kaggle/working/best_model.pt\"\n    jpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\n    bcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\n    label_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\n    \n    try:\n        # Load your trained model\n        model = SCAE()\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        model.to(device)\n        model.eval()\n        print(\"Model loaded successfully\")\n        \n        # Load dataset\n        test_dataset = CombinedImageDataset(\n            jpeg_dir=jpeg_dir, \n            bcf_file=bcf_file, \n            label_file=label_file,\n            num_patch=1,  # Use fewer patches to save memory\n            patch_size=(105, 105)\n        )\n        \n        # Create test loader with smaller batch size\n        collate_fn = partial(patch_collate_fn, patch_size_tuple=(105, 105))\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=32,  # Use a smaller batch size\n            shuffle=True,   # Shuffle to get diverse examples\n            num_workers=2,\n            collate_fn=collate_fn\n        )\n        \n        print(\"Dataset and DataLoader prepared\")\n        \n        # Evaluate reconstruction quality\n        print(\"\\n1. Evaluating reconstruction quality...\")\n        recon_metrics = evaluate_reconstruction(\n            model, test_loader, device, num_samples=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # If model has classification capability, evaluate it\n        print(\"\\n2. Evaluating classification performance...\")\n        try:\n            class_metrics = evaluate_classification(\n                model, test_loader, device, num_classes=2383, save_path=save_dir\n            )\n        except:\n            print(\"Classification evaluation skipped - model may not support classification\")\n        clean_gpu_memory()\n        \n        # Visualize latent space\n        print(\"\\n3. Visualizing latent space with t-SNE...\")\n        latent_tsne, labels = visualize_latent_space(\n            model, test_loader, device, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Generate samples from random latent vectors (if model supports it)\n        print(\"\\n4. Generating samples from latent space...\")\n        try:\n            # Determine latent dimension from model structure\n            # This is just a guess - adapt to your model\n            latent_dim = 128\n            generated_samples = generate_samples_from_latent(\n                model, num_samples=16, latent_dim=latent_dim, device=device, save_path=save_dir\n            )\n        except:\n            print(\"Sample generation skipped - model may not support VAE-style generation\")\n        clean_gpu_memory()\n        \n        # Interpolate in latent space\n        print(\"\\n5. Creating latent space interpolation...\")\n        try:\n            interpolations = interpolate_latent_space(\n                model, test_loader, device, steps=8, save_path=save_dir\n            )\n        except:\n            print(\"Latent interpolation skipped - model may not support this operation\")\n        clean_gpu_memory()\n        \n        print(f\"\\nEvaluation complete! Results saved to {save_dir}\")\n        \n    except Exception as e:\n        print(f\"Error during evaluation: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluation code\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nimport os\nimport gc\nfrom functools import partial\nfrom torch.utils.data import DataLoader\n\ndef clean_gpu_memory():\n    \"\"\"Clean up GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef evaluate_reconstruction(model, dataloader, device, num_samples=10, save_path=None):\n    \"\"\"\n    Evaluate reconstruction quality with metrics (SSIM, PSNR, MSE)\n    and visualize original vs reconstructed images\n    \"\"\"\n    model.eval()\n    \n    # Initialize metrics\n    total_mse = 0\n    total_ssim = 0\n    total_psnr = 0\n    count = 0\n    \n    # Get a batch of images for visualization\n    vis_images = []\n    vis_recons = []\n    \n    with torch.no_grad():\n        # Get one batch for metrics and visualization\n        for images, _ in dataloader:\n            if images.numel() == 0:\n                continue\n                \n            images = images.to(device)\n            reconstructions = model(images)\n            \n            # Calculate MSE\n            mse = torch.mean((reconstructions - images) ** 2).item()\n            total_mse += mse\n            \n            # Convert tensors to numpy for SSIM and PSNR calculation\n            images_np = images.cpu().numpy()\n            recons_np = reconstructions.cpu().numpy()\n            \n            # Calculate metrics for each image in the batch\n            batch_size = images_np.shape[0]\n            for i in range(min(batch_size, num_samples - count)):\n                # Get single image (remove batch and channel dimensions)\n                img = np.squeeze(images_np[i])\n                recon = np.squeeze(recons_np[i])\n                \n                # Calculate SSIM (structural similarity index)\n                ssim_val = ssim(img, recon, data_range=1.0)\n                total_ssim += ssim_val\n                \n                # Calculate PSNR (peak signal-to-noise ratio)\n                psnr_val = psnr(img, recon, data_range=1.0)\n                total_psnr += psnr_val\n                \n                # Save images for visualization\n                if count < num_samples:\n                    vis_images.append(images[i])\n                    vis_recons.append(reconstructions[i])\n                \n                count += 1\n                \n                if count >= num_samples:\n                    break\n            \n            if count >= num_samples:\n                break\n    \n    # Calculate averages\n    avg_mse = total_mse / (count // batch_size + 1)\n    avg_ssim = total_ssim / count\n    avg_psnr = total_psnr / count\n    \n    # Print metrics\n    print(f\"Reconstruction Metrics:\")\n    print(f\"  Average MSE: {avg_mse:.4f}\")\n    print(f\"  Average SSIM: {avg_ssim:.4f} (higher is better, max 1.0)\")\n    print(f\"  Average PSNR: {avg_psnr:.2f} dB (higher is better)\")\n    \n    # Visualize original vs reconstructed images\n    if len(vis_images) > 0:\n        # Combine original and reconstructed images for side-by-side comparison\n        vis_combined = []\n        for img, recon in zip(vis_images, vis_recons):\n            vis_combined.extend([img, recon])\n        \n        # Create a grid of images\n        grid = make_grid(vis_combined, nrow=2, normalize=True, pad_value=1)\n        plt.figure(figsize=(12, num_samples*2))\n        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n        plt.axis('off')\n        plt.title('Original (left) vs Reconstructed (right)')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'reconstruction_comparison.png'), bbox_inches='tight')\n            print(f\"Saved reconstruction comparison to {save_path}\")\n        plt.show()\n    \n    return {\n        'mse': avg_mse,\n        'ssim': avg_ssim,\n        'psnr': avg_psnr\n    }\n\ndef extract_latent_features(model, x):\n    \"\"\"\n    Extract latent features from the model by running through the encoder part only\n    Adapted specifically for the new SCAE architecture\n    \"\"\"\n    # Apply encoder operations manually based on your model's structure\n    x = model.conv1(x)\n    x = model.norm1(x)\n    x = model.act1(x)\n    x = model.drop1(x)\n    x, indices = model.pool1(x)\n    x = model.conv2(x)\n    x = model.norm2(x)\n    x = model.act2(x)\n    x = model.drop2(x)\n    return x, indices\n\ndef generate_from_latent(model, latent, indices, output_size):\n    \"\"\"\n    Generate images from latent features by running through the decoder part only\n    Adapted specifically for the new SCAE architecture\n    \"\"\"\n    # Apply decoder operations manually based on your model's structure\n    x = model.deconv1(latent)\n    x = model.norm3(x)\n    x = model.act3(x)\n    x = model.drop3(x)\n    x = model.unpool1(x, indices, output_size=output_size)\n    x = model.deconv2(x)\n    x = model.final_act(x)\n    return x\n\ndef visualize_latent_space(model, dataloader, device, save_path=None):\n    \"\"\"\n    Visualize the latent space of the autoencoder using t-SNE\n    \"\"\"\n    try:\n        from sklearn.manifold import TSNE\n        \n        # Get encoder output for a batch of images\n        model.eval()\n        latent_vectors = []\n        orig_shapes = []\n        indices_list = []\n        labels = []\n        \n        with torch.no_grad():\n            for images, batch_labels in dataloader:\n                if images.numel() == 0:\n                    continue\n                    \n                images = images.to(device)\n                \n                # Get latent vectors using our custom function\n                latent, indices = extract_latent_features(model, images)\n                \n                # Flatten the latent vectors for t-SNE\n                batch_size = images.size(0)\n                latent_flat = latent.view(batch_size, -1).cpu().numpy()\n                \n                latent_vectors.append(latent_flat)\n                labels.append(batch_labels.numpy())\n                \n                # Store original shapes and indices for potential reconstruction\n                for i in range(batch_size):\n                    orig_shapes.append(torch.Size([1, latent.size(1), latent.size(2), latent.size(3)]))\n                    indices_list.append(indices[i:i+1])\n                \n                if len(latent_vectors) * batch_size >= 1000:  # Limit number of points for t-SNE\n                    break\n        \n        # Concatenate batches\n        latent_vectors = np.vstack(latent_vectors)\n        labels = np.concatenate(labels)\n        \n        # Apply t-SNE\n        tsne = TSNE(n_components=2, random_state=42)\n        latent_tsne = tsne.fit_transform(latent_vectors)\n        \n        # Plot t-SNE results\n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\n        plt.colorbar(scatter, label='Font Class')\n        plt.title('t-SNE Visualization of Latent Space')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_tsne.png'), bbox_inches='tight')\n        plt.show()\n        \n        return latent_tsne, labels, latent_vectors, orig_shapes, indices_list\n        \n    except Exception as e:\n        print(f\"Error in latent space visualization: {e}\")\n        return None, None, None, None, None\n\ndef interpolate_latent_space(model, dataloader, device, steps=10, save_path=None):\n    \"\"\"\n    Interpolate between two points in latent space and decode them\n    Adapted for the new SCAE architecture\n    \"\"\"\n    model.eval()\n    \n    try:\n        # Get two images from the dataset\n        for images, _ in dataloader:\n            if images.shape[0] >= 2 and images.numel() > 0:\n                break\n        else:\n            print(\"Couldn't find suitable images for interpolation\")\n            return None\n            \n        # Select two images\n        img1 = images[0:1].to(device)\n        img2 = images[1:2].to(device)\n        \n        # Get latent representations and indices for unpooling\n        with torch.no_grad():\n            z1, indices1 = extract_latent_features(model, img1)\n            z2, indices2 = extract_latent_features(model, img2)\n        \n        # Get output size for the unpool operation\n        output_size = torch.Size([1, z1.size(1), 48, 48])  # Based on your architecture\n        \n        # Interpolate between the two latent vectors\n        interpolations = []\n        with torch.no_grad():\n            for alpha in np.linspace(0, 1, steps):\n                z_interp = alpha * z1 + (1 - alpha) * z2\n                \n                # Use indices from first image for interpolation (simplification)\n                decoded = generate_from_latent(model, z_interp, indices1, output_size)\n                \n                interpolations.append(decoded)\n        \n        # Combine original images and interpolation\n        all_images = []\n        with torch.no_grad():\n            # Add the first original image\n            all_images.append(img1.cpu())\n            # Add interpolations\n            all_images.extend([interp.cpu() for interp in interpolations])\n            # Add the second original image\n            all_images.append(img2.cpu())\n        \n        # Create a grid\n        grid = make_grid(torch.cat(all_images), nrow=steps+2, normalize=True, pad_value=1)\n        plt.figure(figsize=(15, 4))\n        plt.imshow(grid.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.title('Latent Space Interpolation')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_interpolation.png'), bbox_inches='tight')\n        plt.show()\n        \n        return interpolations\n        \n    except Exception as e:\n        print(f\"Error during latent space interpolation: {e}\")\n        return None\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create output directory for visualizations\n    save_dir = \"evaluation_results\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define paths\n    model_path = \"/kaggle/working/best_model.pt\"\n    jpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\n    bcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\n    label_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\n    \n    try:\n        # Import necessary modules - add these imports here to avoid issues in Kaggle\n        # Load your trained model\n        model = SCAE()\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        model.to(device)\n        model.eval()\n        print(\"Model loaded successfully\")\n        \n        # Load dataset\n        combined_dataset = CombinedImageDataset(\n            jpeg_dir=jpeg_dir,\n            bcf_file=bcf_file,\n            label_file=label_file,\n            num_patch=3,  # Reduced from 3\n        )\n    \n        # Create memory-optimized dataloaders with smaller batch size\n        test_loader, val_loader = create_patch_dataloaders(\n            combined_dataset,\n            batch_size=512,  # Reduced from 1024\n            num_workers=2,   # Reduced from 4\n            val_split=0\n        )\n        # test_dataset = CombinedImageDataset(\n        #     jpeg_dir=jpeg_dir, \n        #     bcf_file=bcf_file, \n        #     label_file=label_file,\n        #     num_patch=3,  # Use fewer patches to save memory\n        #     patch_size=(105, 105)\n        # )\n        \n        # # Create test loader with smaller batch size\n        # collate_fn = partial(patch_collate_fn, patch_size_tuple=(105, 105))\n        # test_loader = DataLoader(\n        #     test_dataset,\n        #     batch_size=128,  # Use a smaller batch size\n        #     shuffle=True,   # Shuffle to get diverse examples\n        #     num_workers=2,\n        #     collate_fn=collate_fn\n        # )\n        \n        print(\"Dataset and DataLoader prepared\")\n        \n        # Evaluate reconstruction quality\n        print(\"\\n1. Evaluating reconstruction quality...\")\n        recon_metrics = evaluate_reconstruction(\n            model, test_loader, device, num_samples=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Visualize latent space\n        print(\"\\n2. Visualizing latent space with t-SNE...\")\n        latent_tsne, labels, latent_vectors, orig_shapes, indices_list = visualize_latent_space(\n            model, test_loader, device, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Interpolate in latent space\n        print(\"\\n3. Creating latent space interpolation...\")\n        interpolations = interpolate_latent_space(\n            model, test_loader, device, steps=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        print(f\"\\nEvaluation complete! Results saved to {save_dir}\")\n        \n    except Exception as e:\n        print(f\"Error during evaluation: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nimport os\nimport gc\nfrom functools import partial\nfrom torch.utils.data import DataLoader\n\ndef clean_gpu_memory():\n    \"\"\"Clean up GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef evaluate_reconstruction(model, dataloader, device, num_samples=10, save_path=None):\n    \"\"\"\n    Evaluate reconstruction quality with metrics (SSIM, PSNR, MSE)\n    and visualize original vs reconstructed images\n    \"\"\"\n    model.eval()\n    \n    # Initialize metrics\n    total_mse = 0\n    total_ssim = 0\n    total_psnr = 0\n    count = 0\n    \n    # Get a batch of images for visualization\n    vis_images = []\n    vis_recons = []\n    \n    with torch.no_grad():\n        # Get one batch for metrics and visualization\n        for images, _ in dataloader:\n            # Skip empty batches\n            if images.numel() == 0:\n                continue\n                \n            # Ensure we have the right shape for processing\n            if len(images.shape) == 3:\n                images = images.unsqueeze(1)  # Add channel dimension if missing\n                \n            images = images.to(device)\n            reconstructions = model(images)\n            \n            # Calculate MSE\n            mse = torch.mean((reconstructions - images) ** 2).item()\n            total_mse += mse\n            \n            # Convert tensors to numpy for SSIM and PSNR calculation\n            images_np = images.cpu().numpy()\n            recons_np = reconstructions.cpu().numpy()\n            \n            # Calculate metrics for each image in the batch\n            batch_size = images_np.shape[0]\n            for i in range(min(batch_size, num_samples - count)):\n                # Get single image (remove batch and channel dimensions if needed)\n                img = np.squeeze(images_np[i])\n                recon = np.squeeze(recons_np[i])\n                \n                # If image has 3 channels, convert to grayscale for SSIM/PSNR calculation\n                if len(img.shape) == 3 and img.shape[0] == 3:\n                    img_gray = np.mean(img, axis=0)\n                    recon_gray = np.mean(recon, axis=0)\n                    ssim_val = ssim(img_gray, recon_gray, data_range=1.0)\n                    psnr_val = psnr(img_gray, recon_gray, data_range=1.0)\n                else:\n                    # Ensure images are properly squeezed but retain needed dimensions\n                    if len(img.shape) > 2:\n                        img = np.squeeze(img)\n                    if len(recon.shape) > 2:\n                        recon = np.squeeze(recon)\n                    \n                    # Handle case where squeeze removed too many dimensions\n                    if len(img.shape) == 0:\n                        img = img.reshape(1, 1)\n                    if len(recon.shape) == 0:\n                        recon = recon.reshape(1, 1)\n                        \n                    ssim_val = ssim(img, recon, data_range=1.0)\n                    psnr_val = psnr(img, recon, data_range=1.0)\n                \n                total_ssim += ssim_val\n                total_psnr += psnr_val\n                \n                # Save images for visualization\n                if count < num_samples:\n                    vis_images.append(images[i])\n                    vis_recons.append(reconstructions[i])\n                \n                count += 1\n                \n                if count >= num_samples:\n                    break\n            \n            if count >= num_samples:\n                break\n    \n    # Calculate averages\n    if count > 0:\n        avg_mse = total_mse / (count // batch_size + 1)\n        avg_ssim = total_ssim / count\n        avg_psnr = total_psnr / count\n    else:\n        avg_mse = float('nan')\n        avg_ssim = float('nan')\n        avg_psnr = float('nan')\n        print(\"Warning: No valid samples were processed in evaluation\")\n    \n    # Print metrics\n    print(f\"Reconstruction Metrics:\")\n    print(f\"  Average MSE: {avg_mse:.4f}\")\n    print(f\"  Average SSIM: {avg_ssim:.4f} (higher is better, max 1.0)\")\n    print(f\"  Average PSNR: {avg_psnr:.2f} dB (higher is better)\")\n    \n    # Visualize original vs reconstructed images\n    if len(vis_images) > 0:\n        # Combine original and reconstructed images for side-by-side comparison\n        vis_combined = []\n        for img, recon in zip(vis_images, vis_recons):\n            vis_combined.extend([img, recon])\n        \n        # Create a grid of images\n        grid = make_grid(vis_combined, nrow=2, normalize=True, pad_value=1)\n        plt.figure(figsize=(12, num_samples*2))\n        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n        plt.axis('off')\n        plt.title('Original (left) vs Reconstructed (right)')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'reconstruction_comparison.png'), bbox_inches='tight')\n            print(f\"Saved reconstruction comparison to {save_path}\")\n        plt.show()\n    \n    return {\n        'mse': avg_mse,\n        'ssim': avg_ssim,\n        'psnr': avg_psnr\n    }\n\ndef safely_extract_latent_features(model, x):\n    \"\"\"\n    Extract latent features from the model - with safety checks and error handling\n    Handles various model architectures by attempting different approaches\n    \"\"\"\n    try:\n        # Method 1: Direct call to encoder if available\n        if hasattr(model, 'encode'):\n            return model.encode(x)\n        \n        # Method 2: Manual application of encoder layers as in original code\n        # Capture as much as possible through a try/except approach\n        try:\n            x = model.conv1(x)\n            x = model.norm1(x) if hasattr(model, 'norm1') else x\n            x = model.act1(x) if hasattr(model, 'act1') else torch.relu(x)\n            x = model.drop1(x) if hasattr(model, 'drop1') else x\n            \n            # Handle pooling layers with or without indices\n            if hasattr(model, 'pool1'):\n                if 'MaxPool' in model.pool1.__class__.__name__ and 'return_indices=True' in str(model.pool1):\n                    x, indices = model.pool1(x)\n                else:\n                    x = model.pool1(x)\n                    indices = None\n            else:\n                indices = None\n                \n            # Continue with more layers if they exist\n            if hasattr(model, 'conv2'):\n                x = model.conv2(x)\n                x = model.norm2(x) if hasattr(model, 'norm2') else x\n                x = model.act2(x) if hasattr(model, 'act2') else torch.relu(x)\n                x = model.drop2(x) if hasattr(model, 'drop2') else x\n            \n            return x, indices\n            \n        except Exception as e:\n            print(f\"Warning: Error in manual feature extraction: {e}\")\n            \n            # Method 3: Simplified approach - extract features up to a bottleneck\n            # This is a more generic approach that can work with different model architectures\n            features = []\n            indices_list = []\n            \n            # Get all model's modules\n            for name, module in model.named_modules():\n                # Skip the top level module (the model itself)\n                if name == '':\n                    continue\n                    \n                # Apply the layer\n                try:\n                    if isinstance(module, nn.MaxPool2d) and module.return_indices:\n                        x, indices = module(x)\n                        indices_list.append(indices)\n                    else:\n                        x = module(x)\n                        \n                    # Collect features at bottleneck or certain layers\n                    if isinstance(module, (nn.Conv2d, nn.Linear)) and len(features) < 5:\n                        features.append(x)\n                        \n                    # If we've reached the bottleneck/middle of the network, stop\n                    if 'bottleneck' in name.lower() or 'latent' in name.lower():\n                        break\n                        \n                except Exception:\n                    continue\n            \n            # If we've collected features, return the last one\n            if features:\n                return features[-1], indices_list[-1] if indices_list else None\n    \n    except Exception as e:\n        print(f\"Error in feature extraction: {e}\")\n        # Return input as fallback (will produce poor results but prevents crashes)\n        return x, None\n\ndef safely_generate_from_latent(model, latent, indices=None, output_size=None):\n    \"\"\"\n    Generate images from latent features with safety checks and error handling\n    Handles various model architectures by attempting different approaches\n    \"\"\"\n    try:\n        # Method 1: Direct call to decoder if available\n        if hasattr(model, 'decode'):\n            return model.decode(latent)\n        \n        # Method 2: Manual application of decoder layers as in original code\n        try:\n            x = latent\n            \n            # Apply decoder operations based on model attributes\n            if hasattr(model, 'deconv1'):\n                x = model.deconv1(x)\n                x = model.norm3(x) if hasattr(model, 'norm3') else x\n                x = model.act3(x) if hasattr(model, 'act3') else torch.relu(x)\n                x = model.drop3(x) if hasattr(model, 'drop3') else x\n            \n            # Handle unpooling layers if they exist\n            if hasattr(model, 'unpool1') and indices is not None:\n                x = model.unpool1(x, indices, output_size=output_size)\n            \n            # Continue with more layers if they exist\n            if hasattr(model, 'deconv2'):\n                x = model.deconv2(x)\n            \n            # Apply final activation if it exists\n            if hasattr(model, 'final_act'):\n                x = model.final_act(x)\n            elif hasattr(model, 'sigmoid'):\n                x = model.sigmoid(x)\n            else:\n                x = torch.sigmoid(x)  # Default to sigmoid for image generation\n                \n            return x\n            \n        except Exception as e:\n            print(f\"Warning: Error in manual latent generation: {e}\")\n            \n            # Method 3: Forward pass through the full model as fallback\n            # This is a reasonable fallback that should work with most autoencoders\n            return model(latent)\n    \n    except Exception as e:\n        print(f\"Error in generating from latent: {e}\")\n        # Return latent as fallback (will produce poor results but prevents crashes)\n        return latent\n\ndef visualize_latent_space(model, dataloader, device, save_path=None):\n    \"\"\"\n    Visualize the latent space of the autoencoder using t-SNE\n    \"\"\"\n    try:\n        from sklearn.manifold import TSNE\n        \n        # Get encoder output for a batch of images\n        model.eval()\n        latent_vectors = []\n        orig_shapes = []\n        indices_list = []\n        labels = []\n        \n        with torch.no_grad():\n            for images, batch_labels in dataloader:\n                # Skip empty batches\n                if images.numel() == 0:\n                    continue\n                    \n                # Ensure we have the right shape for processing\n                if len(images.shape) == 3:\n                    images = images.unsqueeze(1)  # Add channel dimension if missing\n                    \n                images = images.to(device)\n                \n                # Get latent vectors using our safe extraction function\n                latent, indices = safely_extract_latent_features(model, images)\n                \n                # Flatten the latent vectors for t-SNE\n                batch_size = images.size(0)\n                \n                # Handle case where latent is not a tensor\n                if not isinstance(latent, torch.Tensor):\n                    print(\"Warning: Latent features are not a tensor, skipping batch\")\n                    continue\n                \n                # Flatten the latent representations\n                if len(latent.shape) > 2:\n                    latent_flat = latent.reshape(batch_size, -1).cpu().numpy()\n                else:\n                    latent_flat = latent.cpu().numpy()\n                \n                latent_vectors.append(latent_flat)\n                labels.append(batch_labels.cpu().numpy())\n                \n                # Store original shapes and indices for potential reconstruction\n                for i in range(batch_size):\n                    if indices is not None:\n                        if isinstance(indices, list):\n                            indices_list.append([idx[i:i+1] for idx in indices])\n                        else:\n                            indices_list.append(indices[i:i+1])\n                    else:\n                        indices_list.append(None)\n                    \n                    if len(latent.shape) > 2:\n                        orig_shapes.append(torch.Size([1, latent.size(1), latent.size(2), latent.size(3)]))\n                    else:\n                        # Handle case where latent is already flattened\n                        orig_shapes.append(None)\n                \n                if len(latent_vectors) * batch_size >= 1000:  # Limit number of points for t-SNE\n                    break\n        \n        # Concatenate batches\n        if latent_vectors:\n            latent_vectors = np.vstack(latent_vectors)\n            labels = np.concatenate(labels)\n            \n            # Apply t-SNE\n            tsne = TSNE(n_components=2, random_state=42)\n            latent_tsne = tsne.fit_transform(latent_vectors)\n            \n            # Plot t-SNE results\n            plt.figure(figsize=(10, 8))\n            scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\n            plt.colorbar(scatter, label='Font Class')\n            plt.title('t-SNE Visualization of Latent Space')\n            \n            if save_path:\n                plt.savefig(os.path.join(save_path, 'latent_tsne.png'), bbox_inches='tight')\n            plt.show()\n            \n            return latent_tsne, labels, latent_vectors, orig_shapes, indices_list\n        else:\n            print(\"No valid latent vectors were extracted\")\n            return None, None, None, None, None\n        \n    except Exception as e:\n        print(f\"Error in latent space visualization: {e}\")\n        return None, None, None, None, None\n\ndef interpolate_latent_space(model, dataloader, device, steps=10, save_path=None):\n    \"\"\"\n    Interpolate between two points in latent space and decode them\n    With improved error handling and support for different model architectures\n    \"\"\"\n    model.eval()\n    \n    try:\n        # Get two images from the dataset\n        valid_images = []\n        \n        for images, _ in dataloader:\n            # Skip empty batches\n            if images.numel() == 0:\n                continue\n                \n            # Ensure we have the right shape for processing\n            if len(images.shape) == 3:\n                images = images.unsqueeze(1)  # Add channel dimension if missing\n                \n            valid_images.append(images)\n            \n            if len(valid_images) > 0 and valid_images[0].shape[0] >= 2:\n                break\n        \n        if not valid_images or valid_images[0].shape[0] < 2:\n            print(\"Couldn't find suitable images for interpolation\")\n            return None\n            \n        # Select two images\n        img1 = valid_images[0][0:1].to(device)\n        img2 = valid_images[0][1:2].to(device)\n        \n        # Get latent representations and indices for unpooling\n        with torch.no_grad():\n            z1, indices1 = safely_extract_latent_features(model, img1)\n            z2, indices2 = safely_extract_latent_features(model, img2)\n            \n            # Check if latent features were extracted correctly\n            if z1 is None or z2 is None:\n                print(\"Failed to extract latent features for interpolation\")\n                return None\n        \n        # Get output size for the unpool operation if needed\n        # This is a reasonable default that should work with many architectures\n        if len(z1.shape) == 4:\n            output_size = torch.Size([1, z1.size(1), z1.size(2)*2, z1.size(3)*2])\n        else:\n            output_size = None\n        \n        # Interpolate between the two latent vectors\n        interpolations = []\n        with torch.no_grad():\n            for alpha in np.linspace(0, 1, steps):\n                # Linear interpolation in latent space\n                z_interp = alpha * z1 + (1 - alpha) * z2\n                \n                # Use indices from first image for interpolation (simplification)\n                decoded = safely_generate_from_latent(model, z_interp, indices1, output_size)\n                \n                interpolations.append(decoded)\n        \n        # Combine original images and interpolation\n        all_images = []\n        with torch.no_grad():\n            # Add the first original image\n            reconstructed_img1 = model(img1)\n            all_images.append(reconstructed_img1.cpu())\n            \n            # Add interpolations\n            all_images.extend([interp.cpu() for interp in interpolations])\n            \n            # Add the second original image\n            reconstructed_img2 = model(img2)\n            all_images.append(reconstructed_img2.cpu())\n        \n        # Create a grid\n        grid = make_grid(torch.cat(all_images), nrow=steps+2, normalize=True, pad_value=1)\n        plt.figure(figsize=(15, 4))\n        plt.imshow(grid.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.title('Latent Space Interpolation')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_interpolation.png'), bbox_inches='tight')\n        plt.show()\n        \n        return interpolations\n        \n    except Exception as e:\n        print(f\"Error during latent space interpolation: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\ndef main():\n    \"\"\"\n    Main evaluation function that runs all the evaluation steps\n    \"\"\"\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create output directory for visualizations\n    save_dir = \"evaluation_results\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define paths - make these configurable in your actual implementation\n    model_path = \"/kaggle/working/best_model.pt\"\n    jpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\n    bcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\n    label_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\n    \n    try:        \n        # Load your trained model\n        model = SCAE()\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        model.to(device)\n        model.eval()\n        print(\"Model loaded successfully\")\n        \n        # Load dataset\n        combined_dataset = CombinedImageDataset(\n            jpeg_dir=jpeg_dir,\n            bcf_file=bcf_file,\n            label_file=label_file,\n            num_patch=3,\n        )\n    \n        # Create dataloaders using our fixed implementation\n        test_loader, val_loader = create_patch_dataloaders(\n            combined_dataset,\n            batch_size=64,  # Reduced batch size for better stability\n            num_workers=2,\n            val_split=0.1,\n            patch_size=(105, 105)\n        )\n        \n        print(\"Dataset and DataLoader prepared\")\n        \n        # Evaluate reconstruction quality\n        print(\"\\n1. Evaluating reconstruction quality...\")\n        recon_metrics = evaluate_reconstruction(\n            model, test_loader, device, num_samples=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Visualize latent space\n        print(\"\\n2. Visualizing latent space with t-SNE...\")\n        latent_tsne, labels, latent_vectors, orig_shapes, indices_list = visualize_latent_space(\n            model, test_loader, device, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Interpolate in latent space\n        print(\"\\n3. Creating latent space interpolation...\")\n        interpolations = interpolate_latent_space(\n            model, test_loader, device, steps=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        print(f\"\\nEvaluation complete! Results saved to {save_dir}\")\n        \n    except Exception as e:\n        print(f\"Error during evaluation: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/evaluation_results.zip /kaggle/working/evaluation_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Augumentation Steps \n1) Noise\n2) Blur\n3) Perpective Rotation\n4) Shading\n5) Variable Character Spacing\n6) Variable Aspect Ratio","metadata":{"id":"1hbTCU2qndht"}},{"cell_type":"code","source":"# augmentation functions\nfrom PIL import ImageFilter, Image\nimport random\nimport numpy as np\nimport cv2\n\ndef noise_image(np_img, mean=0, std=2):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n    img_array = np_img.astype(np.float32)\n    noise = np.random.normal(mean, std, img_array.shape)\n    noisy_img = img_array + noise\n    noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n    return cv2.resize(noisy_img, (105, 105))\n\ndef blur_image(np_img):\n    if isinstance(np_img, np.ndarray):\n        np_img = Image.fromarray(np_img.astype('uint8'))\n    blur_img = np_img.filter(ImageFilter.GaussianBlur(radius=1.5))\n    blur_img = blur_img.resize((105, 105))\n    return np.array(blur_img)\n\ndef affine_rotation(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    if len(np_img.shape) == 2:\n        np_img = np.expand_dims(np_img, axis=-1)\n\n    rows, cols = np_img.shape[:2]\n    src_pts = np.float32([[0, 0], [cols - 1, 0], [0, rows - 1]])\n    max_shift = 0.05\n    dst_pts = src_pts + np.random.uniform(-max_shift * cols, max_shift * cols, src_pts.shape).astype(np.float32)\n\n    A = cv2.getAffineTransform(src_pts, dst_pts)\n    warped = cv2.warpAffine(np_img, A, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n\n    if warped.ndim == 3 and warped.shape[-1] == 1:\n        warped = warped[:, :, 0]\n\n    warped = cv2.resize(warped, (105, 105))\n    return warped\n\ndef gradient_fill(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if len(np_img.shape) == 3:\n        gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = np_img\n\n    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n    abs_lap = np.absolute(laplacian) * 0.5\n    lap_uint8 = np.uint8(np.clip(abs_lap, 0, 255))\n    lap_resized = cv2.resize(lap_uint8, (105, 105))\n    return lap_resized\n\ndef variable_aspect_ratio_preprocess(np_img, target_size=(105, 105)):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    if len(np_img.shape) == 3:\n        h, w, c = np_img.shape\n    else:\n        h, w = np_img.shape\n        c = None\n\n    scale_ratio = np.random.uniform(0.95, 1.05)\n    new_width = int(w * scale_ratio)\n\n    resized = cv2.resize(np_img, (new_width, h), interpolation=cv2.INTER_LINEAR)\n    final = cv2.resize(resized, target_size, interpolation=cv2.INTER_LINEAR)\n    return final\n\ndef augmentation_pipeline(np_img):\n    \"\"\"\n    Tăng cường ảnh đầu vào với các phép biến đổi ngẫu nhiên.\n    Hỗ trợ ảnh grayscale hoặc RGB dưới dạng NumPy array hoặc PIL Image.\n    \"\"\"\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    img = variable_aspect_ratio_preprocess(np_img)\n\n    augmentations = [\n        lambda x: noise_image(x),\n        lambda x: blur_image(x),\n        lambda x: affine_rotation(x),\n        lambda x: gradient_fill(x)\n    ]\n\n    num_aug = random.randint(1, 3)\n    selected_augs = random.sample(augmentations, num_aug)\n\n    for aug in selected_augs:\n        img = aug(img)\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        if img.dtype != np.uint8:\n            img = np.clip(img, 0, 255).astype(np.uint8)\n\n    return img\n\n","metadata":{"id":"MLCHbBKsndhv","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-19T08:30:10.88756Z","iopub.execute_input":"2025-05-19T08:30:10.888097Z","iopub.status.idle":"2025-05-19T08:30:10.94852Z","shell.execute_reply.started":"2025-05-19T08:30:10.88807Z","shell.execute_reply":"2025-05-19T08:30:10.947998Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# extract patches from an image\nimport easyocr\nimport numpy as np\nimport os\nimport tempfile\nfrom PIL import Image\n\n# Global OCR reader for efficiency\n_ocr_reader = None\n\ndef get_ocr_reader(languages=[\"en\"]):\n    global _ocr_reader\n    if _ocr_reader is None:\n        _ocr_reader = easyocr.Reader(languages)\n    return _ocr_reader\n\ndef extract_patches(image_array, num_patch=3, patch_size=(105, 105), \n                    extract_text=True, min_text_coverage=0.3, max_attempts=20):\n    # Handle dimension check\n    if image_array.ndim == 2:  # Grayscale\n        h, w = image_array.shape\n        is_grayscale = True\n    elif image_array.ndim == 3:  # Color\n        h, w, _ = image_array.shape\n        is_grayscale = False\n    else:\n        print(f\"Warning: Unexpected image array dimension: {image_array.ndim}\")\n        return []\n\n    patch_h, patch_w = patch_size\n\n    # Check if image is large enough for at least one patch\n    if h < patch_h or w < patch_w:\n        return []  # Return empty list if image is too small\n        \n    # If not extracting text or image is too small, use random patches\n    if not extract_text:\n        patches = []\n        for _ in range(num_patch):\n            x = np.random.randint(0, w - patch_w + 1)\n            y = np.random.randint(0, h - patch_h + 1)\n            if is_grayscale:\n                patch = image_array[y:y+patch_h, x:x+patch_w]\n            else:\n                patch = image_array[y:y+patch_h, x:x+patch_w, :]\n            patches.append(patch)\n        return patches\n        \n    # For text extraction, we'll try to find patches with text\n    reader = get_ocr_reader()\n    text_patches = []\n    attempts = 0\n    \n    # Keep extracting until we have enough or reach max attempts\n    while len(text_patches) < num_patch and attempts < max_attempts:\n        # Generate a random patch\n        x = np.random.randint(0, w - patch_w + 1)\n        y = np.random.randint(0, h - patch_h + 1)\n        \n        if is_grayscale:\n            patch = image_array[y:y+patch_h, x:x+patch_w]\n        else:\n            patch = image_array[y:y+patch_h, x:x+patch_w, :]\n            \n        # Save patch to temporary file for OCR\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:\n            tmp_path = tmp.name\n            patch_img = Image.fromarray(patch if is_grayscale else patch)\n            patch_img.save(tmp_path)\n            \n        try:\n            # Run OCR on the patch\n            ocr_results = reader.readtext(tmp_path)\n            \n            # Clean up temp file\n            os.unlink(tmp_path)\n            \n            # Calculate total text area\n            patch_area = patch_h * patch_w\n            text_area = 0\n            \n            for bbox, text, conf in ocr_results:\n                if conf < 0.5:  # Skip low confidence detections\n                    continue\n                    \n                # Convert bbox points to integers\n                bbox = [[int(point[0]), int(point[1])] for point in bbox]\n                \n                # Calculate bbox area (text area)\n                text_min_x = max(0, min(point[0] for point in bbox))\n                text_max_x = min(patch_w, max(point[0] for point in bbox))\n                text_min_y = max(0, min(point[1] for point in bbox))\n                text_max_y = min(patch_h, max(point[1] for point in bbox))\n                \n                box_width = text_max_x - text_min_x\n                box_height = text_max_y - text_min_y\n                \n                if box_width > 0 and box_height > 0:\n                    text_area += box_width * box_height\n            \n            # Check if patch has enough text\n            coverage = text_area / patch_area\n            if coverage >= min_text_coverage:\n                text_patches.append(patch)\n                # print(f\"Found text patch with coverage {coverage:.2f}\")\n        \n        except Exception as e:\n            print(f\"Error in OCR: {e}\")\n            try:\n                os.unlink(tmp_path)\n            except:\n                pass\n                \n        attempts += 1\n    \n    # Return whatever text patches we found, even if fewer than requested\n    return text_patches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:30:13.641339Z","iopub.execute_input":"2025-05-19T08:30:13.641856Z","iopub.status.idle":"2025-05-19T08:30:16.863495Z","shell.execute_reply.started":"2025-05-19T08:30:13.641831Z","shell.execute_reply":"2025-05-19T08:30:16.862918Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# combined dataset (them moi dong if ... is None, unnecessary bro)\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom io import BytesIO\nfrom datasets import Dataset\nimport warnings\nfrom torchvision import transforms\n\nclass FontDataset(Dataset):\n    def __init__(self, jpeg_dir, bcf_file, label_file, num_patch=3, patch_size=(105, 105), \n                 extract_text=False, min_text_coverage=0.3, max_attempts=20, ocr_languages=[\"en\"]):\n        self.jpeg_dir = jpeg_dir\n        self.bcf_file = bcf_file\n        self.label_file = label_file\n        self.num_patch = num_patch\n        self.patch_size = patch_size\n        self.extract_text = extract_text\n        self.min_text_coverage = min_text_coverage\n        self.max_attempts = max_attempts\n        self.ocr_languages = ocr_languages\n\n        # Initialize OCR reader if needed\n        if extract_text:\n            self.reader = get_ocr_reader(ocr_languages)\n\n        self.jpeg_data = []\n        self.bcf_data = []\n\n        # Load jpeg data\n        self._load_jpeg_data(jpeg_dir)\n\n        # Load bcf data\n        self._load_bcf_data(bcf_file, label_file)\n\n    def _load_jpeg_data(self, jpeg_dir):\n        # add login for the case where jpeg_dir is None\n        if jpeg_dir is None:\n            print(\"Warning: jpeg_dir is None, skipping JPEG data loading.\")\n            return\n        if not os.path.exists(jpeg_dir):\n            print(f\"Warning: JPEG directory {jpeg_dir} does not exist.\")\n            return\n            \n        image_filenames = [f for f in os.listdir(jpeg_dir) if f.lower().endswith(('.jpeg', '.jpg'))]\n        self.jpeg_data = [(os.path.join(jpeg_dir, f), 0) for f in image_filenames]  # Assuming label is 0 for .jpeg files\n        print(f\"Loaded {len(self.jpeg_data)} .jpeg images.\")\n\n    def _load_bcf_data(self, bcf_file, label_file):\n        if not (os.path.exists(bcf_file) and os.path.exists(label_file)):\n            print(f\"Warning: BCF file {bcf_file} or label file {label_file} does not exist.\")\n            return\n            \n        try:\n            with open(label_file, 'rb') as f:\n                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n                print(f\"Loaded {len(self.labels)} labels from {label_file}.\")\n\n            with open(bcf_file, 'rb') as f:\n                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n                print(f\"Loaded {self.num_images} images from {bcf_file}.\")\n\n                sizes_bytes = f.read(self.num_images * 8)\n                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n\n                self.data_start_offset = 8 + self.num_images * 8\n                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n\n                for idx in range(self.num_images):\n                    self.bcf_data.append((idx, self.labels[idx]))\n                \n            print(f\"Loaded {len(self.bcf_data)} .bcf images.\")\n        except Exception as e:\n            print(f\"Error loading .bcf data: {e}\")\n\n    def __len__(self):\n        return len(self.jpeg_data) + len(self.bcf_data)\n\n    def _extract_patches(self, img_array):\n        return extract_patches(\n            img_array, \n            num_patch=self.num_patch, \n            patch_size=self.patch_size,\n            extract_text=self.extract_text, \n            min_text_coverage=self.min_text_coverage,\n            max_attempts=self.max_attempts\n        )\n\n    def __getitem__(self, idx):\n        if isinstance(idx, list):\n            results = []\n            labels = []\n            for single_idx in idx:\n                try:\n                    patches, label = self.__getitem__(single_idx)\n                    if patches and label != -1:\n                        results.append(patches)\n                        labels.append(label)\n                except Exception as e:\n                    print(f\"Error processing index {single_idx}: {e}\")\n            return results, labels\n\n        max_retries = 3\n        for retry in range(max_retries):\n            try:\n                if idx < len(self.jpeg_data):\n                    # Handle .jpeg images\n                    img_path, label = self.jpeg_data[idx]\n                    try:\n                        with warnings.catch_warnings():\n                            warnings.simplefilter(\"ignore\")\n                            img = Image.open(img_path)\n                            img.verify()\n\n                        img = Image.open(img_path).convert('L')\n                        img_array = np.array(img)\n\n                        # ✅ Apply augmentation before extracting patches\n                        patches = self._extract_patches(img_array)\n                        patches = [augmentation_pipeline(patch) for patch in patches]\n\n                        del img, img_array\n                        return patches, label\n\n                    except (OSError, IOError, ValueError) as e:\n                        print(f\"Warning: Corrupt image at {img_path}: {e}\")\n                        return [], -1\n\n                else:\n                    # Handle .bcf images\n                    bcf_idx = idx - len(self.jpeg_data)\n                    if bcf_idx >= len(self.bcf_data):\n                        return [], -1\n\n                    label = self.bcf_data[bcf_idx][1]\n                    offset = self.image_offsets[bcf_idx]\n                    size = self.image_sizes[bcf_idx]\n\n                    try:\n                        with open(self.bcf_file, 'rb') as f:\n                            f.seek(self.data_start_offset + offset)\n                            image_bytes = f.read(size)\n\n                        buffer = BytesIO(image_bytes)\n                        img = Image.open(buffer)\n                        img.verify()\n\n                        buffer.seek(0)\n                        img = Image.open(buffer).convert('L')\n                        img_array = np.array(img)\n\n                        # ✅ Apply augmentation before extracting patches\n                        patches = self._extract_patches(img_array)\n                        patches = [augmentation_pipeline(patch) for patch in patches]\n\n                        del img, img_array, buffer, image_bytes\n                        return patches, label\n\n                    except (OSError, IOError, ValueError) as e:\n                        print(f\"Warning: Corrupt BCF image at index {bcf_idx}: {e}\")\n                        return [], -1\n\n            except Exception as e:\n                print(f\"Unexpected error processing idx {idx}: {e}\")\n\n            if retry < max_retries - 1:\n                idx = (int(idx) + 1) % len(self)\n\n        return [], -1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:30:18.715515Z","iopub.execute_input":"2025-05-19T08:30:18.71645Z","iopub.status.idle":"2025-05-19T08:30:19.628421Z","shell.execute_reply.started":"2025-05-19T08:30:18.716413Z","shell.execute_reply":"2025-05-19T08:30:19.627634Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# memory_efficient_patch_collate_fn\nimport gc\nimport warnings\nfrom functools import partial\n\n# Add this memory-efficient patch collate function\ndef memory_efficient_patch_collate_fn(batch, patch_size_tuple):\n    \"\"\"\n    Memory-efficient version of patch_collate_fn that processes one patch at a time\n    and includes robust error handling.\n    \"\"\"\n    import gc  # Import inside function for worker processes\n    \n    all_patches = []\n    all_labels = []\n    valid_batch_items = 0\n\n    # Process one item at a time to avoid large memory allocations\n    for item in batch:\n        patches, label = item\n        # Ensure item is valid\n        if patches and label != -1:\n            # Process patches one by one\n            for patch in patches:\n                all_patches.append(patch)\n                all_labels.append(label)\n            valid_batch_items += 1\n    \n    # Periodically force garbage collection\n    if len(all_patches) > 100:\n        gc.collect()\n    \n    # Empty batch handling\n    if not all_patches:\n        patch_h, patch_w = patch_size_tuple\n        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n\n    # Process in smaller chunks to reduce peak memory usage\n    max_chunk_size = 64  # Adjust based on your GPU memory\n    num_patches = len(all_patches)\n    patches_tensor_list = []\n    \n    for i in range(0, num_patches, max_chunk_size):\n        chunk = all_patches[i:i+max_chunk_size]\n        # Convert to NumPy array\n        chunk_np = np.stack(chunk)\n        # Convert to tensor, normalize and add channel dimension\n        chunk_tensor = torch.from_numpy(chunk_np).float() / 255.0\n        chunk_tensor = chunk_tensor.unsqueeze(1)\n        patches_tensor_list.append(chunk_tensor)\n        \n        # Clear variables to free memory\n        del chunk, chunk_np\n    \n    # Concatenate chunks\n    patches_tensor = torch.cat(patches_tensor_list, dim=0)\n    labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n    \n    # Clean up\n    del patches_tensor_list, all_patches, all_labels\n    gc.collect()\n    \n    return patches_tensor, labels_tensor\n\n# Add this function to create optimized DataLoaders\nimport torch\nfrom torch.utils.data import DataLoader\nfrom functools import partial\n\ndef create_optimized_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper error handling, avoiding HuggingFace datasets compatibility issues.\n    \n    Args:\n        dataset: The image dataset instance\n        batch_size: Batch size for training\n        num_workers: Number of worker processes\n        val_split: Validation split ratio (0-1)\n        \n    Returns:\n        tuple: (train_loader, val_loader)\n    \"\"\"\n    from torch.utils.data import DataLoader, Subset\n    import numpy as np\n    \n    # Calculate split sizes\n    dataset_size = len(dataset)\n    indices = np.arange(dataset_size)\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets - this avoids Hugging Face's __getitems__ implementation\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    # Custom collate function with error handling\n    def safe_collate(batch):\n        # Filter out empty or invalid items\n        valid_batch = [(patches, label) for patches, label in batch if patches and label != -1]\n        \n        if not valid_batch:\n            # Return empty tensors if no valid items\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n        \n        # Process valid items\n        all_patches = []\n        all_labels = []\n        \n        for patches, label in valid_batch:\n            if isinstance(patches, list) and patches:\n                all_patches.extend(patches)\n                all_labels.extend([label] * len(patches))\n        \n        if not all_patches:\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n            \n        # Convert to PyTorch tensors\n        try:\n            patches_np = np.array(all_patches)\n            patches_tensor = torch.tensor(patches_np, dtype=torch.float) / 255.0\n            \n            # Add channel dimension if needed\n            if len(patches_tensor.shape) == 3:  # (B, H, W)\n                patches_tensor = patches_tensor.unsqueeze(1)  # -> (B, 1, H, W)\n                \n            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n            return patches_tensor, labels_tensor\n        except Exception as e:\n            print(f\"Error in collate function: {e}\")\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n    \n    # Create DataLoaders with minimal worker configuration for stability\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:30:23.397969Z","iopub.execute_input":"2025-05-19T08:30:23.39882Z","iopub.status.idle":"2025-05-19T08:30:23.413415Z","shell.execute_reply.started":"2025-05-19T08:30:23.398789Z","shell.execute_reply":"2025-05-19T08:30:23.412511Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# load dataset -> create dataloader \nimport torch\nimport torch.nn as nn\nimport os\nimport gc\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\n\n# Clean memory before starting\ngc.collect()\ntorch.cuda.empty_cache()\n\njpeg_dir = None\nbcf_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf\"\nlabel_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label\"\n\n# Print GPU info\nif torch.cuda.is_available():\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n    print(f\"Available memory: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n\n# Create dataset with smaller patch size and fewer patches per image\nclassifier_dataset = FontDataset(\n    jpeg_dir=jpeg_dir,\n    bcf_file=bcf_file,\n    label_file=label_file,\n    num_patch=3,  # Number of patches per image\n)\n\n# from torch.utils.data import Subset\n# classifier_dataset = Subset(classifier_dataset, indices=list(range(10000)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:30:34.860757Z","iopub.execute_input":"2025-05-19T08:30:34.861421Z","iopub.status.idle":"2025-05-19T08:30:35.150105Z","shell.execute_reply.started":"2025-05-19T08:30:34.861388Z","shell.execute_reply":"2025-05-19T08:30:35.149421Z"}},"outputs":[{"name":"stdout","text":"Using GPU: Tesla P100-PCIE-16GB\nTotal memory: 17.06 GB\nAvailable memory: 0.00 GB\nWarning: jpeg_dir is None, skipping JPEG data loading.\nLoaded 200000 labels from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label.\nLoaded 200000 images from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf.\nLoaded 200000 .bcf images.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, Subset\nimport gc\nimport torch\nimport numpy as np\n\nclass DatasetWrapper(Dataset):\n    \"\"\"\n    A wrapper for your dataset to ensure compatibility with DataLoader\n    \"\"\"\n    def __init__(self, original_dataset):\n        self.dataset = original_dataset\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # Get a single item by index, handling both direct dataset access\n        # and access through Subset indices\n        try:\n            # Handle if we're accessing through a Subset\n            if hasattr(self.dataset, 'dataset') and hasattr(self.dataset, 'indices'):\n                original_idx = self.dataset.indices[idx]\n                return self.dataset.dataset[original_idx]\n            # Normal access\n            return self.dataset[idx]\n        except Exception as e:\n            print(f\"Error accessing item {idx}: {e}\")\n            # Return a placeholder for invalid items\n            return [], -1\n\n\ndef create_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper handling for HuggingFace datasets.\n    \"\"\"\n    # Ensure the dataset is properly wrapped\n    wrapped_dataset = DatasetWrapper(dataset)\n    \n    # Calculate split sizes\n    dataset_size = len(wrapped_dataset)\n    indices = list(range(dataset_size))\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets\n    train_dataset = Subset(wrapped_dataset, train_indices)\n    val_dataset = Subset(wrapped_dataset, val_indices)\n    \n    # Custom collate function\n    def custom_collate_fn(batch):\n        # Filter out invalid items\n        batch = [(img, label) for img, label in batch if img is not None and len(img) > 0 and label != -1]\n        \n        if not batch:\n            # Return empty tensors with appropriate dimensions\n            return torch.empty((0, 3, 105, 105), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n        \n        # Extract images and labels\n        images, labels = zip(*batch)\n        \n        # Convert to tensors\n        images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in images])\n        labels_tensor = torch.tensor(labels, dtype=torch.long)\n        \n        # Normalize images if needed\n        if images_tensor.max() > 1.0:\n            images_tensor = images_tensor / 255.0\n            \n        return images_tensor, labels_tensor\n    \n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=custom_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=custom_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    return train_loader, val_loader\n\ndef create_patch_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1, patch_size=(105, 105)):\n    \"\"\"\n    Creates DataLoaders specifically for patch-based datasets where each item\n    may contain multiple patches.\n    \"\"\"\n    # Ensure the dataset is properly wrapped\n    wrapped_dataset = DatasetWrapper(dataset)\n    \n    # Calculate split sizes\n    dataset_size = len(wrapped_dataset)\n    indices = list(range(dataset_size))\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets\n    train_dataset = Subset(wrapped_dataset, train_indices)\n    val_dataset = Subset(wrapped_dataset, val_indices)\n    \n    # Memory efficient collate function for patches\n    def patch_collate_fn(batch):\n        all_patches = []\n        all_labels = []\n        \n        # Process one batch item at a time\n        for patches, label in batch:\n            if patches is not None and len(patches) > 0 and label != -1:\n                # Handle both single patches and lists of patches\n                if isinstance(patches, list):\n                    all_patches.extend(patches)\n                    all_labels.extend([label] * len(patches))\n                else:\n                    all_patches.append(patches)\n                    all_labels.append(label)\n        \n        # Return empty tensors if batch is empty\n        if not all_patches:\n            return torch.empty((0, 3, patch_size[0], patch_size[1]), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n            \n        try:\n            # Process in chunks to reduce memory usage\n            max_chunk_size = min(64, len(all_patches))\n            patches_tensors = []\n            \n            for i in range(0, len(all_patches), max_chunk_size):\n                chunk = all_patches[i:i+max_chunk_size]\n                chunk_tensor = torch.stack([torch.tensor(p, dtype=torch.float) for p in chunk])\n                \n                # Normalize if needed\n                if chunk_tensor.max() > 1.0:\n                    chunk_tensor = chunk_tensor / 255.0\n                    \n                patches_tensors.append(chunk_tensor)\n                \n            # Combine chunks\n            patches_tensor = torch.cat(patches_tensors, dim=0).unsqueeze(1)\n            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n            \n            # Clean up to save memory\n            del patches_tensors, all_patches, all_labels\n            gc.collect()\n            \n            return patches_tensor, labels_tensor\n        except Exception as e:\n            print(f\"Error in collate function: {e}\")\n            return torch.empty((0, 3, patch_size[0], patch_size[1]), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n    \n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=patch_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=patch_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    return train_loader, val_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:30:39.933187Z","iopub.execute_input":"2025-05-19T08:30:39.933797Z","iopub.status.idle":"2025-05-19T08:30:39.94953Z","shell.execute_reply.started":"2025-05-19T08:30:39.933774Z","shell.execute_reply":"2025-05-19T08:30:39.948606Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_font_loader, val_font_loader = create_patch_dataloaders(\n    classifier_dataset,\n    batch_size=1028,\n    num_workers=2,\n    val_split=0.2,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:30:46.565328Z","iopub.execute_input":"2025-05-19T08:30:46.565842Z","iopub.status.idle":"2025-05-19T08:30:46.593046Z","shell.execute_reply.started":"2025-05-19T08:30:46.565817Z","shell.execute_reply":"2025-05-19T08:30:46.59218Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"len(train_font_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:30:56.296322Z","iopub.execute_input":"2025-05-19T08:30:56.296604Z","iopub.status.idle":"2025-05-19T08:30:56.301525Z","shell.execute_reply.started":"2025-05-19T08:30:56.296584Z","shell.execute_reply":"2025-05-19T08:30:56.300793Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"156"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Visualization some samples from the combined dataset \ndef visualize_simple_images_and_patches(dataset, num_images=2, seed=None):\n    \"\"\"\n    Visualizes full images and their extracted patches in a simple layout.\n    Shows images and their 3 patches in a clean format with error handling.\n    \n    Args:\n        dataset: A CombinedImageDataset or BCFImagePatchDataset instance\n        num_images: Number of images to visualize (default: 2)\n        seed: Random seed for reproducibility\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import random\n    from PIL import Image, ImageFile\n    from io import BytesIO\n    import os\n    \n    # Allow loading of truncated images\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n    \n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Find valid images (with patches)\n    valid_indices = []\n    attempts = 0\n    max_attempts = min(len(dataset) * 2, 100)  # Limit search attempts\n    \n    while len(valid_indices) < num_images and attempts < max_attempts:\n        idx = random.randint(0, len(dataset) - 1)\n        if idx not in valid_indices:  # Avoid duplicates\n            try:\n                patches, label = dataset[idx]\n                if patches and len(patches) > 0:\n                    valid_indices.append(idx)\n            except Exception as e:\n                print(f\"Error loading index {idx}: {e}\")\n            attempts += 1\n    \n    # If we couldn't find enough valid images\n    if len(valid_indices) < num_images:\n        print(f\"Warning: Could only find {len(valid_indices)} valid images with patches\")\n        if len(valid_indices) == 0:\n            print(\"No valid images found. Check your dataset.\")\n            return\n    \n    # Create figure with enough space for all elements\n    fig, axes = plt.subplots(len(valid_indices), 4, figsize=(16, 5 * len(valid_indices)))\n    \n    # If only one image is requested, make axes indexable as 2D\n    if len(valid_indices) == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i, idx in enumerate(valid_indices):\n        try:\n            # Get item directly from dataset\n            patches, label = dataset[idx]\n            \n            # Get the original full image\n            img_array = None\n            \n            if hasattr(dataset, 'jpeg_data') and idx < len(dataset.jpeg_data):\n                # From jpeg_data\n                img_path, _ = dataset.jpeg_data[idx]\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {os.path.basename(img_path)}\"\n                \n            elif hasattr(dataset, 'image_filenames') and not hasattr(dataset, 'num_images'):\n                # From BCFImagePatchDataset with JPEG source\n                img_path = os.path.join(dataset.data_source, dataset.image_filenames[idx])\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {dataset.image_filenames[idx]}\"\n                \n            else:\n                # From BCF file (either CombinedImageDataset or BCFImagePatchDataset)\n                if hasattr(dataset, 'bcf_data'):\n                    # CombinedImageDataset\n                    bcf_idx = idx - len(dataset.jpeg_data)\n                    if bcf_idx < 0 or bcf_idx >= len(dataset.bcf_data):\n                        print(f\"Invalid BCF index: {bcf_idx}\")\n                        continue\n                        \n                    offset = dataset.image_offsets[bcf_idx]\n                    size = dataset.image_sizes[bcf_idx]\n                    data_file = dataset.bcf_file\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {bcf_idx})\"\n                else:\n                    # BCFImagePatchDataset\n                    offset = dataset.image_offsets[idx]\n                    size = dataset.image_sizes[idx]\n                    data_file = dataset.data_source\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {idx})\"\n                \n                with open(data_file, 'rb') as f:\n                    f.seek(data_start + offset)\n                    image_bytes = f.read(size)\n                img = Image.open(BytesIO(image_bytes)).convert('L')\n                img_array = np.array(img)\n            \n            # Plot original image if we successfully loaded it\n            if img_array is not None:\n                axes[i, 0].imshow(img_array, cmap='gray')\n                axes[i, 0].set_title(f\"Original Image\\nLabel: {label}\\nSource: {source} Shape: {img_array.shape}\")\n                axes[i, 0].axis('off')\n            else:\n                axes[i, 0].text(0.5, 0.5, \"Image loading failed\", ha='center', va='center')\n                axes[i, 0].axis('off')\n            \n            # Plot the patches - ensure we have patches to display\n            if patches and len(patches) > 0:\n                for j in range(3):\n                    if j < len(patches):\n                        patch = patches[j]\n                        axes[i, j+1].imshow(patch, cmap='gray')\n                        axes[i, j+1].set_title(f\"Patch {j+1}\\nShape: {patch.shape}\")\n                    else:\n                        # No more patches to display\n                        axes[i, j+1].text(0.5, 0.5, \"No patch\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            else:\n                # No patches for this image\n                for j in range(3):\n                    axes[i, j+1].text(0.5, 0.5, \"No patches extracted\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            \n        except Exception as e:\n            print(f\"Error processing index {idx}: {e}\")\n            # Create error message in subplot\n            for j in range(4):\n                axes[i, j].text(0.5, 0.5, f\"Error: {str(e)[:50]}...\", ha='center', va='center')\n                axes[i, j].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return the indices we used (helpful for debugging)\n    return valid_indices\n\n# Example usage:\nvisualize_simple_images_and_patches(combined_dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load test_font_loader\njpeg_dir = None\nbcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\nlabel_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\n\n# Create dataset with smaller patch size and fewer patches per image\nclassifier_test_dataset = FontDataset(\n    jpeg_dir=jpeg_dir,\n    bcf_file=bcf_file,\n    label_file=label_file,\n    num_patch=1,  # Number of patches per image\n)\n\n# classifier_test_dataset = Subset(classifier_test_dataset, indices=list(range(10000)))\n\ntest_font_loader, test_val_font_loader = create_patch_dataloaders(\n    classifier_dataset,\n    batch_size=1024,\n    num_workers=2,\n    val_split=0,\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(test_font_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sample = (next(iter(train_classifier_loader)))\nval_sample = (next(iter(val_classifier_loader)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_sample[1].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nscae = SCAE().to(device)\nscae.load_state_dict(torch.load(\"/kaggle/working/best_model.pt\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# classifier\nclass FontClassifier(nn.Module):\n    def __init__(self, pretrained_scae, num_classes=2383, normalization_type=\"batch_norm\", \n                 use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n        super().__init__()\n        self.pretrained_scae = pretrained_scae  # Use pretrained SCAE encoder\n        \n        # Define helper functions for creating layers\n        def norm_layer(num_features, spatial_size=None):\n            if normalization_type == \"batch_norm\":\n                return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\":\n                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\" and spatial_size is not None:\n                return nn.LayerNorm([num_features, spatial_size, spatial_size])\n            else:\n                return nn.Identity()\n\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n        \n        # CNN head after the SCAE encoder\n        # SCAE encoder output is 128 x 26 x 26\n        self.cnn_head = nn.Sequential(\n            # Conv layer 4\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Out: 256 x 12 x 12\n            norm_layer(256, 12),\n            activation_layer(),\n            \n            # Conv layer 5\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Out: 256 x 12 x 12\n            norm_layer(256, 13),\n            activation_layer(),\n            dropout_layer()\n        )\n        \n        # Flatten layer\n        self.flatten = nn.Flatten()\n        \n        # Fully connected layers\n        # Input size is 256 * 12 * 12 = 43,264\n        self.fully_connected = nn.Sequential(\n            nn.Linear(256 * 12 * 12, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_prob if use_dropout else 0),\n            \n            nn.Linear(4096, 2048),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_prob if use_dropout else 0),\n            \n            nn.Linear(2048, num_classes),\n            # nn.Softmax(dim=1) no softmax here bro, crossentropy does the softmax automatically\n        )\n\n    def forward(self, x):\n        # Use the encoder part of SCAE\n        x = self.pretrained_scae.encoder(x)\n        # Continue with additional CNN layers\n        x = self.cnn_head(x)\n        \n        # Flatten and apply fully connected layers\n        x = self.flatten(x)\n        x = self.fully_connected(x)\n        \n        return x\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classifier = FontClassifier(scae, num_classes=2383).to(device)\n\nfor batch in train_classifier_loader:\n    print(batch[0].shape)\n    print(classifier(batch[0].to(device)).shape)\n    # show_images_in_grid(batch.permute(0, 2, 3, 1).numpy(), titles=[f'Patch {i+1}' for i in range(len(batch))], cols=4)\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_classifier_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training script\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\nimport numpy as np\n\n# Initialize model, loss, optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nscae = SCAE().to(device)\nscae.load_state_dict(torch.load(\"/kaggle/working/best_model.pt\", weights_only=True))\nclassifier = FontClassifier(scae, num_classes=2383).to(device)\n\n# For classification, use CrossEntropyLoss\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer with momentum and weight decay\noptimizer = optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n\n# Learning rate scheduler to reduce LR when validation loss plateaus\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n\n# Training parameters\nnum_epochs = 5\nbest_val_loss = float('inf')\npatience = 7  # For early stopping\npatience_counter = 0\n\n# Tracking metrics\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Training phase\n    classifier.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    batch_count = 0\n    \n    # Training loop\n    for inputs, labels in tqdm(train_classifier_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n        # Skip empty batches\n        if inputs.numel() == 0:\n            continue\n            \n        # Move tensors to device\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = classifier(inputs)\n        # print(outputs.shape, labels.shape)\n        # Calculate loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        batch_count += 1\n    \n    # Calculate epoch statistics\n    if batch_count > 0:\n        epoch_loss = running_loss / batch_count\n        epoch_acc = 100 * correct / total\n        train_losses.append(epoch_loss)\n        print(f'Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.2f}%')\n    else:\n        print(\"No batches processed during training\")\n    \n    # Validation phase\n    classifier.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    val_batch_count = 0\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(val_classifier_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n            # Skip empty batches\n            if inputs.numel() == 0:\n                continue\n                \n            # Move tensors to device\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = classifier(inputs)\n            # print(outputs.shape, labels.shape)\n            \n            # Calculate loss\n            loss = criterion(outputs, labels)\n            \n            # Statistics\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            val_batch_count += 1\n    \n    # Calculate validation statistics\n    if val_batch_count > 0:\n        epoch_val_loss = val_loss / val_batch_count\n        epoch_val_acc = 100 * val_correct / val_total\n        val_losses.append(epoch_val_loss)\n        val_accuracies.append(epoch_val_acc)\n        print(f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%')\n        \n        # Update learning rate based on validation loss\n        scheduler.step(epoch_val_loss)\n        \n        # Check if this is the best model so far\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss\n            patience_counter = 0\n            # Save best model\n            torch.save(classifier.state_dict(), 'best_font_classifier.pt')\n            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")\n        else:\n            patience_counter += 1\n            # Early stopping\n            if patience_counter >= patience:\n                print(f\"Early stopping after {epoch+1} epochs\")\n                break\n    else:\n        print(\"No batches processed during validation\")\n\nprint(\"Training completed!\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluation\nimport torch\nfrom tqdm import tqdm\n\ndef calculate_top_k_error(model, test_loader, device, k_values=[1, 5]):\n    \"\"\"\n    Calculate top-k error rates for a model on a test dataset.\n    \n    Args:\n        model: The PyTorch model to evaluate\n        test_loader: DataLoader for the test dataset\n        device: Device to run evaluation on (cuda or cpu)\n        k_values: List of k values for top-k error calculation\n    \n    Returns:\n        dict: Dictionary containing error rates and accuracies\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    \n    # Initialize counters\n    total_samples = 0\n    correct_predictions = {k: 0 for k in k_values}\n    \n    with torch.no_grad():  # No need to track gradients\n        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            if inputs.numel() == 0:  # Skip empty batches\n                continue\n                \n            # Move data to the appropriate device\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            batch_size = labels.size(0)\n            total_samples += batch_size\n            \n            # Get topk predictions for each sample\n            for k in k_values:\n                # Get top-k class indices for each sample\n                _, topk_indices = torch.topk(outputs, k, dim=1)\n                \n                # Check if true label is in top-k predictions for each sample\n                for i in range(batch_size):\n                    if labels[i].item() in topk_indices[i]:\n                        correct_predictions[k] += 1\n    \n    # Calculate error rates and accuracies\n    error_rates = {k: 1.0 - (correct_predictions[k] / total_samples) for k in k_values}\n    accuracies = {k: correct_predictions[k] / total_samples for k in k_values}\n    \n    return {\"error_rates\": error_rates, \"accuracies\": accuracies}","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make sure your model is properly loaded and in eval mode\nclassifier.eval()\n\n# Run the evaluation\nresults = calculate_top_k_error(\n    model=classifier,\n    test_loader=test_classifier_loader,\n    device=device,\n    k_values=[1, 5]\n)\n\n# Print the results with percentages\nprint(f\"Top-1 Error: {results['error_rates'][1]:.4f} ({results['error_rates'][1]*100:.2f}%)\")\nprint(f\"Top-5 Error: {results['error_rates'][5]:.4f} ({results['error_rates'][5]*100:.2f}%)\")\nprint(f\"Top-1 Accuracy: {results['accuracies'][1]:.4f} ({results['accuracies'][1]*100:.2f}%)\")\nprint(f\"Top-5 Accuracy: {results['accuracies'][5]:.4f} ({results['accuracies'][5]*100:.2f}%)\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## HENet","metadata":{}},{"cell_type":"code","source":"# extract_patches_eval\nimport os\nimport numpy as np\nfrom io import BytesIO\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\ndef extract_patches_eval(image_array: np.ndarray,\n                          num_squeezes: int = 3,\n                          patches_per_squeeze: int = 5,\n                          patch_size: tuple = (105, 105),\n                          squeeze_range: tuple = (1.5, 3.5)) -> list:\n    \"\"\"\n    Evaluation-time patch extractor:\n      1. Normalize height to patch_size[0], keep aspect ratio.\n      2. Apply `num_squeezes` random width-squeezing factors from squeeze_range.\n      3. On each squeezed image, sample `patches_per_squeeze` random crops of size patch_size.\n      4. Return list of numpy patches.\n    \"\"\"\n    pil = Image.fromarray(image_array)\n    target_h, target_w = patch_size\n    # 1. Normalize height\n    orig_w, orig_h = pil.size\n    new_w = int(orig_w * (target_h / orig_h))\n    pil_hnorm = pil.resize((new_w, target_h), Image.BILINEAR)\n\n    patches = []\n    for _ in range(num_squeezes):\n        # 2. Squeeze width\n        factor = np.random.uniform(*squeeze_range)\n        w_sq = max(1, int(new_w / factor))\n        pil_sq = pil_hnorm.resize((w_sq, target_h), Image.BILINEAR)\n\n        # 3. Sample random crops\n        for __ in range(patches_per_squeeze):\n            if w_sq <= target_w:\n                pad = (0, 0, target_w - w_sq, 0)\n                crop_img = transforms.Pad(pad)(pil_sq)\n            else:\n                x = np.random.randint(0, w_sq - target_w + 1)\n                crop_img = pil_sq.crop((x, 0, x + target_w, target_h))\n            patches.append(np.array(crop_img))\n\n    return patches\n\n\nclass EvalFontDataset(Dataset):\n    \"\"\"\n    Dataset for evaluation: returns fixed 15 patches per image plus its label.\n    \"\"\"\n    def __init__(self,\n                 jpeg_dir: str = None,\n                 bcf_file: str = None,\n                 label_file: str = None,\n                 num_squeezes: int = 3,\n                 patches_per_squeeze: int = 5,\n                 patch_size: tuple = (105, 105),\n                 squeeze_range: tuple = (1.5, 3.5),\n                 transform=None):\n        self.jpeg_data = []    # list of (path, label)\n        self.bcf_data = []     # list of (offset, size, label)\n        self.bcf_file = bcf_file\n        self.label_file = label_file\n        self.num_squeezes = num_squeezes\n        self.patches_per_squeeze = patches_per_squeeze\n        self.patch_size = patch_size\n        self.squeeze_range = squeeze_range\n        self.transform = transform or transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        # Load JPEG entries if provided\n        if jpeg_dir and os.path.isdir(jpeg_dir):\n            for fname in os.listdir(jpeg_dir):\n                if fname.lower().endswith(('.jpg', '.jpeg')):\n                    # label for JPEGs not supported by default\n                    self.jpeg_data.append((os.path.join(jpeg_dir, fname), None))\n\n        # Load BCF entries\n        if bcf_file and label_file and os.path.exists(bcf_file) and os.path.exists(label_file):\n            # Read labels\n            with open(label_file, 'rb') as lf:\n                labels = np.frombuffer(lf.read(), dtype=np.uint32)\n            # Read image sizes and offsets\n            with open(bcf_file, 'rb') as bf:\n                num_images = int(np.frombuffer(bf.read(8), dtype=np.int64)[0])\n                sizes = np.frombuffer(bf.read(num_images * 8), dtype=np.int64)\n                self.data_start_offset = 8 + num_images * 8\n                offsets = np.concatenate(([0], np.cumsum(sizes)[:-1]))\n                for idx in range(num_images):\n                    self.bcf_data.append((int(offsets[idx]), int(sizes[idx]), int(labels[idx])))\n        else:\n            if bcf_file and not os.path.exists(bcf_file):\n                print(f\"Warning: BCF file '{bcf_file}' not found.\")\n            if label_file and not os.path.exists(label_file):\n                print(f\"Warning: Label file '{label_file}' not found.\")\n\n    def __len__(self):\n        return len(self.jpeg_data) + len(self.bcf_data)\n\n    def __getitem__(self, idx):\n        # JPEG path\n        if idx < len(self.jpeg_data):\n            img_path, label = self.jpeg_data[idx]\n            img = Image.open(img_path).convert('L')\n            img_array = np.array(img)\n        else:\n            # BCF path\n            bidx = idx - len(self.jpeg_data)\n            offset, size, label = self.bcf_data[bidx]\n            with open(self.bcf_file, 'rb') as bf:\n                bf.seek(self.data_start_offset + offset)\n                data = bf.read(size)\n            buffer = BytesIO(data)\n            img = Image.open(buffer).convert('L')\n            img_array = np.array(img)\n\n        # Extract and transform patches\n        patches_np = extract_patches_eval(\n            img_array,\n            num_squeezes=self.num_squeezes,\n            patches_per_squeeze=self.patches_per_squeeze,\n            patch_size=self.patch_size,\n            squeeze_range=self.squeeze_range\n        )\n        patches = [self.transform(Image.fromarray(p)) for p in patches_np]\n        patches = torch.stack(patches, dim=0)\n        return patches, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:31:47.49835Z","iopub.execute_input":"2025-05-19T08:31:47.498644Z","iopub.status.idle":"2025-05-19T08:31:47.517119Z","shell.execute_reply.started":"2025-05-19T08:31:47.498624Z","shell.execute_reply":"2025-05-19T08:31:47.516422Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"jpeg_dir = None\nbcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\nlabel_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\ntest_dataset = EvalFontDataset(jpeg_dir, bcf_file, label_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:31:55.604156Z","iopub.execute_input":"2025-05-19T08:31:55.604746Z","iopub.status.idle":"2025-05-19T08:31:55.622812Z","shell.execute_reply.started":"2025-05-19T08:31:55.604723Z","shell.execute_reply":"2025-05-19T08:31:55.622115Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    \"\"\"\n    Custom collate function for batching samples with (15, 1, 105, 105) images\n    \"\"\"\n    images = torch.stack([item[0] for item in batch])  # Stack images correctly\n    labels = torch.tensor([item[1] for item in batch])  # Convert labels to tensor\n    \n    return images, labels\n\ntest_font_loader = DataLoader(\n    test_dataset,\n    batch_size=128,       # adjust to your GPU memory\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True,\n    collate_fn=custom_collate_fn  # Use our custom collate function\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:32:03.853796Z","iopub.execute_input":"2025-05-19T08:32:03.854474Z","iopub.status.idle":"2025-05-19T08:32:03.859299Z","shell.execute_reply.started":"2025-05-19T08:32:03.854449Z","shell.execute_reply":"2025-05-19T08:32:03.858341Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"test_dataset[0][0].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = next(iter(test_font_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:32:10.201037Z","iopub.execute_input":"2025-05-19T08:32:10.201343Z","iopub.status.idle":"2025-05-19T08:32:12.898004Z","shell.execute_reply.started":"2025-05-19T08:32:10.201322Z","shell.execute_reply":"2025-05-19T08:32:12.897022Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"sample[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:32:13.650159Z","iopub.execute_input":"2025-05-19T08:32:13.650507Z","iopub.status.idle":"2025-05-19T08:32:13.656657Z","shell.execute_reply.started":"2025-05-19T08:32:13.65048Z","shell.execute_reply":"2025-05-19T08:32:13.655927Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Size([128, 15, 1, 105, 105])"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# HEBlock + HENet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport numpy as np\n\nclass HEBlock(nn.Module):\n    \"\"\"\n    Optimized HE (Hide and Enhance) Block implementation.\n    Vectorized implementation to eliminate slow Python loops.\n    \"\"\"\n    def __init__(self, beta=0.5):\n        \"\"\"\n        Args:\n            beta: weight of mask (default: 0.5 as recommended in the paper)\n        \"\"\"\n        super(HEBlock, self).__init__()\n        self.beta = beta\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: input feature map of shape (batch_size, C, H, W)\n        Returns:\n            Modified feature map with suppressed maximum activations\n        \"\"\"\n        if not self.training:  # Only apply during training\n            return x\n        \n        # Get shape information\n        batch_size, channels, h, w = x.size()\n        \n        # Find maximum values for each channel in each sample in batch\n        # Shape: [batch_size, channels, 1, 1]\n        max_vals = x.view(batch_size, channels, -1).max(dim=2)[0].view(batch_size, channels, 1, 1)\n        \n        # Create masks where the value equals the max value\n        # Broadcasting handles the comparison efficiently\n        mask = (x == max_vals).float()\n        \n        # Apply the beta factor to maximum values using the mask\n        # This is a vectorized operation that replaces the nested loops\n        output = torch.where(mask == 1, self.beta * x, x)\n        \n        return output\n\n\nclass HENet(nn.Module):\n    \"\"\"\n    Optimized HENet implementation for font recognition.\n    \"\"\"\n    def __init__(self, num_classes=2383, beta=0.5, use_amp=True):\n        \"\"\"\n        Args:\n            num_classes: Number of font classes (default: 2383)\n            beta: Weight for the HE Block mask (default: 0.5)\n            use_amp: Whether to use Automatic Mixed Precision (default: True)\n        \"\"\"\n        super(HENet, self).__init__()\n        \n        # Track whether to use mixed precision\n        self.use_amp = use_amp\n        \n        # Load pretrained ResNet18 - efficient backbone architecture\n        self.resnet = models.resnet18(pretrained=True)\n        \n        # Modify the first convolutional layer to accept grayscale input\n        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Remove the final fully connected layer to use as feature extractor\n        self.features = nn.Sequential(*list(self.resnet.children())[:-2])\n        \n        # 1x1 convolution to match the number of classes (more efficient than FC for large number of classes)\n        self.conv_final = nn.Conv2d(512, num_classes, kernel_size=1)\n        \n        # Optimized HE Block\n        self.he_block = HEBlock(beta=beta)\n        \n        # Global average pooling for efficient dimensionality reduction\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n    \n    def forward(self, x):\n        # Use AMP if specified (faster computation with minimal accuracy loss)\n        with torch.cuda.amp.autocast() if self.use_amp and torch.cuda.is_available() else torch.no_grad():\n            # Feature extraction using ResNet backbone\n            x = self.features(x)\n            \n            # 1x1 convolution to get class-specific activation maps\n            x = self.conv_final(x)\n            \n            # Apply HE Block during training (now optimized)\n            x = self.he_block(x)\n            \n            # Global average pooling and flatten\n            x = self.avgpool(x)\n            x = torch.flatten(x, 1)\n            \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:32:17.059457Z","iopub.execute_input":"2025-05-19T08:32:17.059758Z","iopub.status.idle":"2025-05-19T08:32:17.069892Z","shell.execute_reply.started":"2025-05-19T08:32:17.059735Z","shell.execute_reply":"2025-05-19T08:32:17.069102Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# optimized_train_eval.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm.auto import tqdm\nimport time\nimport os\nfrom datetime import timedelta\nimport numpy as np\nimport math\n\ndef train_model_optimized(model, train_loader, val_loader, num_epochs=100, \n                          device='cuda', use_amp=True, use_compile=False,\n                          gradient_accumulation_steps=4, save_dir='checkpoints'):\n    \"\"\"\n    Optimized training function with support for:\n    - Automatic Mixed Precision (AMP)\n    - Gradient accumulation\n    - Detailed monitoring\n    - Model checkpointing\n    - Compatibility with older GPUs\n    \"\"\"\n    # Create directory for checkpoints\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Check GPU compatibility for torch.compile\n    if use_compile and hasattr(torch, 'compile'):\n        # Only enable on supported hardware (CUDA capability >= 7.0)\n        if torch.cuda.is_available():\n            device_cap = torch.cuda.get_device_capability(torch.cuda.current_device())\n            if device_cap[0] < 7:\n                use_compile = False\n                print(f\"GPU CUDA capability {device_cap[0]}.{device_cap[1]} is too old for torch.compile(). Disabling.\")\n            else:\n                model = torch.compile(model)\n                print(\"Using torch.compile() to optimize model execution\")\n    \n    # Loss function, optimizer and scheduler\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=0.01,             # base learning rate from the paper\n        momentum=0.9,        # typical momentum\n        weight_decay=5e-4    # small L2 regularization\n    )\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=0.1,          \n        total_steps=math.ceil(len(train_loader) / gradient_accumulation_steps) * num_epochs,  # Use math.ceil instead of //\n        pct_start=0.2,       \n        anneal_strategy='cos',\n        div_factor=10,       \n        final_div_factor=1e4 \n    )\n    \n    # Initialize AMP scaler if using AMP\n    scaler = GradScaler() if use_amp and device != 'cpu' else None\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # For tracking best model\n    best_val_error = float('inf')\n    best_model_state = None\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        epoch_start = time.time()\n        \n        # === TRAINING PHASE ===\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Progress bar for training\n        train_pbar = tqdm(\n            total=len(train_loader),\n            desc=f\"Training\",\n            unit=\"batch\",\n            leave=True,\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n        )\n        \n        # Track batch-level metrics\n        batch_times = []\n        \n        # Zero gradients at the start of epoch\n        optimizer.zero_grad()\n        \n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            batch_start = time.time()\n            \n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass with AMP if enabled\n            step_count = 0\n            total_steps = math.ceil(len(train_loader) / gradient_accumulation_steps) * num_epochs\n            if use_amp and device != 'cpu':\n                with autocast():\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    # Scale loss by gradient accumulation steps\n                    loss = loss / gradient_accumulation_steps\n                \n                # Backward pass with AMP\n                scaler.scale(loss).backward()\n                \n                # Step optimizer every gradient_accumulation_steps\n                if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n                    # Unscale gradients for proper gradient clipping\n                    scaler.unscale_(optimizer)\n                    \n                    # Clip gradients to prevent exploding gradients\n                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    \n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                    if step_count < total_steps:\n                        scheduler.step()\n                        step_count += 1\n            else:\n                # Standard forward pass\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                # Scale loss by gradient accumulation steps\n                loss = loss / gradient_accumulation_steps\n                \n                # Standard backward pass\n                loss.backward()\n                \n                # Step optimizer every gradient_accumulation_steps\n                if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n                    # Clip gradients to prevent exploding gradients\n                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    \n                    optimizer.step()\n                    optimizer.zero_grad()\n                    if step_count < total_steps:\n                        scheduler.step()\n                        step_count += 1\n            \n            # Calculate batch statistics\n            batch_loss = loss.item() * gradient_accumulation_steps\n            running_loss += batch_loss\n            _, predicted = outputs.max(1)\n            batch_correct = predicted.eq(labels).sum().item()\n            batch_size = labels.size(0)\n            total += batch_size\n            correct += batch_correct\n            \n            # Measure batch time\n            batch_end = time.time()\n            batch_time = batch_end - batch_start\n            batch_times.append(batch_time)\n            \n            # Update progress bar with detailed metrics\n            batch_acc = 100. * batch_correct / batch_size\n            current_lr = optimizer.param_groups[0]['lr']\n            \n            train_pbar.set_postfix({\n                'loss': f\"{batch_loss:.4f}\",\n                'acc': f\"{batch_acc:.2f}%\",\n                'lr': f\"{current_lr:.6f}\",\n                'time': f\"{batch_time:.3f}s\"\n            })\n            train_pbar.update()\n        torch.save(model.state_dict(), f'/kaggle/working/checkpoints/henet_final_model_epoch_{epoch+1}.pt')\n        \n        train_pbar.close()\n        \n        # Calculate training statistics\n        train_loss = running_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        train_error = 100. - train_acc\n        avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n        \n        # === VALIDATION PHASE ===\n        val_metrics = validate_model(model, val_loader, criterion, device, use_amp)\n        val_loss = val_metrics['val_loss']\n        \n        # Print epoch summary (now only loss)\n        epoch_time = time.time() - epoch_start\n\n        # Print epoch summary\n        print(f\"Epoch {epoch+1}/{num_epochs} completed in {timedelta(seconds=int(epoch_time))}\")\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Train Error: {train_error:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f}\")\n        \n        # Save best model\n        if val_loss < best_val_error:\n             best_val_error = val_loss\n             best_model_state = {\n                 'epoch': epoch + 1,\n                 'model_state_dict': model.state_dict(),\n                 'optimizer_state_dict': optimizer.state_dict(),\n                 'scheduler_state_dict': scheduler.state_dict(),\n                 'val_loss': val_loss,\n                 'train_error': train_error,\n             }\n             save_path = os.path.join(save_dir, f\"best_model_epoch{epoch+1}_val{val_loss:.4f}.pt\")\n        \n        # Free up memory\n        torch.cuda.empty_cache()\n    \n    # Restore best model if available\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state['model_state_dict'])\n        print(f\"Restored best model with validation error: {best_val_error:.2f}%\")\n    \n    return model\n\ndef validate_model(model, val_loader, criterion, device, use_amp=False):\n    \"\"\"\n    Evaluation-phase (during training): only average loss.\n    Returns: {'val_loss': float}\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_samples = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n            # inputs: [B, P, C, H, W] or [B, C, H, W]\n            B = labels.size(0)\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # flatten patches if needed\n            if inputs.ndim == 5:\n                B, P, C, H, W = inputs.shape\n                inputs = inputs.view(B*P, C, H, W)\n\n            # forward\n            if use_amp and device != 'cpu':\n                with autocast():\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels.repeat_interleave(inputs.size(0)//B))\n            else:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels.repeat_interleave(inputs.size(0)//B))\n\n            total_loss += loss.item() * B\n            total_samples += B\n\n    return {'val_loss': total_loss / total_samples}\n\ndef evaluate_model_optimized(model, test_loader, device='cuda', use_amp=False):\n    \"\"\"\n    Testing-phase: aggregates 15 patches per sample, computes loss + top1/top5 metrics.\n    Returns (top1_error, top5_error).\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    model.eval()\n    total_loss = 0.0\n    total_samples = 0\n    correct1 = 0\n    correct5 = 0\n\n    test_pbar = tqdm(\n        total=len(test_loader),\n        desc=\"Testing\",\n        unit=\"batch\",\n        leave=True,\n        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n    )\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            # inputs: [B, P, C, H, W]\n            B, P, C, H, W = inputs.shape\n            inputs = inputs.view(B*P, C, H, W).to(device)\n            labels = labels.to(device)\n\n            # Forward all patches\n            if use_amp and device != 'cpu':\n                with autocast():\n                    logits = model(inputs)\n            else:\n                logits = model(inputs)\n\n            # Reshape + average over patches → [B, num_classes]\n            num_classes = logits.size(1)\n            avg_logits = logits.view(B, P, num_classes).mean(dim=1)\n\n            # Compute loss on averaged logits\n            loss = criterion(avg_logits, labels)\n            total_loss += loss.item() * B\n\n            # Top-1\n            _, pred1 = avg_logits.max(1)\n            correct1 += pred1.eq(labels).sum().item()\n\n            # Top-5\n            _, pred5 = avg_logits.topk(5, dim=1, largest=True, sorted=True)\n            correct5 += (pred5 == labels.view(-1, 1)).any(dim=1).sum().item()\n\n            total_samples += B\n\n            test_pbar.set_postfix({\n                'loss':    f\"{loss.item():.4f}\",\n                'top1_acc': f\"{100.*correct1/total_samples:.2f}%\",\n                'top5_acc': f\"{100.*correct5/total_samples:.2f}%\"\n            })\n            test_pbar.update()\n\n    test_pbar.close()\n\n    avg_loss = total_loss / total_samples\n    top1_acc = 100. * correct1 / total_samples\n    top5_acc = 100. * correct5 / total_samples\n\n    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n    print(f\"Top-1 Accuracy: {top1_acc:.2f}% | Top-1 Error: {100.-top1_acc:.2f}%\")\n    print(f\"Top-5 Accuracy: {top5_acc:.2f}% | Top-5 Error: {100.-top5_acc:.2f}%\")\n\n    return 100. - top1_acc, 100. - top5_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:39:28.9897Z","iopub.execute_input":"2025-05-19T08:39:28.990402Z","iopub.status.idle":"2025-05-19T08:39:29.015077Z","shell.execute_reply.started":"2025-05-19T08:39:28.990368Z","shell.execute_reply":"2025-05-19T08:39:29.014508Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# main.py\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport os\nimport gc\n\n# Set device and enable deterministic mode for reproducibility\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Print GPU info\nif torch.cuda.is_available():\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    device_cap = torch.cuda.get_device_capability(0)\n    print(f\"CUDA Capability: {device_cap[0]}.{device_cap[1]}\")\n    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n\n# Clean up memory before starting\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Hyperparameters\nnum_classes = 2383\nbatch_size = 64  # Base batch size\nnum_epochs = 10\nbeta = 0.5  # HE Block mask weight\n\n# Create the model with optimized HEBlock\nmodel = HENet(num_classes=num_classes, beta=beta)\n\n# Configure DataLoader with optimal settings\ntrain_loader = train_font_loader\nval_loader = val_font_loader\ntest_loader = test_font_loader\n\n# Train the model with optimizations\ntrained_model = train_model_optimized(\n    model, \n    train_loader, \n    val_loader, \n    num_epochs=num_epochs, \n    device=device,\n    use_amp=True,          # Enable Mixed Precision\n    use_compile=False,     # Disable torch.compile for P100\n    gradient_accumulation_steps=2,  # Effective batch size = 64 * 4 = 256\n    save_dir='/kaggle/working/checkpoints'\n)\n\n# Save the final trained model\n# torch.save(trained_model.state_dict(), '/kaggle/working/checkpoints/henet_final_model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:39:33.853797Z","iopub.execute_input":"2025-05-19T08:39:33.854539Z","iopub.status.idle":"2025-05-19T09:17:38.802118Z","shell.execute_reply.started":"2025-05-19T08:39:33.854517Z","shell.execute_reply":"2025-05-19T09:17:38.800686Z"}},"outputs":[{"name":"stdout","text":"Using GPU: Tesla P100-PCIE-16GB\nCUDA Capability: 6.0\nTotal memory: 17.06 GB\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_293/1912267699.py:58: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler() if use_amp and device != 'cpu' else None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4af8bbffdda3455f9ab5c38010361663"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_293/1912267699.py:102: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/tmp/ipykernel_293/2273509577.py:88: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast() if self.use_amp and torch.cuda.is_available() else torch.no_grad():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_293/1912267699.py:238: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 completed in 0:03:51\nTrain Loss: 4.9354 | Train Acc: 13.16% | Train Error: 86.84%\nVal Loss: 2.3638\n\nEpoch 2/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1948f4e11794ecb915e51c24149a2cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/10 completed in 0:03:50\nTrain Loss: 1.3092 | Train Acc: 62.73% | Train Error: 37.27%\nVal Loss: 0.9622\n\nEpoch 3/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17064b67aa4c492f829f58c0ffe89eb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/10 completed in 0:03:51\nTrain Loss: 0.5712 | Train Acc: 83.09% | Train Error: 16.91%\nVal Loss: 0.5134\n\nEpoch 4/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7095764afaad459ba69aa8c00205c80b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/10 completed in 0:03:50\nTrain Loss: 0.3429 | Train Acc: 89.69% | Train Error: 10.31%\nVal Loss: 0.3621\n\nEpoch 5/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd0632f2cf244b59822f7896ef46ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/10 completed in 0:03:50\nTrain Loss: 0.2491 | Train Acc: 92.38% | Train Error: 7.62%\nVal Loss: 0.2321\n\nEpoch 6/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e104520669041cfa633576078b0ed5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/10 completed in 0:03:44\nTrain Loss: 0.1919 | Train Acc: 94.14% | Train Error: 5.86%\nVal Loss: 0.1774\n\nEpoch 7/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e242f47ad0647c4918b1b500bdc0721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/10 completed in 0:03:48\nTrain Loss: 0.1531 | Train Acc: 95.33% | Train Error: 4.67%\nVal Loss: 0.1392\n\nEpoch 8/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e1325d414c94d99a86f845b9d43cfc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/10 completed in 0:03:41\nTrain Loss: 0.1222 | Train Acc: 96.36% | Train Error: 3.64%\nVal Loss: 0.0971\n\nEpoch 9/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b9da0460fa34625a61405b7a50f0872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/10 completed in 0:03:44\nTrain Loss: 0.0960 | Train Acc: 97.27% | Train Error: 2.73%\nVal Loss: 0.0777\n\nEpoch 10/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/156 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6af787962a243908353c1d1d9eb4180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/10 completed in 0:03:49\nTrain Loss: 0.0834 | Train Acc: 97.70% | Train Error: 2.30%\nVal Loss: 0.0749\nRestored best model with validation error: 0.07%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"top1_error, top5_error = evaluate_model_optimized(\n    trained_model, \n    test_font_loader, \n    device=device,\n    use_amp=True\n)\n\nprint(f\"Final Results:\")\nprint(f\"Top-1 Error: {top1_error:.2f}%\")\nprint(f\"Top-5 Error: {top5_error:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:22:35.74796Z","iopub.execute_input":"2025-05-19T09:22:35.748329Z","iopub.status.idle":"2025-05-19T09:22:44.910877Z","shell.execute_reply.started":"2025-05-19T09:22:35.748304Z","shell.execute_reply":"2025-05-19T09:22:44.909909Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/26 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c967d20021084da19b920f148553a382"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_293/1912267699.py:279: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/tmp/ipykernel_293/2273509577.py:88: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast() if self.use_amp and torch.cuda.is_available() else torch.no_grad():\n","output_type":"stream"},{"name":"stdout","text":"\nTest Loss: 8.3165\nTop-1 Accuracy: 1.28% | Top-1 Error: 98.72%\nTop-5 Accuracy: 5.72% | Top-5 Error: 94.28%\nFinal Results:\nTop-1 Error: 98.72%\nTop-5 Error: 94.28%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!zip -r /kaggle/working/checkpoints.zip /kaggle/working/checkpoints","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# main \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nnum_classes = 2383  # Updated to match your requirement\nnum_epochs = 1\nbeta = 0.5  # HE Block mask weight\n\n# Create the model\nmodel = HENet(num_classes=num_classes, beta=beta)\n\n# Your data loader provides images with shape (batch_size, 1, 105, 105)\n# so we don't need to apply any transformations\n\n# Assuming your dataloaders are already set up\n# Replace these with your actual dataloaders\ntrain_loader = train_font_loader  # Your train dataloader\nval_loader = val_font_loader    # Your validation dataloader\ntest_loader = test_font_loader   # Your test dataloader\n\n# Train the model\ntrained_model = train_model(model, train_loader, val_loader, num_epochs=num_epochs, device=device)\n\n# Save the trained model\ntorch.save(trained_model.state_dict(), '/kaggle/working/henet_model.pth')\n\n# Evaluate the model\ntop1_error, top5_error = evaluate_model(trained_model, test_loader, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}