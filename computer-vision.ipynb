{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Font_Detect_Updated v1.ipynb","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":11786582,"datasetId":7359793,"databundleVersionId":12273611,"isSourceIdPinned":true},{"sourceType":"datasetVersion","sourceId":11356819,"datasetId":7107437,"databundleVersionId":11785132},{"sourceType":"modelInstanceVersion","sourceId":399921,"databundleVersionId":12351141,"modelInstanceId":327270}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ndannnop/computer-vision?scriptVersionId=240391377\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pybcf pysam keras-layer-normalization","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport random\nimport math # Needed for ceiling division\n    \nclass BCFImagePatchDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for loading either images from a BCF file or from a folder of JPEG files,\n    and extracting patches on the fly. Loads images as grayscale.\n    \"\"\"\n    def __init__(self, data_source, label_file=None, num_patch=3, patch_size=(105, 105)):\n        \"\"\"\n        Initializes the dataset. Can handle both BCF files and JPEG images.\n\n        Args:\n            data_source (str): Path to the BCF file or directory containing JPEG images.\n            label_file (str): Path to the label file (required only for BCF files).\n            num_patch (int): Number of patches to extract per image.\n            patch_size (tuple): (height, width) of patches.\n        \"\"\"\n        self.data_source = data_source\n        self.label_file = label_file\n        self.num_patch = num_patch\n        self.patch_size = patch_size\n\n        self.labels = None\n        self.image_filenames = []\n\n        # Determine whether the dataset is based on BCF or JPEG files\n        if data_source.endswith('.bcf'):\n            self._read_bcf_metadata()\n        else:\n            self._read_image_filenames()\n\n    def _read_bcf_metadata(self):\n        \"\"\"Reads labels and image size/offset information from BCF files.\"\"\"\n        try:\n            # Read label file\n            if not self.label_file:\n                raise ValueError(\"Label file is required for BCF data source.\")\n            with open(self.label_file, 'rb') as f:\n                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n                print(f\"Read {len(self.labels)} labels.\")\n\n            # Read BCF header\n            with open(self.data_source, 'rb') as f:\n                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n                print(f\"BCF header indicates {self.num_images} images.\")\n\n                if len(self.labels) != self.num_images:\n                    raise ValueError(f\"Mismatch between number of labels ({len(self.labels)}) and images in BCF header ({self.num_images}).\")\n\n                # Read all image sizes\n                sizes_bytes = f.read(self.num_images * 8)\n                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n                print(f\"Read {len(self.image_sizes)} image sizes.\")\n\n                # Calculate the starting offset of the actual image data blob\n                self.data_start_offset = 8 + self.num_images * 8\n                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n                print(\"Calculated image offsets.\")\n\n        except FileNotFoundError as e:\n            print(f\"Error: File not found - {e}\")\n            raise\n        except Exception as e:\n            print(f\"Error reading metadata: {e}\")\n            raise\n\n    def _read_image_filenames(self):\n        \"\"\"Reads image filenames from a folder (only for JPEG images).\"\"\"\n        try:\n            # List all JPEG images in the folder\n            self.image_filenames = [f for f in os.listdir(self.data_source) if f.endswith('.jpeg') or f.endswith('.jpg')]\n            print(f\"Found {len(self.image_filenames)} JPEG images.\")\n\n            if len(self.image_filenames) == 0:\n                raise ValueError(\"No JPEG images found in the specified folder.\")\n        except FileNotFoundError as e:\n            print(f\"Error: Folder not found - {e}\")\n            raise\n        except Exception as e:\n            print(f\"Error reading filenames: {e}\")\n            raise\n\n    def __len__(self):\n        \"\"\"Returns the total number of images in the dataset.\"\"\"\n        if hasattr(self, 'num_images'):\n            return self.num_images  # For BCF files\n        return len(self.image_filenames)  # For JPEG images\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Loads one image as grayscale, extracts patches, and returns patches with the label.\n    \n        Args:\n            idx (int): The index of the image to retrieve.\n    \n        Returns:\n            tuple: (list[np.ndarray], int): A tuple containing:\n                     - A list of NumPy arrays, each representing a patch (H, W).\n                     - The integer label for the image (or 0 for JPEG).\n                   Returns ([], -1) if image reading or patch extraction fails.\n        \"\"\"\n        if hasattr(self, 'num_images'):  # BCF source\n            if idx >= self.num_images or idx < 0:\n                raise IndexError(f\"Index {idx} out of bounds for {self.num_images} images.\")\n    \n            label = self.labels[idx]\n            offset = self.image_offsets[idx]\n            size = self.image_sizes[idx]\n    \n            try:\n                with open(self.data_source, 'rb') as f:\n                    f.seek(self.data_start_offset + offset)\n                    image_bytes = f.read(size)\n    \n                img = Image.open(BytesIO(image_bytes)).convert('L')\n                img_array = np.array(img)\n                patches = extract_patches(img_array, self.num_patch, self.patch_size)\n    \n                return patches, label\n    \n            except Exception as e:\n                print(f\"Error processing image index {idx}: {e}\")\n                return [], -1  # Indicate error\n    \n        else:  # JPEG source\n            if idx >= len(self.image_filenames) or idx < 0:\n                raise IndexError(f\"Index {idx} out of bounds for {len(self.image_filenames)} images.\")\n    \n            image_filename = self.image_filenames[idx]\n            image_path = os.path.join(self.data_source, image_filename)\n    \n            try:\n                img = Image.open(image_path).convert('L')\n                img_array = np.array(img)\n                patches = extract_patches(img_array, self.num_patch, self.patch_size)\n    \n                return patches, 0\n    \n            except Exception as e:\n                print(f\"Error processing image index {idx}: {e}\")\n                return [], -1  # Indicate error\n\ndef patch_collate_fn(batch, patch_size_tuple):\n    \"\"\"\n    Collates data from the BCFImagePatchDataset (handling grayscale) or JPEG-based dataset.\n\n    Takes a batch of [(patches_list_img1, label1), (patches_list_img2, label2), ...],\n    flattens the patches, converts them to a tensor, adds a channel dimension,\n    normalizes, and returns a single batch tensor for patches and labels.\n\n    Args:\n        batch (list): A list of tuples, where each tuple is the output\n                      of BCFImagePatchDataset.__getitem__.\n        patch_size_tuple (tuple): The (height, width) of patches, needed for empty tensor shape.\n\n    Returns:\n        tuple: (torch.Tensor, torch.Tensor): A tuple containing:\n                 - Patches tensor (BatchSize * NumPatches, 1, Height, Width)\n                 - Labels tensor (BatchSize * NumPatches)\n    \"\"\"\n    all_patches = []\n    all_labels = []\n    valid_batch_items = 0\n\n    for item in batch:\n        patches, label = item\n        # Ensure item is valid (e.g., image wasn't too small, no read errors)\n        if patches and label != -1:\n            # Only add patches if the list is not empty\n            all_patches.extend(patches)\n            # Repeat the label for each patch extracted from the image\n            all_labels.extend([label] * len(patches))\n            valid_batch_items += 1\n\n    # If no valid patches were collected in the batch (e.g., all images too small)\n    if not all_patches:\n        # Return empty tensors of appropriate type but 0 size in the batch dimension\n        patch_h, patch_w = patch_size_tuple\n        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n\n    # Convert list of NumPy arrays (each H, W) to a single NumPy array\n    patches_np = np.array(all_patches)  # Shape: (TotalPatches, H, W)\n\n    # Convert to PyTorch tensor, normalize\n    patches_tensor = torch.tensor(patches_np).float() / 255.0  # Shape: (TotalPatches, H, W)\n\n    # Add channel dimension: (TotalPatches, H, W) -> (TotalPatches, 1, H, W)\n    patches_tensor = patches_tensor.unsqueeze(1)\n\n    # Convert labels to PyTorch tensor\n    labels_tensor = torch.tensor(all_labels, dtype=torch.long)  # Use long for classification labels\n\n    return patches_tensor, labels_tensor","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import easyocr\nimport numpy as np\nimport os\nimport tempfile\nfrom PIL import Image\n\n# Global OCR reader for efficiency\n_ocr_reader = None\n\ndef get_ocr_reader(languages=[\"en\"]):\n    \"\"\"Get or initialize the EasyOCR reader\"\"\"\n    global _ocr_reader\n    if _ocr_reader is None:\n        _ocr_reader = easyocr.Reader(languages)\n    return _ocr_reader\n\ndef extract_patches(image_array, num_patch=3, patch_size=(105, 105), \n                    extract_text=True, min_text_coverage=0.3, max_attempts=20):\n    \"\"\"\n    Extract patches after resizing image to fixed height (105), preserving aspect ratio.\n    Then randomly crop patches of size 105x105.\n    \"\"\"\n    patch_h, patch_w = patch_size\n\n    # Determine if grayscale or color\n    if image_array.ndim == 2:\n        h, w = image_array.shape\n        is_grayscale = True\n    elif image_array.ndim == 3:\n        h, w, _ = image_array.shape\n        is_grayscale = False\n    else:\n        print(f\"Unexpected image shape: {image_array.shape}\")\n        return []\n\n    # === Step 1: Resize image to height = 105, maintain aspect ratio ===\n    scale_factor = patch_h / h\n    new_w = int(w * scale_factor)\n    if is_grayscale:\n        resized = cv2.resize(image_array, (new_w, patch_h), interpolation=cv2.INTER_LINEAR)\n    else:\n        resized = cv2.resize(image_array, (new_w, patch_h), interpolation=cv2.INTER_LINEAR)\n\n    # === Step 2: Check if width is enough for patch ===\n    if new_w < patch_w:\n        # Too narrow, cannot crop a patch\n        return []\n\n    # === Step 3: If not extracting text, return random crops ===\n    if not extract_text:\n        patches = []\n        for _ in range(num_patch):\n            x = np.random.randint(0, new_w - patch_w + 1)\n            if is_grayscale:\n                patch = resized[:, x:x + patch_w]\n            else:\n                patch = resized[:, x:x + patch_w, :]\n            patches.append(patch)\n        return patches\n\n    # === Step 4: Try to find text patches using OCR ===\n    reader = get_ocr_reader()\n    text_patches = []\n    attempts = 0\n\n    while len(text_patches) < num_patch and attempts < max_attempts:\n        x = np.random.randint(0, new_w - patch_w + 1)\n        if is_grayscale:\n            patch = resized[:, x:x + patch_w]\n        else:\n            patch = resized[:, x:x + patch_w, :]\n\n        # Save patch to temporary file for OCR\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:\n            tmp_path = tmp.name\n            patch_img = Image.fromarray(patch)\n            patch_img.save(tmp_path)\n\n        try:\n            ocr_results = reader.readtext(tmp_path)\n            os.unlink(tmp_path)\n\n            patch_area = patch_h * patch_w\n            text_area = 0\n            for bbox, text, conf in ocr_results:\n                if conf < 0.5:\n                    continue\n                bbox = [[int(p[0]), int(p[1])] for p in bbox]\n                min_x = max(0, min(p[0] for p in bbox))\n                max_x = min(patch_w, max(p[0] for p in bbox))\n                min_y = max(0, min(p[1] for p in bbox))\n                max_y = min(patch_h, max(p[1] for p in bbox))\n                if max_x > min_x and max_y > min_y:\n                    text_area += (max_x - min_x) * (max_y - min_y)\n\n            if text_area / patch_area >= min_text_coverage:\n                text_patches.append(patch)\n\n        except Exception as e:\n            print(f\"OCR error: {e}\")\n            try:\n                os.unlink(tmp_path)\n            except:\n                pass\n\n        attempts += 1\n\n    return text_patches\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# augmentation functions\nfrom PIL import ImageFilter, Image\nimport random\nimport numpy as np\nimport cv2\n\ndef noise_image(np_img, mean=0, std=2):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n    img_array = np_img.astype(np.float32)\n    noise = np.random.normal(mean, std, img_array.shape)\n    noisy_img = img_array + noise\n    noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n    return cv2.resize(noisy_img, (105, 105))\n\ndef blur_image(np_img):\n    if isinstance(np_img, np.ndarray):\n        np_img = Image.fromarray(np_img.astype('uint8'))\n    blur_img = np_img.filter(ImageFilter.GaussianBlur(radius=1.5))\n    blur_img = blur_img.resize((105, 105))\n    return np.array(blur_img)\n\ndef affine_rotation(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    if len(np_img.shape) == 2:\n        np_img = np.expand_dims(np_img, axis=-1)\n\n    rows, cols = np_img.shape[:2]\n    src_pts = np.float32([[0, 0], [cols - 1, 0], [0, rows - 1]])\n    max_shift = 0.05\n    dst_pts = src_pts + np.random.uniform(-max_shift * cols, max_shift * cols, src_pts.shape).astype(np.float32)\n\n    A = cv2.getAffineTransform(src_pts, dst_pts)\n    warped = cv2.warpAffine(np_img, A, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n\n    if warped.ndim == 3 and warped.shape[-1] == 1:\n        warped = warped[:, :, 0]\n\n    warped = cv2.resize(warped, (105, 105))\n    return warped\n\ndef gradient_fill(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if len(np_img.shape) == 3:\n        gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = np_img\n\n    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n    abs_lap = np.absolute(laplacian) * 0.5\n    lap_uint8 = np.uint8(np.clip(abs_lap, 0, 255))\n    lap_resized = cv2.resize(lap_uint8, (105, 105))\n    return lap_resized\n\ndef variable_aspect_ratio_preprocess(np_img, target_size=(105, 105)):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    if len(np_img.shape) == 3:\n        h, w, c = np_img.shape\n    else:\n        h, w = np_img.shape\n        c = None\n\n    scale_ratio = np.random.uniform(0.95, 1.05)\n    new_width = int(w * scale_ratio)\n\n    resized = cv2.resize(np_img, (new_width, h), interpolation=cv2.INTER_LINEAR)\n    final = cv2.resize(resized, target_size, interpolation=cv2.INTER_LINEAR)\n    return final\n\ndef augmentation_pipeline(np_img):\n    \"\"\"\n    Tăng cường ảnh đầu vào với các phép biến đổi ngẫu nhiên.\n    Hỗ trợ ảnh grayscale hoặc RGB dưới dạng NumPy array hoặc PIL Image.\n    \"\"\"\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    img = variable_aspect_ratio_preprocess(np_img)\n\n    augmentations = [\n        lambda x: noise_image(x),\n        lambda x: blur_image(x),\n        lambda x: affine_rotation(x),\n        lambda x: gradient_fill(x)\n    ]\n\n    num_aug = random.randint(1, 3)\n    selected_augs = random.sample(augmentations, num_aug)\n\n    for aug in selected_augs:\n        img = aug(img)\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        if img.dtype != np.uint8:\n            img = np.clip(img, 0, 255).astype(np.uint8)\n\n    return img","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# combined dataset\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom io import BytesIO\nfrom datasets import Dataset\n\nclass CombinedImageDataset(Dataset):\n    \"\"\"\n    A dataset class that combines both .jpeg files and .bcf files into a single dataset.\n    This class can handle loading and patch extraction from both .jpeg and .bcf files.\n    \"\"\"\n    def __init__(self, jpeg_dir, bcf_file, label_file, num_patch=3, patch_size=(105, 105), \n                 extract_text=False, min_text_coverage=0.3, max_attempts=20, ocr_languages=[\"en\"]):\n        \"\"\"\n        Initializes the dataset by loading both jpeg files and bcf files into one dataset.\n\n        Args:\n            jpeg_dir (str): Directory containing .jpeg files.\n            bcf_file (str): Path to the .bcf file.\n            label_file (str): Path to the label file corresponding to the .bcf file.\n            num_patch (int): Number of patches to extract per image.\n            patch_size (tuple): Tuple (height, width) for the size of the patches.\n            extract_text (bool): Whether to prioritize patches containing text\n            min_text_coverage (float): Minimum ratio of text area to patch area (0-1)\n            max_attempts (int): Maximum number of attempts to find text patches\n            ocr_languages (list): Languages for EasyOCR to detect\n        \"\"\"\n        self.jpeg_dir = jpeg_dir\n        self.bcf_file = bcf_file\n        self.label_file = label_file\n        self.num_patch = num_patch\n        self.patch_size = patch_size\n        self.extract_text = extract_text\n        self.min_text_coverage = min_text_coverage\n        self.max_attempts = max_attempts\n        self.ocr_languages = ocr_languages\n\n        # Initialize OCR reader if needed\n        if extract_text:\n            self.reader = get_ocr_reader(ocr_languages)\n\n        self.jpeg_data = []\n        self.bcf_data = []\n\n        # Load jpeg data\n        self._load_jpeg_data(jpeg_dir)\n\n        # Load bcf data\n        self._load_bcf_data(bcf_file, label_file)\n\n    def _load_jpeg_data(self, jpeg_dir):\n        \"\"\"Loads the .jpeg files from the specified directory.\"\"\"\n        if not os.path.exists(jpeg_dir):\n            print(f\"Warning: JPEG directory {jpeg_dir} does not exist.\")\n            return\n            \n        image_filenames = [f for f in os.listdir(jpeg_dir) if f.lower().endswith(('.jpeg', '.jpg'))]\n        self.jpeg_data = [(os.path.join(jpeg_dir, f), 0) for f in image_filenames]  # Assuming label is 0 for .jpeg files\n        print(f\"Loaded {len(self.jpeg_data)} .jpeg images.\")\n\n    def _load_bcf_data(self, bcf_file, label_file):\n        \"\"\"Loads the .bcf file and the associated label file.\"\"\"\n        if not (os.path.exists(bcf_file) and os.path.exists(label_file)):\n            print(f\"Warning: BCF file {bcf_file} or label file {label_file} does not exist.\")\n            return\n            \n        try:\n            with open(label_file, 'rb') as f:\n                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n                print(f\"Loaded {len(self.labels)} labels from {label_file}.\")\n\n            with open(bcf_file, 'rb') as f:\n                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n                print(f\"Loaded {self.num_images} images from {bcf_file}.\")\n\n                sizes_bytes = f.read(self.num_images * 8)\n                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n\n                self.data_start_offset = 8 + self.num_images * 8\n                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n\n                for idx in range(self.num_images):\n                    self.bcf_data.append((idx, self.labels[idx]))\n                \n            print(f\"Loaded {len(self.bcf_data)} .bcf images.\")\n        except Exception as e:\n            print(f\"Error loading .bcf data: {e}\")\n\n    def __len__(self):\n        \"\"\"Returns the total number of images in the combined dataset.\"\"\"\n        return len(self.jpeg_data) + len(self.bcf_data)\n\n    def _extract_patches(self, img_array):\n        \"\"\"Helper function to extract patches from an image.\"\"\"\n        return extract_patches(\n            img_array, \n            num_patch=self.num_patch, \n            patch_size=self.patch_size,\n            extract_text=self.extract_text, \n            min_text_coverage=self.min_text_coverage,\n            max_attempts=self.max_attempts\n        )\n\n    def __getitem__(self, idx):\n        \"\"\"Fetches one item with robust error handling for corrupted images.\"\"\"\n        # Handle case where idx is a list (batch loading)\n        if isinstance(idx, list):\n            # Handle batch indices properly\n            results = []\n            labels = []\n            for single_idx in idx:\n                try:\n                    patches, label = self.__getitem__(single_idx)  # Call recursively with single index\n                    if patches and label != -1:\n                        results.append(patches)\n                        labels.append(label)\n                except Exception as e:\n                    print(f\"Error processing index {single_idx}: {e}\")\n                    # Skip this item on error\n            \n            # Return whatever valid items we were able to get\n            return results, labels\n        \n        # Original single-item loading logic\n        max_retries = 3  # Try a few times before giving up on an index\n        \n        for retry in range(max_retries):\n            try:\n                if idx < len(self.jpeg_data):\n                    # JPEG image with error handling\n                    img_path, label = self.jpeg_data[idx]\n                    try:\n                        with warnings.catch_warnings():\n                            warnings.simplefilter(\"ignore\")  # Ignore PIL warnings\n                            img = Image.open(img_path)\n                            img.verify()  # Verify image is not corrupted\n                        \n                        # Re-open since verify() closes the file\n                        img = Image.open(img_path).convert('L')\n                        img_array = np.array(img)\n                        patches = self._extract_patches(img_array)\n                        patches = [augmentation_pipeline(patch) for patch in patches]\n                        \n                        # Clean memory\n                        del img, img_array\n                        \n                        return patches, label\n                    \n                    except (OSError, IOError, ValueError) as e:\n                        # Image is corrupted, return empty list\n                        print(f\"Warning: Corrupt image at {img_path}: {e}\")\n                        return [], -1\n                        \n                else:\n                    # BCF image with error handling\n                    bcf_idx = idx - len(self.jpeg_data)\n                    if bcf_idx >= len(self.bcf_data):\n                        return [], -1\n                        \n                    label = self.bcf_data[bcf_idx][1]\n                    offset = self.image_offsets[bcf_idx]\n                    size = self.image_sizes[bcf_idx]\n                    \n                    try:\n                        with open(self.bcf_file, 'rb') as f:\n                            f.seek(self.data_start_offset + offset)\n                            image_bytes = f.read(size)\n                        \n                        # Use BytesIO to catch corruption\n                        buffer = BytesIO(image_bytes)\n                        img = Image.open(buffer)\n                        img.verify()  # Verify it's valid\n                        \n                        # Re-open since verify() closes the file\n                        buffer.seek(0)\n                        img = Image.open(buffer).convert('L')\n                        img_array = np.array(img)\n                        patches = self._extract_patches(img_array)\n                        patches = [augmentation_pipeline(patch) for patch in patches]\n                        \n                        # Clean memory\n                        del img, img_array, buffer, image_bytes\n                        \n                        return patches, label\n                    \n                    except (OSError, IOError, ValueError) as e:\n                        print(f\"Warning: Corrupt BCF image at index {bcf_idx}: {e}\")\n                        return [], -1\n                        \n            except Exception as e:\n                print(f\"Unexpected error processing idx {idx}: {e}\")\n            \n            # If we got here, there was an issue with this index - try a different one\n            # Important: increment as an integer, not trying to add to a list\n            if retry < max_retries - 1:  # Only increment if we have retries left\n                idx = (int(idx) + 1) % len(self)\n        \n        # If all retries failed, return empty\n        return [], -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:17:23.337931Z","iopub.execute_input":"2025-05-18T09:17:23.338417Z","iopub.status.idle":"2025-05-18T09:17:23.362323Z","shell.execute_reply.started":"2025-05-18T09:17:23.338384Z","shell.execute_reply":"2025-05-18T09:17:23.361464Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# memory_efficient_patch_collate_fn\nimport gc\nimport warnings\nfrom functools import partial\n\n# Add this memory-efficient patch collate function\ndef memory_efficient_patch_collate_fn(batch, patch_size_tuple):\n    \"\"\"\n    Memory-efficient version of patch_collate_fn that processes one patch at a time\n    and includes robust error handling.\n    \"\"\"\n    import gc  # Import inside function for worker processes\n    \n    all_patches = []\n    all_labels = []\n    valid_batch_items = 0\n\n    # Process one item at a time to avoid large memory allocations\n    for item in batch:\n        patches, label = item\n        # Ensure item is valid\n        if patches and label != -1:\n            # Process patches one by one\n            for patch in patches:\n                all_patches.append(patch)\n                all_labels.append(label)\n            valid_batch_items += 1\n    \n    # Periodically force garbage collection\n    if len(all_patches) > 100:\n        gc.collect()\n    \n    # Empty batch handling\n    if not all_patches:\n        patch_h, patch_w = patch_size_tuple\n        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n\n    # Process in smaller chunks to reduce peak memory usage\n    max_chunk_size = 64  # Adjust based on your GPU memory\n    num_patches = len(all_patches)\n    patches_tensor_list = []\n    \n    for i in range(0, num_patches, max_chunk_size):\n        chunk = all_patches[i:i+max_chunk_size]\n        # Convert to NumPy array\n        chunk_np = np.stack(chunk)\n        # Convert to tensor, normalize and add channel dimension\n        chunk_tensor = torch.from_numpy(chunk_np).float() / 255.0\n        chunk_tensor = chunk_tensor.unsqueeze(1)\n        patches_tensor_list.append(chunk_tensor)\n        \n        # Clear variables to free memory\n        del chunk, chunk_np\n    \n    # Concatenate chunks\n    patches_tensor = torch.cat(patches_tensor_list, dim=0)\n    labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n    \n    # Clean up\n    del patches_tensor_list, all_patches, all_labels\n    gc.collect()\n    \n    return patches_tensor, labels_tensor\n\n# Add this function to create optimized DataLoaders\nimport torch\nfrom torch.utils.data import DataLoader\nfrom functools import partial\n\ndef create_optimized_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper error handling, avoiding HuggingFace datasets compatibility issues.\n    \n    Args:\n        dataset: The image dataset instance\n        batch_size: Batch size for training\n        num_workers: Number of worker processes\n        val_split: Validation split ratio (0-1)\n        \n    Returns:\n        tuple: (train_loader, val_loader)\n    \"\"\"\n    from torch.utils.data import DataLoader, Subset\n    import numpy as np\n    \n    # Calculate split sizes\n    dataset_size = len(dataset)\n    indices = np.arange(dataset_size)\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets - this avoids Hugging Face's __getitems__ implementation\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    # Custom collate function with error handling\n    def safe_collate(batch):\n        # Filter out empty or invalid items\n        valid_batch = [(patches, label) for patches, label in batch if patches and label != -1]\n        \n        if not valid_batch:\n            # Return empty tensors if no valid items\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n        \n        # Process valid items\n        all_patches = []\n        all_labels = []\n        \n        for patches, label in valid_batch:\n            if isinstance(patches, list) and patches:\n                all_patches.extend(patches)\n                all_labels.extend([label] * len(patches))\n        \n        if not all_patches:\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n            \n        # Convert to PyTorch tensors\n        try:\n            patches_np = np.array(all_patches)\n            patches_tensor = torch.tensor(patches_np, dtype=torch.float) / 255.0\n            \n            # Add channel dimension if needed\n            if len(patches_tensor.shape) == 3:  # (B, H, W)\n                patches_tensor = patches_tensor.unsqueeze(1)  # -> (B, 1, H, W)\n                \n            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n            return patches_tensor, labels_tensor\n        except Exception as e:\n            print(f\"Error in collate function: {e}\")\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n    \n    # Create DataLoaders with minimal worker configuration for stability\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data wrapper\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport gc\n\n\nclass DatasetWrapper(Dataset):\n    \"\"\"\n    A wrapper for your dataset to ensure compatibility with DataLoader\n    \"\"\"\n    def __init__(self, original_dataset):\n        self.dataset = original_dataset\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # Get a single item by index, handling both direct dataset access\n        # and access through Subset indices\n        try:\n            # Handle if we're accessing through a Subset\n            if hasattr(self.dataset, 'dataset') and hasattr(self.dataset, 'indices'):\n                original_idx = self.dataset.indices[idx]\n                return self.dataset.dataset[original_idx]\n            # Normal access\n            return self.dataset[idx]\n        except Exception as e:\n            print(f\"Error accessing item {idx}: {e}\")\n            # Return a placeholder for invalid items\n            return [], -1\n\n\ndef create_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper handling for HuggingFace datasets.\n    \"\"\"\n    # Ensure the dataset is properly wrapped\n    wrapped_dataset = DatasetWrapper(dataset)\n    \n    # Calculate split sizes\n    dataset_size = len(wrapped_dataset)\n    indices = list(range(dataset_size))\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets\n    train_dataset = Subset(wrapped_dataset, train_indices)\n    val_dataset = Subset(wrapped_dataset, val_indices)\n    \n    # Custom collate function\n    def custom_collate_fn(batch):\n        # Filter out invalid items\n        batch = [(img, label) for img, label in batch if img is not None and len(img) > 0 and label != -1]\n        \n        if not batch:\n            # Return empty tensors with appropriate dimensions\n            return torch.empty((0, 3, 105, 105), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n        \n        # Extract images and labels\n        images, labels = zip(*batch)\n        \n        # Convert to tensors\n        images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in images])\n        labels_tensor = torch.tensor(labels, dtype=torch.long)\n        \n        # Normalize images if needed\n        if images_tensor.max() > 1.0:\n            images_tensor = images_tensor / 255.0\n            \n        return images_tensor, labels_tensor\n    \n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=custom_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=custom_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    return train_loader, val_loader\n\n\n# For datasets where you're dealing with patches\ndef create_patch_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1, patch_size=(105, 105)):\n    \"\"\"\n    Creates DataLoaders specifically for patch-based datasets where each item\n    may contain multiple patches.\n    \"\"\"\n    # Ensure the dataset is properly wrapped\n    wrapped_dataset = DatasetWrapper(dataset)\n    \n    # Calculate split sizes\n    dataset_size = len(wrapped_dataset)\n    indices = list(range(dataset_size))\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets\n    train_dataset = Subset(wrapped_dataset, train_indices)\n    val_dataset = Subset(wrapped_dataset, val_indices)\n    \n    # Memory efficient collate function for patches\n    def patch_collate_fn(batch):\n        all_patches = []\n        all_labels = []\n        \n        # Process one batch item at a time\n        for patches, label in batch:\n            if patches is not None and len(patches) > 0 and label != -1:\n                # Handle both single patches and lists of patches\n                if isinstance(patches, list):\n                    all_patches.extend(patches)\n                    all_labels.extend([label] * len(patches))\n                else:\n                    all_patches.append(patches)\n                    all_labels.append(label)\n        \n        # Return empty tensors if batch is empty\n        if not all_patches:\n            return torch.empty((0, 3, patch_size[0], patch_size[1]), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n            \n        try:\n            # Process in chunks to reduce memory usage\n            max_chunk_size = min(64, len(all_patches))\n            patches_tensors = []\n            \n            for i in range(0, len(all_patches), max_chunk_size):\n                chunk = all_patches[i:i+max_chunk_size]\n                chunk_tensor = torch.stack([torch.tensor(p, dtype=torch.float) for p in chunk])\n                \n                # Normalize if needed\n                if chunk_tensor.max() > 1.0:\n                    chunk_tensor = chunk_tensor / 255.0\n                    \n                patches_tensors.append(chunk_tensor)\n                \n            # Combine chunks\n            patches_tensor = torch.cat(patches_tensors, dim=0).unsqueeze(1)\n            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n            \n            # Clean up to save memory\n            del patches_tensors, all_patches, all_labels\n            gc.collect()\n            \n            return patches_tensor, labels_tensor\n        except Exception as e:\n            print(f\"Error in collate function: {e}\")\n            return torch.empty((0, 3, patch_size[0], patch_size[1]), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n    \n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=patch_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=patch_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    return train_loader, val_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:55.545094Z","iopub.execute_input":"2025-05-18T07:58:55.545746Z","iopub.status.idle":"2025-05-18T07:58:55.565739Z","shell.execute_reply.started":"2025-05-18T07:58:55.545718Z","shell.execute_reply":"2025-05-18T07:58:55.564721Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Check all elements of combined_dataset for shape consistency\ndef check_shapes(dataset):\n    \"\"\"\n    Check if all patches in the dataset have the same shape.\n    \n    Args:\n        dataset: The dataset instance to check.\n        \n    Returns:\n        bool: True if all patches have the same shape, False otherwise.\n    \"\"\"\n    first_shape = None\n    for idx in range(len(dataset)):\n        patches, _ = dataset[idx]\n        if not patches:\n            continue  # Skip empty patches\n        current_shape = patches[0].shape\n        if first_shape is None:\n            first_shape = current_shape\n        elif current_shape != first_shape:\n            print(f\"Shape mismatch at index {idx}: {current_shape} != {first_shape}\")\n            return False\n    return True \ncheck_shapes(combined_dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load dataset -> create dataloader -> training script\nimport torch\nimport torch.nn as nn\nimport os\nimport gc\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\n\n# Clean memory before starting\ngc.collect()\ntorch.cuda.empty_cache()\n\njpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\nbcf_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf\"\nlabel_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label\"\n\n# Print GPU info\nif torch.cuda.is_available():\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n    print(f\"Available memory: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n\n# Create dataset with smaller patch size and fewer patches per image\ncombined_dataset = CombinedImageDataset(\n    jpeg_dir=jpeg_dir,\n    bcf_file=bcf_file,\n    label_file=label_file,\n    num_patch=3,  # Number of patches per image\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:17:38.042754Z","iopub.execute_input":"2025-05-18T09:17:38.043253Z","iopub.status.idle":"2025-05-18T09:17:42.023468Z","shell.execute_reply.started":"2025-05-18T09:17:38.043228Z","shell.execute_reply":"2025-05-18T09:17:42.0227Z"}},"outputs":[{"name":"stdout","text":"Using GPU: Tesla P100-PCIE-16GB\nTotal memory: 17.06 GB\nAvailable memory: 2.37 GB\nLoaded 82389 .jpeg images.\nLoaded 202000 labels from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label.\nLoaded 202000 images from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf.\nLoaded 202000 .bcf images.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Create memory-optimized dataloaders with smaller batch size\ntrain_loader, val_loader = create_patch_dataloaders(\n    combined_dataset,\n    batch_size=64,\n    num_workers=2,\n    val_split=0.1,\n    patch_size=(105, 105)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:17:48.747332Z","iopub.execute_input":"2025-05-18T09:17:48.74788Z","iopub.status.idle":"2025-05-18T09:17:48.78171Z","shell.execute_reply.started":"2025-05-18T09:17:48.747858Z","shell.execute_reply":"2025-05-18T09:17:48.781049Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"!touch /kaggle/working/dataloaders.pkl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:18:45.950327Z","iopub.execute_input":"2025-05-18T09:18:45.950872Z","iopub.status.idle":"2025-05-18T09:18:46.108273Z","shell.execute_reply.started":"2025-05-18T09:18:45.950848Z","shell.execute_reply":"2025-05-18T09:18:46.107365Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import pickle\nimport torch\nfrom torch.utils.data import DataLoader, Subset\n\ndef save_dataloaders(train_loader, val_loader, filename='dataloaders.pkl'):\n    \"\"\"\n    Save dataset indices and DataLoader configurations to a pickle file.\n    \n    Args:\n        train_loader: Training DataLoader\n        val_loader: Validation DataLoader\n        filename: Name of the output pickle file\n    \"\"\"\n    # Extract the dataset and indices from the loaders\n    if isinstance(train_loader.dataset, Subset):\n        train_indices = train_loader.dataset.indices\n        dataset = train_loader.dataset.dataset  # Get the original dataset\n    else:\n        train_indices = list(range(len(train_loader.dataset)))\n        dataset = train_loader.dataset\n        \n    if isinstance(val_loader.dataset, Subset):\n        val_indices = val_loader.dataset.indices\n    else:\n        val_indices = list(range(len(val_loader.dataset)))\n    \n    # Extract DataLoader configurations - with defaults for attributes that might not be accessible\n    train_config = {\n        'batch_size': getattr(train_loader, 'batch_size', 64),\n        'shuffle': True,  # Default to True for training loader\n        'num_workers': getattr(train_loader, 'num_workers', 0),\n        'pin_memory': getattr(train_loader, 'pin_memory', False),\n        'drop_last': getattr(train_loader, 'drop_last', False),\n    }\n    \n    val_config = {\n        'batch_size': getattr(val_loader, 'batch_size', 64),\n        'shuffle': False,  # Default to False for validation loader\n        'num_workers': getattr(val_loader, 'num_workers', 0),\n        'pin_memory': getattr(val_loader, 'pin_memory', False),\n        'drop_last': getattr(val_loader, 'drop_last', False),\n    }\n    \n    # Try to get collate_fn name safely\n    collate_fn_name = None\n    if hasattr(train_loader, 'collate_fn') and train_loader.collate_fn is not None:\n        if hasattr(train_loader.collate_fn, '__name__'):\n            collate_fn_name = train_loader.collate_fn.__name__\n    \n    # Save dataset, indices, and configs to a file\n    save_data = {\n        'dataset': dataset,\n        'train_indices': train_indices,\n        'val_indices': val_indices,\n        'train_config': train_config,\n        'val_config': val_config,\n        'collate_fn_name': collate_fn_name\n    }\n    \n    with open(filename, 'wb') as f:\n        pickle.dump(save_data, f)\n    \n    print(f\"DataLoader configurations saved to {filename}\")\n\ndef load_dataloaders(filename='dataloaders.pkl', custom_collate_fn=None):\n    \"\"\"\n    Load dataset and recreate DataLoaders from a pickle file.\n    \n    Args:\n        filename: Name of the pickle file to load\n        custom_collate_fn: Custom collate function if needed\n        \n    Returns:\n        tuple: (train_loader, val_loader)\n    \"\"\"\n    with open(filename, 'rb') as f:\n        saved_data = pickle.load(f)\n    \n    dataset = saved_data['dataset']\n    train_indices = saved_data['train_indices']\n    val_indices = saved_data['val_indices']\n    train_config = saved_data['train_config']\n    val_config = saved_data['val_config']\n    \n    # Use provided collate_fn or None\n    collate_fn = custom_collate_fn\n    \n    # Create the subsets\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    # Recreate the DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        collate_fn=collate_fn,\n        **train_config\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        collate_fn=collate_fn,\n        **val_config\n    )\n    \n    print(f\"DataLoaders successfully loaded from {filename}\")\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:24:00.74964Z","iopub.execute_input":"2025-05-18T09:24:00.750288Z","iopub.status.idle":"2025-05-18T09:24:00.759389Z","shell.execute_reply.started":"2025-05-18T09:24:00.750261Z","shell.execute_reply":"2025-05-18T09:24:00.758576Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Save your DataLoaders\nsave_dataloaders(train_loader, val_loader, '/kaggle/working/dataloaders.pkl')\n\n# Later, load them with your custom collate function\n# from functools import partial\n# memory_efficient_collate = partial(memory_efficient_patch_collate_fn, patch_size_tuple=(105, 105))\n\n# train_loader, val_loader = load_dataloaders(\n#     'font_dataloaders.pkl', \n#     custom_collate_fn=memory_efficient_collate\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:24:04.019372Z","iopub.execute_input":"2025-05-18T09:24:04.020099Z","iopub.status.idle":"2025-05-18T09:24:04.613249Z","shell.execute_reply.started":"2025-05-18T09:24:04.020074Z","shell.execute_reply":"2025-05-18T09:24:04.612335Z"}},"outputs":[{"name":"stdout","text":"DataLoader configurations saved to /kaggle/working/dataloaders.pkl\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from functools import partial\nmemory_efficient_collate = partial(memory_efficient_patch_collate_fn, patch_size_tuple=(105, 105))\ntrain_loader, val_loader = load_dataloaders(\n    '/kaggle/working/dataloaders.pkl', \n    custom_collate_fn=memory_efficient_collate\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:24:55.804393Z","iopub.execute_input":"2025-05-18T09:24:55.804654Z","iopub.status.idle":"2025-05-18T09:24:55.932871Z","shell.execute_reply.started":"2025-05-18T09:24:55.804638Z","shell.execute_reply":"2025-05-18T09:24:55.932098Z"}},"outputs":[{"name":"stdout","text":"DataLoaders successfully loaded from /kaggle/working/dataloaders.pkl\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"len(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:25:01.971071Z","iopub.execute_input":"2025-05-18T09:25:01.97163Z","iopub.status.idle":"2025-05-18T09:25:01.976631Z","shell.execute_reply.started":"2025-05-18T09:25:01.971607Z","shell.execute_reply":"2025-05-18T09:25:01.976033Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"4000"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Visualization some samples from the combined dataset (there's bug now, I'll try to fix it)\ndef visualize_simple_images_and_patches(dataset, num_images=2, seed=None):\n    \"\"\"\n    Visualizes full images and their extracted patches in a simple layout.\n    Shows images and their 3 patches in a clean format with error handling.\n    \n    Args:\n        dataset: A CombinedImageDataset or BCFImagePatchDataset instance\n        num_images: Number of images to visualize (default: 2)\n        seed: Random seed for reproducibility\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import random\n    from PIL import Image, ImageFile\n    from io import BytesIO\n    import os\n    \n    # Allow loading of truncated images\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n    \n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Find valid images (with patches)\n    valid_indices = []\n    attempts = 0\n    max_attempts = min(len(dataset) * 2, 100)  # Limit search attempts\n    \n    while len(valid_indices) < num_images and attempts < max_attempts:\n        idx = random.randint(0, len(dataset) - 1)\n        if idx not in valid_indices:  # Avoid duplicates\n            try:\n                patches, label = dataset[idx]\n                if patches and len(patches) > 0:\n                    valid_indices.append(idx)\n            except Exception as e:\n                print(f\"Error loading index {idx}: {e}\")\n            attempts += 1\n    \n    # If we couldn't find enough valid images\n    if len(valid_indices) < num_images:\n        print(f\"Warning: Could only find {len(valid_indices)} valid images with patches\")\n        if len(valid_indices) == 0:\n            print(\"No valid images found. Check your dataset.\")\n            return\n    \n    # Create figure with enough space for all elements\n    fig, axes = plt.subplots(len(valid_indices), 4, figsize=(16, 5 * len(valid_indices)))\n    \n    # If only one image is requested, make axes indexable as 2D\n    if len(valid_indices) == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i, idx in enumerate(valid_indices):\n        try:\n            # Get item directly from dataset\n            patches, label = dataset[idx]\n            \n            # Get the original full image\n            img_array = None\n            \n            if hasattr(dataset, 'jpeg_data') and idx < len(dataset.jpeg_data):\n                # From jpeg_data\n                img_path, _ = dataset.jpeg_data[idx]\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {os.path.basename(img_path)}\"\n                \n            elif hasattr(dataset, 'image_filenames') and not hasattr(dataset, 'num_images'):\n                # From BCFImagePatchDataset with JPEG source\n                img_path = os.path.join(dataset.data_source, dataset.image_filenames[idx])\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {dataset.image_filenames[idx]}\"\n                \n            else:\n                # From BCF file (either CombinedImageDataset or BCFImagePatchDataset)\n                if hasattr(dataset, 'bcf_data'):\n                    # CombinedImageDataset\n                    bcf_idx = idx - len(dataset.jpeg_data)\n                    if bcf_idx < 0 or bcf_idx >= len(dataset.bcf_data):\n                        print(f\"Invalid BCF index: {bcf_idx}\")\n                        continue\n                        \n                    offset = dataset.image_offsets[bcf_idx]\n                    size = dataset.image_sizes[bcf_idx]\n                    data_file = dataset.bcf_file\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {bcf_idx})\"\n                else:\n                    # BCFImagePatchDataset\n                    offset = dataset.image_offsets[idx]\n                    size = dataset.image_sizes[idx]\n                    data_file = dataset.data_source\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {idx})\"\n                \n                with open(data_file, 'rb') as f:\n                    f.seek(data_start + offset)\n                    image_bytes = f.read(size)\n                img = Image.open(BytesIO(image_bytes)).convert('L')\n                img_array = np.array(img)\n            \n            # Plot original image if we successfully loaded it\n            if img_array is not None:\n                axes[i, 0].imshow(img_array, cmap='gray')\n                axes[i, 0].set_title(f\"Original Image\\nLabel: {label}\\nSource: {source}\")\n                axes[i, 0].axis('off')\n            else:\n                axes[i, 0].text(0.5, 0.5, \"Image loading failed\", ha='center', va='center')\n                axes[i, 0].axis('off')\n            \n            # Plot the patches - ensure we have patches to display\n            if patches and len(patches) > 0:\n                for j in range(3):\n                    if j < len(patches):\n                        patch = patches[j]\n                        axes[i, j+1].imshow(patch, cmap='gray')\n                        axes[i, j+1].set_title(f\"Patch {j+1}\\nShape: {patch.shape}\")\n                    else:\n                        # No more patches to display\n                        axes[i, j+1].text(0.5, 0.5, \"No patch\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            else:\n                # No patches for this image\n                for j in range(3):\n                    axes[i, j+1].text(0.5, 0.5, \"No patches extracted\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            \n        except Exception as e:\n            print(f\"Error processing index {idx}: {e}\")\n            # Create error message in subplot\n            for j in range(4):\n                axes[i, j].text(0.5, 0.5, f\"Error: {str(e)[:50]}...\", ha='center', va='center')\n                axes[i, j].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return the indices we used (helpful for debugging)\n    return valid_indices\n\n# Example usage:\nvisualize_simple_images_and_patches(combined_dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch in train_loader:\n    images, labels = batch\n    print(f\"Batch size: {images.size()}, Labels size: {labels.size()}\")\n    break  # Just check the first batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_memory_efficient_model\n\nimport torch.cuda.amp as amp\nimport gc\n\ndef train_memory_efficient_model(model, train_loader, val_loader=None, \n                                num_epochs=5, learning_rate=0.0001,\n                                checkpoint_dir=\"/kaggle/working/\"):\n    \"\"\"\n    Memory-efficient training function for SCAE model.\n    \"\"\"\n    # Ensure checkpoint directory exists\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Setup device and optimization tools\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Training on {device} with {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n    print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n    print(f\"Memory reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Set up mixed precision training\n    scaler = amp.GradScaler()\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n    \n    # Track best model\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    try:\n        for epoch in range(num_epochs):\n            # Clean memory before each epoch\n            gc.collect()\n            torch.cuda.empty_cache()\n            \n            # TRAINING PHASE\n            model.train()\n            running_loss = 0.0\n            valid_batches = 0\n            \n            # Use tqdm for progress tracking\n            pbar = tqdm(train_loader)\n            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n            \n            for batch_idx, (patches, _) in enumerate(pbar):\n                # Skip empty batches\n                if patches.numel() == 0:\n                    continue\n                \n                # Move data to device\n                patches = patches.to(device, non_blocking=True)\n                \n                # Mixed precision forward pass\n                with amp.autocast():\n                    outputs = model(patches)\n                    loss = criterion(outputs, patches)\n                \n                # Backward pass with gradient scaling\n                optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                \n                # Update metrics\n                running_loss += loss.item()\n                valid_batches += 1\n                \n                # Update progress bar\n                pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n                \n                # Aggressive memory cleanup every few batches\n                if batch_idx % 10 == 0:\n                    del outputs, loss, patches\n                    gc.collect()\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n            \n            # Calculate epoch metrics\n            if valid_batches > 0:\n                train_loss = running_loss / valid_batches\n                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}\")\n            else:\n                print(f\"Epoch {epoch+1}/{num_epochs}, No valid batches!\")\n                continue\n                \n            # Save checkpoint every epoch\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': train_loss,\n            }, f\"{checkpoint_dir}/model_epoch_{epoch+1}.pt\")\n            \n            # VALIDATION PHASE\n            if val_loader:\n                val_loss = validate_memory_efficient(model, val_loader, criterion, device)\n                scheduler.step(val_loss)\n                \n                # Early stopping logic\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    patience_counter = 0\n                    torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pt\")\n                    print(f\"New best model saved with val_loss: {val_loss:.6f}\")\n                else:\n                    patience_counter += 1\n                    if patience_counter >= 3:  # Adjust patience as needed\n                        print(\"Early stopping triggered!\")\n                        break\n    \n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        # Save emergency checkpoint\n        torch.save(model.state_dict(), f\"{checkpoint_dir}/emergency_model.pt\")\n        raise\n        \n    return model\n\ndef validate_memory_efficient(model, val_loader, criterion, device):\n    \"\"\"Memory-efficient validation function.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    valid_batches = 0\n    \n    with torch.no_grad():\n        pbar = tqdm(val_loader)\n        pbar.set_description(f\"Validating\")\n        \n        for patches, _ in pbar:\n            if patches.numel() == 0:\n                continue\n                \n            patches = patches.to(device, non_blocking=True)\n            \n            # Using mixed precision even for validation\n            with amp.autocast():\n                outputs = model(patches)\n                loss = criterion(outputs, patches)\n                \n            running_loss += loss.item()\n            valid_batches += 1\n            \n            # Update progress bar\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n            \n            # Clean up\n            del outputs, patches, loss\n    \n    if valid_batches > 0:\n        val_loss = running_loss / valid_batches\n        print(f\"Validation Loss: {val_loss:.6f}\")\n        return val_loss\n    else:\n        print(\"No valid validation batches!\")\n        return float('inf')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SCAE\nimport torch.nn as nn\nclass SCAE(nn.Module):\n    def __init__(self, normalization_type=\"batch_norm\", use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n        super(SCAE, self).__init__()\n\n        def norm_layer(num_features):\n            if normalization_type == \"batch_norm\":\n                return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\":\n                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\":\n                return nn.LayerNorm([num_features, 12, 12])  # Updated for 12x12 feature maps\n            else:\n                return nn.Identity()\n\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n\n        # Encoder: Input 105x105 -> Output 12x12\n        self.encoder = nn.Sequential(\n            # Layer 1: 105x105 -> 48x48\n            nn.Conv2d(1, 64, kernel_size=11, stride=2, padding=0),\n            norm_layer(64),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 2: 48x48 -> 24x24\n            nn.MaxPool2d(2, 2),\n            \n            # Layer 3: 24x24 -> 24x24\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            norm_layer(128),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 4: 24x24 -> 12x12 (added to get 12x12 output)\n            nn.MaxPool2d(2, 2)\n        )\n        \n        # Decoder: Input 12x12 -> Output 105x105\n        self.decoder = nn.Sequential(\n            # Layer 1: 12x12 -> 24x24\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n            norm_layer(64),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 2: 24x24 -> 48x48\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n            norm_layer(32),\n            activation_layer(),\n            dropout_layer(),\n            \n            # Layer 3: 48x48 -> 105x105 (with precise output size)\n            nn.ConvTranspose2d(32, 1, kernel_size=14, stride=2, padding=2, output_padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Pass through encoder\n        if x.size(1) == 3:\n            # Use standard RGB to grayscale conversion: 0.299*R + 0.587*G + 0.114*B\n            x = 0.299 * x[:, 0:1] + 0.587 * x[:, 1:2] + 0.114 * x[:, 2:3]\n        for layer in self.encoder:\n            x = layer(x)\n            # print(x.shape)\n\n        for layer in self.decoder:\n            x = layer(x)\n            # print(x.shape)\n            \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:02.349768Z","iopub.execute_input":"2025-05-18T07:59:02.350658Z","iopub.status.idle":"2025-05-18T07:59:02.360365Z","shell.execute_reply.started":"2025-05-18T07:59:02.350632Z","shell.execute_reply":"2025-05-18T07:59:02.35973Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nscae = SCAE().to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(scae(sample[0].to(device)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SCAE (Tu's version)\nimport torch\nimport torch.nn as nn\nclass SCAE(nn.Module):\n    def __init__(self, normalization_type=\"batch_norm\", use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n        super(SCAE, self).__init__()\n        def norm_layer(num_features):\n            if normalization_type == \"batch_norm\": return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\": return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\": return nn.LayerNorm([num_features, 48, 48])\n            else: return nn.Identity()\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n\n        # Encoder: conv1 → pool1 → conv2\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=11, stride=2, padding=0)  # 105x105 → 48x48\n        self.norm1 = norm_layer(64)\n        self.act1 = activation_layer()\n        self.drop1 = dropout_layer()\n        self.pool1 = nn.MaxPool2d(2, 2, return_indices=True)                # 48x48 → 24x24\n\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) # 24x24 → 24x24\n        self.norm2 = norm_layer(128)\n        self.act2 = activation_layer()\n        self.drop2 = dropout_layer()\n\n        # Decoder: deconv1 → unpool1 → deconv2\n        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1) # 24x24 → 24x24\n        self.norm3 = norm_layer(64)\n        self.act3 = activation_layer()\n        self.drop3 = dropout_layer()\n        self.unpool1 = nn.MaxUnpool2d(2, 2)                                           # 24x24 → 48x48\n\n        self.deconv2 = nn.ConvTranspose2d(64, 1, kernel_size=11, stride=2, padding=0) # 48x48 → 105x105\n        # No normalization/activation after last layer for output\n        self.final_act = nn.Sigmoid()\n\n    def forward(self, x):\n        # Encoder\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.act1(x)\n        x = self.drop1(x)\n        x, indices = self.pool1(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.act2(x)\n        x = self.drop2(x)\n        # Decoder\n        x = self.deconv1(x)\n        x = self.norm3(x)\n        x = self.act3(x)\n        x = self.drop3(x)\n        x = self.unpool1(x, indices, output_size=torch.Size([x.size(0), x.size(1), 48, 48]))\n        x = self.deconv2(x)\n        x = self.final_act(x)\n        return x\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SCAE().to(device)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del model, optimizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create model and train with memory optimization\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SCAE().to(device)\ntrained_model = train_memory_efficient_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=5,\n    learning_rate=0.0001\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/checkpoint\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom functools import partial\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nimport os\nimport gc\n\n\ndef clean_gpu_memory():\n    \"\"\"Clean up GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef evaluate_reconstruction(model, dataloader, device, num_samples=10, save_path=None):\n    \"\"\"\n    Evaluate reconstruction quality with metrics (SSIM, PSNR, MSE) \n    and visualize original vs reconstructed images\n    \"\"\"\n    model.eval()\n    \n    # Initialize metrics\n    total_mse = 0\n    total_ssim = 0\n    total_psnr = 0\n    count = 0\n    \n    # Get a batch of images for visualization\n    vis_images = []\n    vis_recons = []\n    \n    with torch.no_grad():\n        # Get one batch for metrics and visualization\n        for images, _ in dataloader:\n            if images.numel() == 0:\n                continue\n                \n            images = images.to(device)\n            reconstructions = model(images)\n            \n            # Calculate MSE\n            mse = torch.mean((reconstructions - images) ** 2).item()\n            total_mse += mse\n            \n            # Convert tensors to numpy for SSIM and PSNR calculation\n            images_np = images.cpu().numpy()\n            recons_np = reconstructions.cpu().numpy()\n            \n            # Calculate metrics for each image in the batch\n            batch_size = images_np.shape[0]\n            for i in range(min(batch_size, num_samples - count)):\n                # Get single image (remove batch and channel dimensions)\n                img = np.squeeze(images_np[i])\n                recon = np.squeeze(recons_np[i])\n                \n                # Calculate SSIM (structural similarity index)\n                ssim_val = ssim(img, recon, data_range=1.0)\n                total_ssim += ssim_val\n                \n                # Calculate PSNR (peak signal-to-noise ratio)\n                psnr_val = psnr(img, recon, data_range=1.0)\n                total_psnr += psnr_val\n                \n                # Save images for visualization\n                if count < num_samples:\n                    vis_images.append(images[i])\n                    vis_recons.append(reconstructions[i])\n                \n                count += 1\n                \n                if count >= num_samples:\n                    break\n            \n            if count >= num_samples:\n                break\n    \n    # Calculate averages\n    avg_mse = total_mse / (count // batch_size + 1)\n    avg_ssim = total_ssim / count\n    avg_psnr = total_psnr / count\n    \n    # Print metrics\n    print(f\"Reconstruction Metrics:\")\n    print(f\"  Average MSE: {avg_mse:.4f}\")\n    print(f\"  Average SSIM: {avg_ssim:.4f} (higher is better, max 1.0)\")\n    print(f\"  Average PSNR: {avg_psnr:.2f} dB (higher is better)\")\n    \n    # Visualize original vs reconstructed images\n    if len(vis_images) > 0:\n        # Combine original and reconstructed images for side-by-side comparison\n        vis_combined = []\n        for img, recon in zip(vis_images, vis_recons):\n            vis_combined.extend([img, recon])\n        \n        # Create a grid of images\n        grid = make_grid(vis_combined, nrow=2, normalize=True, pad_value=1)\n        plt.figure(figsize=(12, num_samples*2))\n        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n        plt.axis('off')\n        plt.title('Original (left) vs Reconstructed (right)')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'reconstruction_comparison.png'), bbox_inches='tight')\n            print(f\"Saved reconstruction comparison to {save_path}\")\n        plt.show()\n    \n    return {\n        'mse': avg_mse,\n        'ssim': avg_ssim,\n        'psnr': avg_psnr\n    }\n\ndef evaluate_classification(model, dataloader, device, num_classes, save_path=None):\n    \"\"\"\n    Evaluate classification performance if your SCAE includes classification capability\n    \"\"\"\n    model.eval()\n    \n    # Check if model has a classify method or classification head\n    if not hasattr(model, 'classify') and not hasattr(model, 'classification_head'):\n        print(\"Model doesn't appear to have classification capability\")\n        return None\n        \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in dataloader:\n            if images.numel() == 0:\n                continue\n                \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # Get predictions - adapt this to your model's API\n            try:\n                if hasattr(model, 'classify'):\n                    preds = model.classify(images)\n                else:\n                    # Assume model returns (reconstructions, classifications) if called with return_classifications=True\n                    _, preds = model(images, return_classifications=True)\n                \n                # Convert to class indices\n                _, predicted = torch.max(preds.data, 1)\n                \n                # Store predictions and labels\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                \n            except Exception as e:\n                print(f\"Error during classification evaluation: {e}\")\n                return None\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Classification Accuracy: {accuracy:.4f}\")\n    \n    # Create confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    # Visualize confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    \n    if save_path:\n        plt.savefig(os.path.join(save_path, 'confusion_matrix.png'), bbox_inches='tight')\n    plt.show()\n    \n    return {\n        'accuracy': accuracy,\n        'confusion_matrix': cm\n    }\n\ndef visualize_latent_space(model, dataloader, device, save_path=None):\n    \"\"\"\n    Visualize the latent space of the autoencoder using t-SNE\n    \"\"\"\n    try:\n        from sklearn.manifold import TSNE\n        \n        # Get encoder output for a batch of images\n        model.eval()\n        latent_vectors = []\n        labels = []\n        \n        with torch.no_grad():\n            for images, batch_labels in dataloader:\n                if images.numel() == 0:\n                    continue\n                    \n                images = images.to(device)\n                \n                # Get latent vectors - adapt this to your model's API\n                if hasattr(model, 'encode'):\n                    latent = model.encode(images)\n                else:\n                    # Try to extract the latent representation from your model\n                    # This depends on your model's architecture\n                    x = images\n                    for layer in model.encoder:\n                        x = layer(x)\n                    latent = x\n                \n                # Flatten the latent vectors\n                batch_size = images.size(0)\n                latent_flat = latent.view(batch_size, -1).cpu().numpy()\n                \n                latent_vectors.append(latent_flat)\n                labels.append(batch_labels.numpy())\n                \n                if len(latent_vectors) * batch_size >= 1000:  # Limit number of points for t-SNE\n                    break\n        \n        # Concatenate batches\n        latent_vectors = np.vstack(latent_vectors)\n        labels = np.concatenate(labels)\n        \n        # Apply t-SNE\n        tsne = TSNE(n_components=2, random_state=42)\n        latent_tsne = tsne.fit_transform(latent_vectors)\n        \n        # Plot t-SNE results\n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\n        plt.colorbar(scatter, label='Font Class')\n        plt.title('t-SNE Visualization of Latent Space')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_tsne.png'), bbox_inches='tight')\n        plt.show()\n        \n        return latent_tsne, labels\n        \n    except Exception as e:\n        print(f\"Error in latent space visualization: {e}\")\n        return None, None\n\ndef generate_samples_from_latent(model, num_samples=10, latent_dim=128, device=None, save_path=None):\n    \"\"\"\n    Generate new images by sampling from the latent space (like in VAE)\n    \"\"\"\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model.eval()\n    \n    try:\n        # Generate random latent vectors\n        z = torch.randn(num_samples, latent_dim).to(device)\n        \n        # Generate images\n        with torch.no_grad():\n            if hasattr(model, 'decode'):\n                generated = model.decode(z)\n            else:\n                print(\"Model doesn't have a decode method. Cannot generate samples.\")\n                return None\n        \n        # Visualize generated images\n        grid = make_grid(generated.cpu(), nrow=5, normalize=True, pad_value=1)\n        plt.figure(figsize=(12, 6))\n        plt.imshow(grid.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.title('Generated Samples from Random Latent Vectors')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'generated_samples.png'), bbox_inches='tight')\n        plt.show()\n        \n        return generated\n        \n    except Exception as e:\n        print(f\"Error generating samples from latent space: {e}\")\n        print(f\"Your model might not support the VAE-style generation.\")\n        return None\n\ndef interpolate_latent_space(model, dataloader, device, steps=10, save_path=None):\n    \"\"\"\n    Interpolate between two points in latent space and decode them\n    \"\"\"\n    model.eval()\n    \n    try:\n        # Get two images from the dataset\n        for images, _ in dataloader:\n            if images.shape[0] >= 2 and images.numel() > 0:\n                break\n        else:\n            print(\"Couldn't find suitable images for interpolation\")\n            return None\n            \n        # Select two images\n        img1 = images[0:1].to(device)\n        img2 = images[1:2].to(device)\n        \n        # Get latent representations\n        with torch.no_grad():\n            if hasattr(model, 'encode'):\n                z1 = model.encode(img1)\n                z2 = model.encode(img2)\n            else:\n                # Try to extract the latent representation\n                x1 = img1\n                x2 = img2\n                for layer in model.encoder:\n                    x1 = layer(x1)\n                    x2 = layer(x2)\n                z1 = x1\n                z2 = x2\n        \n        # Interpolate between the two latent vectors\n        interpolations = []\n        with torch.no_grad():\n            for alpha in np.linspace(0, 1, steps):\n                z_interp = alpha * z1 + (1 - alpha) * z2\n                \n                # Decode the interpolated latent vector\n                if hasattr(model, 'decode'):\n                    decoded = model.decode(z_interp)\n                else:\n                    decoded = model.decoder(z_interp)\n                \n                interpolations.append(decoded)\n        \n        # Combine original images and interpolation\n        all_images = [img1.cpu()]\n        all_images.extend([interp.cpu() for interp in interpolations])\n        all_images.append(img2.cpu())\n        \n        # Create a grid\n        grid = make_grid(torch.cat(all_images), nrow=steps+2, normalize=True, pad_value=1)\n        plt.figure(figsize=(15, 4))\n        plt.imshow(grid.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.title('Latent Space Interpolation')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_interpolation.png'), bbox_inches='tight')\n        plt.show()\n        \n        return interpolations\n        \n    except Exception as e:\n        print(f\"Error during latent space interpolation: {e}\")\n        return None\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create output directory for visualizations\n    save_dir = \"/kaggle/working/evaluation_results\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define paths\n    model_path = \"/kaggle/working/best_model.pt\"\n    jpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\n    bcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\n    label_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\n    \n    try:\n        # Load your trained model\n        model = SCAE()\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        model.to(device)\n        model.eval()\n        print(\"Model loaded successfully\")\n        \n        # Load dataset\n        test_dataset = CombinedImageDataset(\n            jpeg_dir=jpeg_dir, \n            bcf_file=bcf_file, \n            label_file=label_file,\n            num_patch=1,  # Use fewer patches to save memory\n            patch_size=(105, 105)\n        )\n        \n        # Create test loader with smaller batch size\n        collate_fn = partial(patch_collate_fn, patch_size_tuple=(105, 105))\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=32,  # Use a smaller batch size\n            shuffle=True,   # Shuffle to get diverse examples\n            num_workers=2,\n            collate_fn=collate_fn\n        )\n        \n        print(\"Dataset and DataLoader prepared\")\n        \n        # Evaluate reconstruction quality\n        print(\"\\n1. Evaluating reconstruction quality...\")\n        recon_metrics = evaluate_reconstruction(\n            model, test_loader, device, num_samples=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # If model has classification capability, evaluate it\n        print(\"\\n2. Evaluating classification performance...\")\n        try:\n            class_metrics = evaluate_classification(\n                model, test_loader, device, num_classes=2383, save_path=save_dir\n            )\n        except:\n            print(\"Classification evaluation skipped - model may not support classification\")\n        clean_gpu_memory()\n        \n        # Visualize latent space\n        print(\"\\n3. Visualizing latent space with t-SNE...\")\n        latent_tsne, labels = visualize_latent_space(\n            model, test_loader, device, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Generate samples from random latent vectors (if model supports it)\n        print(\"\\n4. Generating samples from latent space...\")\n        try:\n            # Determine latent dimension from model structure\n            # This is just a guess - adapt to your model\n            latent_dim = 128\n            generated_samples = generate_samples_from_latent(\n                model, num_samples=16, latent_dim=latent_dim, device=device, save_path=save_dir\n            )\n        except:\n            print(\"Sample generation skipped - model may not support VAE-style generation\")\n        clean_gpu_memory()\n        \n        # Interpolate in latent space\n        print(\"\\n5. Creating latent space interpolation...\")\n        try:\n            interpolations = interpolate_latent_space(\n                model, test_loader, device, steps=8, save_path=save_dir\n            )\n        except:\n            print(\"Latent interpolation skipped - model may not support this operation\")\n        clean_gpu_memory()\n        \n        print(f\"\\nEvaluation complete! Results saved to {save_dir}\")\n        \n    except Exception as e:\n        print(f\"Error during evaluation: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluation code\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nimport os\nimport gc\nfrom functools import partial\nfrom torch.utils.data import DataLoader\n\ndef clean_gpu_memory():\n    \"\"\"Clean up GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef evaluate_reconstruction(model, dataloader, device, num_samples=10, save_path=None):\n    \"\"\"\n    Evaluate reconstruction quality with metrics (SSIM, PSNR, MSE)\n    and visualize original vs reconstructed images\n    \"\"\"\n    model.eval()\n    \n    # Initialize metrics\n    total_mse = 0\n    total_ssim = 0\n    total_psnr = 0\n    count = 0\n    \n    # Get a batch of images for visualization\n    vis_images = []\n    vis_recons = []\n    \n    with torch.no_grad():\n        # Get one batch for metrics and visualization\n        for images, _ in dataloader:\n            if images.numel() == 0:\n                continue\n                \n            images = images.to(device)\n            reconstructions = model(images)\n            \n            # Calculate MSE\n            mse = torch.mean((reconstructions - images) ** 2).item()\n            total_mse += mse\n            \n            # Convert tensors to numpy for SSIM and PSNR calculation\n            images_np = images.cpu().numpy()\n            recons_np = reconstructions.cpu().numpy()\n            \n            # Calculate metrics for each image in the batch\n            batch_size = images_np.shape[0]\n            for i in range(min(batch_size, num_samples - count)):\n                # Get single image (remove batch and channel dimensions)\n                img = np.squeeze(images_np[i])\n                recon = np.squeeze(recons_np[i])\n                \n                # Calculate SSIM (structural similarity index)\n                ssim_val = ssim(img, recon, data_range=1.0)\n                total_ssim += ssim_val\n                \n                # Calculate PSNR (peak signal-to-noise ratio)\n                psnr_val = psnr(img, recon, data_range=1.0)\n                total_psnr += psnr_val\n                \n                # Save images for visualization\n                if count < num_samples:\n                    vis_images.append(images[i])\n                    vis_recons.append(reconstructions[i])\n                \n                count += 1\n                \n                if count >= num_samples:\n                    break\n            \n            if count >= num_samples:\n                break\n    \n    # Calculate averages\n    avg_mse = total_mse / (count // batch_size + 1)\n    avg_ssim = total_ssim / count\n    avg_psnr = total_psnr / count\n    \n    # Print metrics\n    print(f\"Reconstruction Metrics:\")\n    print(f\"  Average MSE: {avg_mse:.4f}\")\n    print(f\"  Average SSIM: {avg_ssim:.4f} (higher is better, max 1.0)\")\n    print(f\"  Average PSNR: {avg_psnr:.2f} dB (higher is better)\")\n    \n    # Visualize original vs reconstructed images\n    if len(vis_images) > 0:\n        # Combine original and reconstructed images for side-by-side comparison\n        vis_combined = []\n        for img, recon in zip(vis_images, vis_recons):\n            vis_combined.extend([img, recon])\n        \n        # Create a grid of images\n        grid = make_grid(vis_combined, nrow=2, normalize=True, pad_value=1)\n        plt.figure(figsize=(12, num_samples*2))\n        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n        plt.axis('off')\n        plt.title('Original (left) vs Reconstructed (right)')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'reconstruction_comparison.png'), bbox_inches='tight')\n            print(f\"Saved reconstruction comparison to {save_path}\")\n        plt.show()\n    \n    return {\n        'mse': avg_mse,\n        'ssim': avg_ssim,\n        'psnr': avg_psnr\n    }\n\ndef extract_latent_features(model, x):\n    \"\"\"\n    Extract latent features from the model by running through the encoder part only\n    Adapted specifically for the new SCAE architecture\n    \"\"\"\n    # Apply encoder operations manually based on your model's structure\n    x = model.conv1(x)\n    x = model.norm1(x)\n    x = model.act1(x)\n    x = model.drop1(x)\n    x, indices = model.pool1(x)\n    x = model.conv2(x)\n    x = model.norm2(x)\n    x = model.act2(x)\n    x = model.drop2(x)\n    return x, indices\n\ndef generate_from_latent(model, latent, indices, output_size):\n    \"\"\"\n    Generate images from latent features by running through the decoder part only\n    Adapted specifically for the new SCAE architecture\n    \"\"\"\n    # Apply decoder operations manually based on your model's structure\n    x = model.deconv1(latent)\n    x = model.norm3(x)\n    x = model.act3(x)\n    x = model.drop3(x)\n    x = model.unpool1(x, indices, output_size=output_size)\n    x = model.deconv2(x)\n    x = model.final_act(x)\n    return x\n\ndef visualize_latent_space(model, dataloader, device, save_path=None):\n    \"\"\"\n    Visualize the latent space of the autoencoder using t-SNE\n    \"\"\"\n    try:\n        from sklearn.manifold import TSNE\n        \n        # Get encoder output for a batch of images\n        model.eval()\n        latent_vectors = []\n        orig_shapes = []\n        indices_list = []\n        labels = []\n        \n        with torch.no_grad():\n            for images, batch_labels in dataloader:\n                if images.numel() == 0:\n                    continue\n                    \n                images = images.to(device)\n                \n                # Get latent vectors using our custom function\n                latent, indices = extract_latent_features(model, images)\n                \n                # Flatten the latent vectors for t-SNE\n                batch_size = images.size(0)\n                latent_flat = latent.view(batch_size, -1).cpu().numpy()\n                \n                latent_vectors.append(latent_flat)\n                labels.append(batch_labels.numpy())\n                \n                # Store original shapes and indices for potential reconstruction\n                for i in range(batch_size):\n                    orig_shapes.append(torch.Size([1, latent.size(1), latent.size(2), latent.size(3)]))\n                    indices_list.append(indices[i:i+1])\n                \n                if len(latent_vectors) * batch_size >= 1000:  # Limit number of points for t-SNE\n                    break\n        \n        # Concatenate batches\n        latent_vectors = np.vstack(latent_vectors)\n        labels = np.concatenate(labels)\n        \n        # Apply t-SNE\n        tsne = TSNE(n_components=2, random_state=42)\n        latent_tsne = tsne.fit_transform(latent_vectors)\n        \n        # Plot t-SNE results\n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\n        plt.colorbar(scatter, label='Font Class')\n        plt.title('t-SNE Visualization of Latent Space')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_tsne.png'), bbox_inches='tight')\n        plt.show()\n        \n        return latent_tsne, labels, latent_vectors, orig_shapes, indices_list\n        \n    except Exception as e:\n        print(f\"Error in latent space visualization: {e}\")\n        return None, None, None, None, None\n\ndef interpolate_latent_space(model, dataloader, device, steps=10, save_path=None):\n    \"\"\"\n    Interpolate between two points in latent space and decode them\n    Adapted for the new SCAE architecture\n    \"\"\"\n    model.eval()\n    \n    try:\n        # Get two images from the dataset\n        for images, _ in dataloader:\n            if images.shape[0] >= 2 and images.numel() > 0:\n                break\n        else:\n            print(\"Couldn't find suitable images for interpolation\")\n            return None\n            \n        # Select two images\n        img1 = images[0:1].to(device)\n        img2 = images[1:2].to(device)\n        \n        # Get latent representations and indices for unpooling\n        with torch.no_grad():\n            z1, indices1 = extract_latent_features(model, img1)\n            z2, indices2 = extract_latent_features(model, img2)\n        \n        # Get output size for the unpool operation\n        output_size = torch.Size([1, z1.size(1), 48, 48])  # Based on your architecture\n        \n        # Interpolate between the two latent vectors\n        interpolations = []\n        with torch.no_grad():\n            for alpha in np.linspace(0, 1, steps):\n                z_interp = alpha * z1 + (1 - alpha) * z2\n                \n                # Use indices from first image for interpolation (simplification)\n                decoded = generate_from_latent(model, z_interp, indices1, output_size)\n                \n                interpolations.append(decoded)\n        \n        # Combine original images and interpolation\n        all_images = []\n        with torch.no_grad():\n            # Add the first original image\n            all_images.append(img1.cpu())\n            # Add interpolations\n            all_images.extend([interp.cpu() for interp in interpolations])\n            # Add the second original image\n            all_images.append(img2.cpu())\n        \n        # Create a grid\n        grid = make_grid(torch.cat(all_images), nrow=steps+2, normalize=True, pad_value=1)\n        plt.figure(figsize=(15, 4))\n        plt.imshow(grid.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.title('Latent Space Interpolation')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_interpolation.png'), bbox_inches='tight')\n        plt.show()\n        \n        return interpolations\n        \n    except Exception as e:\n        print(f\"Error during latent space interpolation: {e}\")\n        return None\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create output directory for visualizations\n    save_dir = \"evaluation_results\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define paths\n    model_path = \"/kaggle/working/best_model.pt\"\n    jpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\n    bcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\n    label_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\n    \n    try:\n        # Import necessary modules - add these imports here to avoid issues in Kaggle\n        # Load your trained model\n        model = SCAE()\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        model.to(device)\n        model.eval()\n        print(\"Model loaded successfully\")\n        \n        # Load dataset\n        combined_dataset = CombinedImageDataset(\n            jpeg_dir=jpeg_dir,\n            bcf_file=bcf_file,\n            label_file=label_file,\n            num_patch=3,  # Reduced from 3\n        )\n    \n        # Create memory-optimized dataloaders with smaller batch size\n        test_loader, val_loader = create_patch_dataloaders(\n            combined_dataset,\n            batch_size=512,  # Reduced from 1024\n            num_workers=2,   # Reduced from 4\n            val_split=0\n        )\n        # test_dataset = CombinedImageDataset(\n        #     jpeg_dir=jpeg_dir, \n        #     bcf_file=bcf_file, \n        #     label_file=label_file,\n        #     num_patch=3,  # Use fewer patches to save memory\n        #     patch_size=(105, 105)\n        # )\n        \n        # # Create test loader with smaller batch size\n        # collate_fn = partial(patch_collate_fn, patch_size_tuple=(105, 105))\n        # test_loader = DataLoader(\n        #     test_dataset,\n        #     batch_size=128,  # Use a smaller batch size\n        #     shuffle=True,   # Shuffle to get diverse examples\n        #     num_workers=2,\n        #     collate_fn=collate_fn\n        # )\n        \n        print(\"Dataset and DataLoader prepared\")\n        \n        # Evaluate reconstruction quality\n        print(\"\\n1. Evaluating reconstruction quality...\")\n        recon_metrics = evaluate_reconstruction(\n            model, test_loader, device, num_samples=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Visualize latent space\n        print(\"\\n2. Visualizing latent space with t-SNE...\")\n        latent_tsne, labels, latent_vectors, orig_shapes, indices_list = visualize_latent_space(\n            model, test_loader, device, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Interpolate in latent space\n        print(\"\\n3. Creating latent space interpolation...\")\n        interpolations = interpolate_latent_space(\n            model, test_loader, device, steps=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        print(f\"\\nEvaluation complete! Results saved to {save_dir}\")\n        \n    except Exception as e:\n        print(f\"Error during evaluation: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nimport os\nimport gc\nfrom functools import partial\nfrom torch.utils.data import DataLoader\n\ndef clean_gpu_memory():\n    \"\"\"Clean up GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef evaluate_reconstruction(model, dataloader, device, num_samples=10, save_path=None):\n    \"\"\"\n    Evaluate reconstruction quality with metrics (SSIM, PSNR, MSE)\n    and visualize original vs reconstructed images\n    \"\"\"\n    model.eval()\n    \n    # Initialize metrics\n    total_mse = 0\n    total_ssim = 0\n    total_psnr = 0\n    count = 0\n    \n    # Get a batch of images for visualization\n    vis_images = []\n    vis_recons = []\n    \n    with torch.no_grad():\n        # Get one batch for metrics and visualization\n        for images, _ in dataloader:\n            # Skip empty batches\n            if images.numel() == 0:\n                continue\n                \n            # Ensure we have the right shape for processing\n            if len(images.shape) == 3:\n                images = images.unsqueeze(1)  # Add channel dimension if missing\n                \n            images = images.to(device)\n            reconstructions = model(images)\n            \n            # Calculate MSE\n            mse = torch.mean((reconstructions - images) ** 2).item()\n            total_mse += mse\n            \n            # Convert tensors to numpy for SSIM and PSNR calculation\n            images_np = images.cpu().numpy()\n            recons_np = reconstructions.cpu().numpy()\n            \n            # Calculate metrics for each image in the batch\n            batch_size = images_np.shape[0]\n            for i in range(min(batch_size, num_samples - count)):\n                # Get single image (remove batch and channel dimensions if needed)\n                img = np.squeeze(images_np[i])\n                recon = np.squeeze(recons_np[i])\n                \n                # If image has 3 channels, convert to grayscale for SSIM/PSNR calculation\n                if len(img.shape) == 3 and img.shape[0] == 3:\n                    img_gray = np.mean(img, axis=0)\n                    recon_gray = np.mean(recon, axis=0)\n                    ssim_val = ssim(img_gray, recon_gray, data_range=1.0)\n                    psnr_val = psnr(img_gray, recon_gray, data_range=1.0)\n                else:\n                    # Ensure images are properly squeezed but retain needed dimensions\n                    if len(img.shape) > 2:\n                        img = np.squeeze(img)\n                    if len(recon.shape) > 2:\n                        recon = np.squeeze(recon)\n                    \n                    # Handle case where squeeze removed too many dimensions\n                    if len(img.shape) == 0:\n                        img = img.reshape(1, 1)\n                    if len(recon.shape) == 0:\n                        recon = recon.reshape(1, 1)\n                        \n                    ssim_val = ssim(img, recon, data_range=1.0)\n                    psnr_val = psnr(img, recon, data_range=1.0)\n                \n                total_ssim += ssim_val\n                total_psnr += psnr_val\n                \n                # Save images for visualization\n                if count < num_samples:\n                    vis_images.append(images[i])\n                    vis_recons.append(reconstructions[i])\n                \n                count += 1\n                \n                if count >= num_samples:\n                    break\n            \n            if count >= num_samples:\n                break\n    \n    # Calculate averages\n    if count > 0:\n        avg_mse = total_mse / (count // batch_size + 1)\n        avg_ssim = total_ssim / count\n        avg_psnr = total_psnr / count\n    else:\n        avg_mse = float('nan')\n        avg_ssim = float('nan')\n        avg_psnr = float('nan')\n        print(\"Warning: No valid samples were processed in evaluation\")\n    \n    # Print metrics\n    print(f\"Reconstruction Metrics:\")\n    print(f\"  Average MSE: {avg_mse:.4f}\")\n    print(f\"  Average SSIM: {avg_ssim:.4f} (higher is better, max 1.0)\")\n    print(f\"  Average PSNR: {avg_psnr:.2f} dB (higher is better)\")\n    \n    # Visualize original vs reconstructed images\n    if len(vis_images) > 0:\n        # Combine original and reconstructed images for side-by-side comparison\n        vis_combined = []\n        for img, recon in zip(vis_images, vis_recons):\n            vis_combined.extend([img, recon])\n        \n        # Create a grid of images\n        grid = make_grid(vis_combined, nrow=2, normalize=True, pad_value=1)\n        plt.figure(figsize=(12, num_samples*2))\n        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n        plt.axis('off')\n        plt.title('Original (left) vs Reconstructed (right)')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'reconstruction_comparison.png'), bbox_inches='tight')\n            print(f\"Saved reconstruction comparison to {save_path}\")\n        plt.show()\n    \n    return {\n        'mse': avg_mse,\n        'ssim': avg_ssim,\n        'psnr': avg_psnr\n    }\n\ndef safely_extract_latent_features(model, x):\n    \"\"\"\n    Extract latent features from the model - with safety checks and error handling\n    Handles various model architectures by attempting different approaches\n    \"\"\"\n    try:\n        # Method 1: Direct call to encoder if available\n        if hasattr(model, 'encode'):\n            return model.encode(x)\n        \n        # Method 2: Manual application of encoder layers as in original code\n        # Capture as much as possible through a try/except approach\n        try:\n            x = model.conv1(x)\n            x = model.norm1(x) if hasattr(model, 'norm1') else x\n            x = model.act1(x) if hasattr(model, 'act1') else torch.relu(x)\n            x = model.drop1(x) if hasattr(model, 'drop1') else x\n            \n            # Handle pooling layers with or without indices\n            if hasattr(model, 'pool1'):\n                if 'MaxPool' in model.pool1.__class__.__name__ and 'return_indices=True' in str(model.pool1):\n                    x, indices = model.pool1(x)\n                else:\n                    x = model.pool1(x)\n                    indices = None\n            else:\n                indices = None\n                \n            # Continue with more layers if they exist\n            if hasattr(model, 'conv2'):\n                x = model.conv2(x)\n                x = model.norm2(x) if hasattr(model, 'norm2') else x\n                x = model.act2(x) if hasattr(model, 'act2') else torch.relu(x)\n                x = model.drop2(x) if hasattr(model, 'drop2') else x\n            \n            return x, indices\n            \n        except Exception as e:\n            print(f\"Warning: Error in manual feature extraction: {e}\")\n            \n            # Method 3: Simplified approach - extract features up to a bottleneck\n            # This is a more generic approach that can work with different model architectures\n            features = []\n            indices_list = []\n            \n            # Get all model's modules\n            for name, module in model.named_modules():\n                # Skip the top level module (the model itself)\n                if name == '':\n                    continue\n                    \n                # Apply the layer\n                try:\n                    if isinstance(module, nn.MaxPool2d) and module.return_indices:\n                        x, indices = module(x)\n                        indices_list.append(indices)\n                    else:\n                        x = module(x)\n                        \n                    # Collect features at bottleneck or certain layers\n                    if isinstance(module, (nn.Conv2d, nn.Linear)) and len(features) < 5:\n                        features.append(x)\n                        \n                    # If we've reached the bottleneck/middle of the network, stop\n                    if 'bottleneck' in name.lower() or 'latent' in name.lower():\n                        break\n                        \n                except Exception:\n                    continue\n            \n            # If we've collected features, return the last one\n            if features:\n                return features[-1], indices_list[-1] if indices_list else None\n    \n    except Exception as e:\n        print(f\"Error in feature extraction: {e}\")\n        # Return input as fallback (will produce poor results but prevents crashes)\n        return x, None\n\ndef safely_generate_from_latent(model, latent, indices=None, output_size=None):\n    \"\"\"\n    Generate images from latent features with safety checks and error handling\n    Handles various model architectures by attempting different approaches\n    \"\"\"\n    try:\n        # Method 1: Direct call to decoder if available\n        if hasattr(model, 'decode'):\n            return model.decode(latent)\n        \n        # Method 2: Manual application of decoder layers as in original code\n        try:\n            x = latent\n            \n            # Apply decoder operations based on model attributes\n            if hasattr(model, 'deconv1'):\n                x = model.deconv1(x)\n                x = model.norm3(x) if hasattr(model, 'norm3') else x\n                x = model.act3(x) if hasattr(model, 'act3') else torch.relu(x)\n                x = model.drop3(x) if hasattr(model, 'drop3') else x\n            \n            # Handle unpooling layers if they exist\n            if hasattr(model, 'unpool1') and indices is not None:\n                x = model.unpool1(x, indices, output_size=output_size)\n            \n            # Continue with more layers if they exist\n            if hasattr(model, 'deconv2'):\n                x = model.deconv2(x)\n            \n            # Apply final activation if it exists\n            if hasattr(model, 'final_act'):\n                x = model.final_act(x)\n            elif hasattr(model, 'sigmoid'):\n                x = model.sigmoid(x)\n            else:\n                x = torch.sigmoid(x)  # Default to sigmoid for image generation\n                \n            return x\n            \n        except Exception as e:\n            print(f\"Warning: Error in manual latent generation: {e}\")\n            \n            # Method 3: Forward pass through the full model as fallback\n            # This is a reasonable fallback that should work with most autoencoders\n            return model(latent)\n    \n    except Exception as e:\n        print(f\"Error in generating from latent: {e}\")\n        # Return latent as fallback (will produce poor results but prevents crashes)\n        return latent\n\ndef visualize_latent_space(model, dataloader, device, save_path=None):\n    \"\"\"\n    Visualize the latent space of the autoencoder using t-SNE\n    \"\"\"\n    try:\n        from sklearn.manifold import TSNE\n        \n        # Get encoder output for a batch of images\n        model.eval()\n        latent_vectors = []\n        orig_shapes = []\n        indices_list = []\n        labels = []\n        \n        with torch.no_grad():\n            for images, batch_labels in dataloader:\n                # Skip empty batches\n                if images.numel() == 0:\n                    continue\n                    \n                # Ensure we have the right shape for processing\n                if len(images.shape) == 3:\n                    images = images.unsqueeze(1)  # Add channel dimension if missing\n                    \n                images = images.to(device)\n                \n                # Get latent vectors using our safe extraction function\n                latent, indices = safely_extract_latent_features(model, images)\n                \n                # Flatten the latent vectors for t-SNE\n                batch_size = images.size(0)\n                \n                # Handle case where latent is not a tensor\n                if not isinstance(latent, torch.Tensor):\n                    print(\"Warning: Latent features are not a tensor, skipping batch\")\n                    continue\n                \n                # Flatten the latent representations\n                if len(latent.shape) > 2:\n                    latent_flat = latent.reshape(batch_size, -1).cpu().numpy()\n                else:\n                    latent_flat = latent.cpu().numpy()\n                \n                latent_vectors.append(latent_flat)\n                labels.append(batch_labels.cpu().numpy())\n                \n                # Store original shapes and indices for potential reconstruction\n                for i in range(batch_size):\n                    if indices is not None:\n                        if isinstance(indices, list):\n                            indices_list.append([idx[i:i+1] for idx in indices])\n                        else:\n                            indices_list.append(indices[i:i+1])\n                    else:\n                        indices_list.append(None)\n                    \n                    if len(latent.shape) > 2:\n                        orig_shapes.append(torch.Size([1, latent.size(1), latent.size(2), latent.size(3)]))\n                    else:\n                        # Handle case where latent is already flattened\n                        orig_shapes.append(None)\n                \n                if len(latent_vectors) * batch_size >= 1000:  # Limit number of points for t-SNE\n                    break\n        \n        # Concatenate batches\n        if latent_vectors:\n            latent_vectors = np.vstack(latent_vectors)\n            labels = np.concatenate(labels)\n            \n            # Apply t-SNE\n            tsne = TSNE(n_components=2, random_state=42)\n            latent_tsne = tsne.fit_transform(latent_vectors)\n            \n            # Plot t-SNE results\n            plt.figure(figsize=(10, 8))\n            scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\n            plt.colorbar(scatter, label='Font Class')\n            plt.title('t-SNE Visualization of Latent Space')\n            \n            if save_path:\n                plt.savefig(os.path.join(save_path, 'latent_tsne.png'), bbox_inches='tight')\n            plt.show()\n            \n            return latent_tsne, labels, latent_vectors, orig_shapes, indices_list\n        else:\n            print(\"No valid latent vectors were extracted\")\n            return None, None, None, None, None\n        \n    except Exception as e:\n        print(f\"Error in latent space visualization: {e}\")\n        return None, None, None, None, None\n\ndef interpolate_latent_space(model, dataloader, device, steps=10, save_path=None):\n    \"\"\"\n    Interpolate between two points in latent space and decode them\n    With improved error handling and support for different model architectures\n    \"\"\"\n    model.eval()\n    \n    try:\n        # Get two images from the dataset\n        valid_images = []\n        \n        for images, _ in dataloader:\n            # Skip empty batches\n            if images.numel() == 0:\n                continue\n                \n            # Ensure we have the right shape for processing\n            if len(images.shape) == 3:\n                images = images.unsqueeze(1)  # Add channel dimension if missing\n                \n            valid_images.append(images)\n            \n            if len(valid_images) > 0 and valid_images[0].shape[0] >= 2:\n                break\n        \n        if not valid_images or valid_images[0].shape[0] < 2:\n            print(\"Couldn't find suitable images for interpolation\")\n            return None\n            \n        # Select two images\n        img1 = valid_images[0][0:1].to(device)\n        img2 = valid_images[0][1:2].to(device)\n        \n        # Get latent representations and indices for unpooling\n        with torch.no_grad():\n            z1, indices1 = safely_extract_latent_features(model, img1)\n            z2, indices2 = safely_extract_latent_features(model, img2)\n            \n            # Check if latent features were extracted correctly\n            if z1 is None or z2 is None:\n                print(\"Failed to extract latent features for interpolation\")\n                return None\n        \n        # Get output size for the unpool operation if needed\n        # This is a reasonable default that should work with many architectures\n        if len(z1.shape) == 4:\n            output_size = torch.Size([1, z1.size(1), z1.size(2)*2, z1.size(3)*2])\n        else:\n            output_size = None\n        \n        # Interpolate between the two latent vectors\n        interpolations = []\n        with torch.no_grad():\n            for alpha in np.linspace(0, 1, steps):\n                # Linear interpolation in latent space\n                z_interp = alpha * z1 + (1 - alpha) * z2\n                \n                # Use indices from first image for interpolation (simplification)\n                decoded = safely_generate_from_latent(model, z_interp, indices1, output_size)\n                \n                interpolations.append(decoded)\n        \n        # Combine original images and interpolation\n        all_images = []\n        with torch.no_grad():\n            # Add the first original image\n            reconstructed_img1 = model(img1)\n            all_images.append(reconstructed_img1.cpu())\n            \n            # Add interpolations\n            all_images.extend([interp.cpu() for interp in interpolations])\n            \n            # Add the second original image\n            reconstructed_img2 = model(img2)\n            all_images.append(reconstructed_img2.cpu())\n        \n        # Create a grid\n        grid = make_grid(torch.cat(all_images), nrow=steps+2, normalize=True, pad_value=1)\n        plt.figure(figsize=(15, 4))\n        plt.imshow(grid.permute(1, 2, 0).numpy())\n        plt.axis('off')\n        plt.title('Latent Space Interpolation')\n        \n        if save_path:\n            plt.savefig(os.path.join(save_path, 'latent_interpolation.png'), bbox_inches='tight')\n        plt.show()\n        \n        return interpolations\n        \n    except Exception as e:\n        print(f\"Error during latent space interpolation: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\ndef main():\n    \"\"\"\n    Main evaluation function that runs all the evaluation steps\n    \"\"\"\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create output directory for visualizations\n    save_dir = \"evaluation_results\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define paths - make these configurable in your actual implementation\n    model_path = \"/kaggle/working/best_model.pt\"\n    jpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\n    bcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\n    label_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\n    \n    try:        \n        # Load your trained model\n        model = SCAE()\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        model.to(device)\n        model.eval()\n        print(\"Model loaded successfully\")\n        \n        # Load dataset\n        combined_dataset = CombinedImageDataset(\n            jpeg_dir=jpeg_dir,\n            bcf_file=bcf_file,\n            label_file=label_file,\n            num_patch=3,\n        )\n    \n        # Create dataloaders using our fixed implementation\n        test_loader, val_loader = create_patch_dataloaders(\n            combined_dataset,\n            batch_size=64,  # Reduced batch size for better stability\n            num_workers=2,\n            val_split=0.1,\n            patch_size=(105, 105)\n        )\n        \n        print(\"Dataset and DataLoader prepared\")\n        \n        # Evaluate reconstruction quality\n        print(\"\\n1. Evaluating reconstruction quality...\")\n        recon_metrics = evaluate_reconstruction(\n            model, test_loader, device, num_samples=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Visualize latent space\n        print(\"\\n2. Visualizing latent space with t-SNE...\")\n        latent_tsne, labels, latent_vectors, orig_shapes, indices_list = visualize_latent_space(\n            model, test_loader, device, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        # Interpolate in latent space\n        print(\"\\n3. Creating latent space interpolation...\")\n        interpolations = interpolate_latent_space(\n            model, test_loader, device, steps=8, save_path=save_dir\n        )\n        clean_gpu_memory()\n        \n        print(f\"\\nEvaluation complete! Results saved to {save_dir}\")\n        \n    except Exception as e:\n        print(f\"Error during evaluation: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/evaluation_results.zip /kaggle/working/evaluation_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Augumentation Steps \n1) Noise\n2) Blur\n3) Perpective Rotation\n4) Shading\n5) Variable Character Spacing\n6) Variable Aspect Ratio","metadata":{"id":"1hbTCU2qndht"}},{"cell_type":"code","source":"# augmentation functions\nfrom PIL import ImageFilter, Image\nimport random\nimport numpy as np\nimport cv2\n\ndef noise_image(np_img, mean=0, std=2):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n    img_array = np_img.astype(np.float32)\n    noise = np.random.normal(mean, std, img_array.shape)\n    noisy_img = img_array + noise\n    noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n    return cv2.resize(noisy_img, (105, 105))\n\ndef blur_image(np_img):\n    if isinstance(np_img, np.ndarray):\n        np_img = Image.fromarray(np_img.astype('uint8'))\n    blur_img = np_img.filter(ImageFilter.GaussianBlur(radius=1.5))\n    blur_img = blur_img.resize((105, 105))\n    return np.array(blur_img)\n\ndef affine_rotation(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    if len(np_img.shape) == 2:\n        np_img = np.expand_dims(np_img, axis=-1)\n\n    rows, cols = np_img.shape[:2]\n    src_pts = np.float32([[0, 0], [cols - 1, 0], [0, rows - 1]])\n    max_shift = 0.05\n    dst_pts = src_pts + np.random.uniform(-max_shift * cols, max_shift * cols, src_pts.shape).astype(np.float32)\n\n    A = cv2.getAffineTransform(src_pts, dst_pts)\n    warped = cv2.warpAffine(np_img, A, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n\n    if warped.ndim == 3 and warped.shape[-1] == 1:\n        warped = warped[:, :, 0]\n\n    warped = cv2.resize(warped, (105, 105))\n    return warped\n\ndef gradient_fill(np_img):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if len(np_img.shape) == 3:\n        gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = np_img\n\n    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n    abs_lap = np.absolute(laplacian) * 0.5\n    lap_uint8 = np.uint8(np.clip(abs_lap, 0, 255))\n    lap_resized = cv2.resize(lap_uint8, (105, 105))\n    return lap_resized\n\ndef variable_aspect_ratio_preprocess(np_img, target_size=(105, 105)):\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    if len(np_img.shape) == 3:\n        h, w, c = np_img.shape\n    else:\n        h, w = np_img.shape\n        c = None\n\n    scale_ratio = np.random.uniform(0.95, 1.05)\n    new_width = int(w * scale_ratio)\n\n    resized = cv2.resize(np_img, (new_width, h), interpolation=cv2.INTER_LINEAR)\n    final = cv2.resize(resized, target_size, interpolation=cv2.INTER_LINEAR)\n    return final\n\ndef augmentation_pipeline(np_img):\n    \"\"\"\n    Tăng cường ảnh đầu vào với các phép biến đổi ngẫu nhiên.\n    Hỗ trợ ảnh grayscale hoặc RGB dưới dạng NumPy array hoặc PIL Image.\n    \"\"\"\n    if isinstance(np_img, Image.Image):\n        np_img = np.array(np_img)\n\n    if np_img.dtype != np.uint8:\n        np_img = np.clip(np_img, 0, 255).astype(np.uint8)\n\n    img = variable_aspect_ratio_preprocess(np_img)\n\n    augmentations = [\n        lambda x: noise_image(x),\n        lambda x: blur_image(x),\n        lambda x: affine_rotation(x),\n        lambda x: gradient_fill(x)\n    ]\n\n    num_aug = random.randint(1, 3)\n    selected_augs = random.sample(augmentations, num_aug)\n\n    for aug in selected_augs:\n        img = aug(img)\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        if img.dtype != np.uint8:\n            img = np.clip(img, 0, 255).astype(np.uint8)\n\n    return img\n\n","metadata":{"id":"MLCHbBKsndhv","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-18T07:58:29.664759Z","iopub.execute_input":"2025-05-18T07:58:29.665456Z","iopub.status.idle":"2025-05-18T07:58:29.734866Z","shell.execute_reply.started":"2025-05-18T07:58:29.665421Z","shell.execute_reply":"2025-05-18T07:58:29.734092Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# extract patches from an image\nimport easyocr\nimport numpy as np\nimport os\nimport tempfile\nfrom PIL import Image\n\n# Global OCR reader for efficiency\n_ocr_reader = None\n\ndef get_ocr_reader(languages=[\"en\"]):\n    global _ocr_reader\n    if _ocr_reader is None:\n        _ocr_reader = easyocr.Reader(languages)\n    return _ocr_reader\n\ndef extract_patches(image_array, num_patch=3, patch_size=(105, 105), \n                    extract_text=True, min_text_coverage=0.3, max_attempts=20):\n    # Handle dimension check\n    if image_array.ndim == 2:  # Grayscale\n        h, w = image_array.shape\n        is_grayscale = True\n    elif image_array.ndim == 3:  # Color\n        h, w, _ = image_array.shape\n        is_grayscale = False\n    else:\n        print(f\"Warning: Unexpected image array dimension: {image_array.ndim}\")\n        return []\n\n    patch_h, patch_w = patch_size\n\n    # Check if image is large enough for at least one patch\n    if h < patch_h or w < patch_w:\n        return []  # Return empty list if image is too small\n        \n    # If not extracting text or image is too small, use random patches\n    if not extract_text:\n        patches = []\n        for _ in range(num_patch):\n            x = np.random.randint(0, w - patch_w + 1)\n            y = np.random.randint(0, h - patch_h + 1)\n            if is_grayscale:\n                patch = image_array[y:y+patch_h, x:x+patch_w]\n            else:\n                patch = image_array[y:y+patch_h, x:x+patch_w, :]\n            patches.append(patch)\n        return patches\n        \n    # For text extraction, we'll try to find patches with text\n    reader = get_ocr_reader()\n    text_patches = []\n    attempts = 0\n    \n    # Keep extracting until we have enough or reach max attempts\n    while len(text_patches) < num_patch and attempts < max_attempts:\n        # Generate a random patch\n        x = np.random.randint(0, w - patch_w + 1)\n        y = np.random.randint(0, h - patch_h + 1)\n        \n        if is_grayscale:\n            patch = image_array[y:y+patch_h, x:x+patch_w]\n        else:\n            patch = image_array[y:y+patch_h, x:x+patch_w, :]\n            \n        # Save patch to temporary file for OCR\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:\n            tmp_path = tmp.name\n            patch_img = Image.fromarray(patch if is_grayscale else patch)\n            patch_img.save(tmp_path)\n            \n        try:\n            # Run OCR on the patch\n            ocr_results = reader.readtext(tmp_path)\n            \n            # Clean up temp file\n            os.unlink(tmp_path)\n            \n            # Calculate total text area\n            patch_area = patch_h * patch_w\n            text_area = 0\n            \n            for bbox, text, conf in ocr_results:\n                if conf < 0.5:  # Skip low confidence detections\n                    continue\n                    \n                # Convert bbox points to integers\n                bbox = [[int(point[0]), int(point[1])] for point in bbox]\n                \n                # Calculate bbox area (text area)\n                text_min_x = max(0, min(point[0] for point in bbox))\n                text_max_x = min(patch_w, max(point[0] for point in bbox))\n                text_min_y = max(0, min(point[1] for point in bbox))\n                text_max_y = min(patch_h, max(point[1] for point in bbox))\n                \n                box_width = text_max_x - text_min_x\n                box_height = text_max_y - text_min_y\n                \n                if box_width > 0 and box_height > 0:\n                    text_area += box_width * box_height\n            \n            # Check if patch has enough text\n            coverage = text_area / patch_area\n            if coverage >= min_text_coverage:\n                text_patches.append(patch)\n                # print(f\"Found text patch with coverage {coverage:.2f}\")\n        \n        except Exception as e:\n            print(f\"Error in OCR: {e}\")\n            try:\n                os.unlink(tmp_path)\n            except:\n                pass\n                \n        attempts += 1\n    \n    # Return whatever text patches we found, even if fewer than requested\n    return text_patches","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-18T07:58:33.343515Z","iopub.execute_input":"2025-05-18T07:58:33.343807Z","iopub.status.idle":"2025-05-18T07:58:36.793879Z","shell.execute_reply.started":"2025-05-18T07:58:33.343784Z","shell.execute_reply":"2025-05-18T07:58:36.793329Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# combined dataset (them moi dong if ... is None, unnecessary bro)\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom io import BytesIO\nfrom datasets import Dataset\nimport warnings\nfrom torchvision import transforms\n\nclass FontDataset(Dataset):\n    def __init__(self, jpeg_dir, bcf_file, label_file, num_patch=3, patch_size=(105, 105), \n                 extract_text=False, min_text_coverage=0.3, max_attempts=20, ocr_languages=[\"en\"]):\n        self.jpeg_dir = jpeg_dir\n        self.bcf_file = bcf_file\n        self.label_file = label_file\n        self.num_patch = num_patch\n        self.patch_size = patch_size\n        self.extract_text = extract_text\n        self.min_text_coverage = min_text_coverage\n        self.max_attempts = max_attempts\n        self.ocr_languages = ocr_languages\n\n        # Initialize OCR reader if needed\n        if extract_text:\n            self.reader = get_ocr_reader(ocr_languages)\n\n        self.jpeg_data = []\n        self.bcf_data = []\n\n        # Load jpeg data\n        self._load_jpeg_data(jpeg_dir)\n\n        # Load bcf data\n        self._load_bcf_data(bcf_file, label_file)\n\n    def _load_jpeg_data(self, jpeg_dir):\n        # add login for the case where jpeg_dir is None\n        if jpeg_dir is None:\n            print(\"Warning: jpeg_dir is None, skipping JPEG data loading.\")\n            return\n        if not os.path.exists(jpeg_dir):\n            print(f\"Warning: JPEG directory {jpeg_dir} does not exist.\")\n            return\n            \n        image_filenames = [f for f in os.listdir(jpeg_dir) if f.lower().endswith(('.jpeg', '.jpg'))]\n        self.jpeg_data = [(os.path.join(jpeg_dir, f), 0) for f in image_filenames]  # Assuming label is 0 for .jpeg files\n        print(f\"Loaded {len(self.jpeg_data)} .jpeg images.\")\n\n    def _load_bcf_data(self, bcf_file, label_file):\n        if not (os.path.exists(bcf_file) and os.path.exists(label_file)):\n            print(f\"Warning: BCF file {bcf_file} or label file {label_file} does not exist.\")\n            return\n            \n        try:\n            with open(label_file, 'rb') as f:\n                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n                print(f\"Loaded {len(self.labels)} labels from {label_file}.\")\n\n            with open(bcf_file, 'rb') as f:\n                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n                print(f\"Loaded {self.num_images} images from {bcf_file}.\")\n\n                sizes_bytes = f.read(self.num_images * 8)\n                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n\n                self.data_start_offset = 8 + self.num_images * 8\n                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n\n                for idx in range(self.num_images):\n                    self.bcf_data.append((idx, self.labels[idx]))\n                \n            print(f\"Loaded {len(self.bcf_data)} .bcf images.\")\n        except Exception as e:\n            print(f\"Error loading .bcf data: {e}\")\n\n    def __len__(self):\n        return len(self.jpeg_data) + len(self.bcf_data)\n\n    def _extract_patches(self, img_array):\n        return extract_patches(\n            img_array, \n            num_patch=self.num_patch, \n            patch_size=self.patch_size,\n            extract_text=self.extract_text, \n            min_text_coverage=self.min_text_coverage,\n            max_attempts=self.max_attempts\n        )\n\n    def __getitem__(self, idx):\n        if isinstance(idx, list):\n            results = []\n            labels = []\n            for single_idx in idx:\n                try:\n                    patches, label = self.__getitem__(single_idx)\n                    if patches and label != -1:\n                        results.append(patches)\n                        labels.append(label)\n                except Exception as e:\n                    print(f\"Error processing index {single_idx}: {e}\")\n            return results, labels\n\n        max_retries = 3\n        for retry in range(max_retries):\n            try:\n                if idx < len(self.jpeg_data):\n                    # Handle .jpeg images\n                    img_path, label = self.jpeg_data[idx]\n                    try:\n                        with warnings.catch_warnings():\n                            warnings.simplefilter(\"ignore\")\n                            img = Image.open(img_path)\n                            img.verify()\n\n                        img = Image.open(img_path).convert('L')\n                        img_array = np.array(img)\n\n                        # ✅ Apply augmentation before extracting patches\n                        patches = self._extract_patches(img_array)\n                        patches = [augmentation_pipeline(patch) for patch in patches]\n\n                        del img, img_array\n                        return patches, label\n\n                    except (OSError, IOError, ValueError) as e:\n                        print(f\"Warning: Corrupt image at {img_path}: {e}\")\n                        return [], -1\n\n                else:\n                    # Handle .bcf images\n                    bcf_idx = idx - len(self.jpeg_data)\n                    if bcf_idx >= len(self.bcf_data):\n                        return [], -1\n\n                    label = self.bcf_data[bcf_idx][1]\n                    offset = self.image_offsets[bcf_idx]\n                    size = self.image_sizes[bcf_idx]\n\n                    try:\n                        with open(self.bcf_file, 'rb') as f:\n                            f.seek(self.data_start_offset + offset)\n                            image_bytes = f.read(size)\n\n                        buffer = BytesIO(image_bytes)\n                        img = Image.open(buffer)\n                        img.verify()\n\n                        buffer.seek(0)\n                        img = Image.open(buffer).convert('L')\n                        img_array = np.array(img)\n\n                        # ✅ Apply augmentation before extracting patches\n                        patches = self._extract_patches(img_array)\n                        patches = [augmentation_pipeline(patch) for patch in patches]\n\n                        del img, img_array, buffer, image_bytes\n                        return patches, label\n\n                    except (OSError, IOError, ValueError) as e:\n                        print(f\"Warning: Corrupt BCF image at index {bcf_idx}: {e}\")\n                        return [], -1\n\n            except Exception as e:\n                print(f\"Unexpected error processing idx {idx}: {e}\")\n\n            if retry < max_retries - 1:\n                idx = (int(idx) + 1) % len(self)\n\n        return [], -1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:37.524067Z","iopub.execute_input":"2025-05-18T07:58:37.524801Z","iopub.status.idle":"2025-05-18T07:58:38.483503Z","shell.execute_reply.started":"2025-05-18T07:58:37.524776Z","shell.execute_reply":"2025-05-18T07:58:38.482854Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# memory_efficient_patch_collate_fn\nimport gc\nimport warnings\nfrom functools import partial\n\n# Add this memory-efficient patch collate function\ndef memory_efficient_patch_collate_fn(batch, patch_size_tuple):\n    \"\"\"\n    Memory-efficient version of patch_collate_fn that processes one patch at a time\n    and includes robust error handling.\n    \"\"\"\n    import gc  # Import inside function for worker processes\n    \n    all_patches = []\n    all_labels = []\n    valid_batch_items = 0\n\n    # Process one item at a time to avoid large memory allocations\n    for item in batch:\n        patches, label = item\n        # Ensure item is valid\n        if patches and label != -1:\n            # Process patches one by one\n            for patch in patches:\n                all_patches.append(patch)\n                all_labels.append(label)\n            valid_batch_items += 1\n    \n    # Periodically force garbage collection\n    if len(all_patches) > 100:\n        gc.collect()\n    \n    # Empty batch handling\n    if not all_patches:\n        patch_h, patch_w = patch_size_tuple\n        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n\n    # Process in smaller chunks to reduce peak memory usage\n    max_chunk_size = 64  # Adjust based on your GPU memory\n    num_patches = len(all_patches)\n    patches_tensor_list = []\n    \n    for i in range(0, num_patches, max_chunk_size):\n        chunk = all_patches[i:i+max_chunk_size]\n        # Convert to NumPy array\n        chunk_np = np.stack(chunk)\n        # Convert to tensor, normalize and add channel dimension\n        chunk_tensor = torch.from_numpy(chunk_np).float() / 255.0\n        chunk_tensor = chunk_tensor.unsqueeze(1)\n        patches_tensor_list.append(chunk_tensor)\n        \n        # Clear variables to free memory\n        del chunk, chunk_np\n    \n    # Concatenate chunks\n    patches_tensor = torch.cat(patches_tensor_list, dim=0)\n    labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n    \n    # Clean up\n    del patches_tensor_list, all_patches, all_labels\n    gc.collect()\n    \n    return patches_tensor, labels_tensor\n\n# Add this function to create optimized DataLoaders\nimport torch\nfrom torch.utils.data import DataLoader\nfrom functools import partial\n\ndef create_optimized_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper error handling, avoiding HuggingFace datasets compatibility issues.\n    \n    Args:\n        dataset: The image dataset instance\n        batch_size: Batch size for training\n        num_workers: Number of worker processes\n        val_split: Validation split ratio (0-1)\n        \n    Returns:\n        tuple: (train_loader, val_loader)\n    \"\"\"\n    from torch.utils.data import DataLoader, Subset\n    import numpy as np\n    \n    # Calculate split sizes\n    dataset_size = len(dataset)\n    indices = np.arange(dataset_size)\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets - this avoids Hugging Face's __getitems__ implementation\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    # Custom collate function with error handling\n    def safe_collate(batch):\n        # Filter out empty or invalid items\n        valid_batch = [(patches, label) for patches, label in batch if patches and label != -1]\n        \n        if not valid_batch:\n            # Return empty tensors if no valid items\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n        \n        # Process valid items\n        all_patches = []\n        all_labels = []\n        \n        for patches, label in valid_batch:\n            if isinstance(patches, list) and patches:\n                all_patches.extend(patches)\n                all_labels.extend([label] * len(patches))\n        \n        if not all_patches:\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n            \n        # Convert to PyTorch tensors\n        try:\n            patches_np = np.array(all_patches)\n            patches_tensor = torch.tensor(patches_np, dtype=torch.float) / 255.0\n            \n            # Add channel dimension if needed\n            if len(patches_tensor.shape) == 3:  # (B, H, W)\n                patches_tensor = patches_tensor.unsqueeze(1)  # -> (B, 1, H, W)\n                \n            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n            return patches_tensor, labels_tensor\n        except Exception as e:\n            print(f\"Error in collate function: {e}\")\n            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n    \n    # Create DataLoaders with minimal worker configuration for stability\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=safe_collate,\n        pin_memory=False,\n        persistent_workers=True if num_workers > 0 else False\n    )\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:41.173554Z","iopub.execute_input":"2025-05-18T07:58:41.174466Z","iopub.status.idle":"2025-05-18T07:58:41.188936Z","shell.execute_reply.started":"2025-05-18T07:58:41.174439Z","shell.execute_reply":"2025-05-18T07:58:41.188083Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# load dataset -> create dataloader \nimport torch\nimport torch.nn as nn\nimport os\nimport gc\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\n\n# Clean memory before starting\ngc.collect()\ntorch.cuda.empty_cache()\n\njpeg_dir = None\nbcf_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf\"\nlabel_file = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label\"\n\n# Print GPU info\nif torch.cuda.is_available():\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n    print(f\"Available memory: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n\n# Create dataset with smaller patch size and fewer patches per image\nclassifier_dataset = FontDataset(\n    jpeg_dir=jpeg_dir,\n    bcf_file=bcf_file,\n    label_file=label_file,\n    num_patch=3,  # Number of patches per image\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:47.111983Z","iopub.execute_input":"2025-05-18T07:58:47.112594Z","iopub.status.idle":"2025-05-18T07:58:47.401282Z","shell.execute_reply.started":"2025-05-18T07:58:47.11257Z","shell.execute_reply":"2025-05-18T07:58:47.400562Z"}},"outputs":[{"name":"stdout","text":"Using GPU: Tesla P100-PCIE-16GB\nTotal memory: 17.06 GB\nAvailable memory: 0.00 GB\nWarning: jpeg_dir is None, skipping JPEG data loading.\nLoaded 202000 labels from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label.\nLoaded 202000 images from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf.\nLoaded 202000 .bcf images.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"jpeg_dir = None\nbcf_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\nlabel_file = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\ntest_classifier_dataset = FontDataset(\n    jpeg_dir=jpeg_dir,\n    bcf_file=bcf_file,\n    label_file=label_file,\n    num_patch=3,  # Number of patches per image\n)\n\ntest_classifier_loader, test_loader = create_patch_dataloaders(\n    classifier_dataset,\n    batch_size=64,\n    num_workers=2,\n    val_split=0,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:57:29.014457Z","iopub.execute_input":"2025-05-18T08:57:29.015229Z","iopub.status.idle":"2025-05-18T08:57:29.096426Z","shell.execute_reply.started":"2025-05-18T08:57:29.015177Z","shell.execute_reply":"2025-05-18T08:57:29.095711Z"}},"outputs":[{"name":"stdout","text":"Warning: jpeg_dir is None, skipping JPEG data loading.\nLoaded 3202 labels from /kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label.\nLoaded 3202 images from /kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf.\nLoaded 3202 .bcf images.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"len(test_classifier_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:57:39.682836Z","iopub.execute_input":"2025-05-18T08:57:39.683134Z","iopub.status.idle":"2025-05-18T08:57:39.688749Z","shell.execute_reply.started":"2025-05-18T08:57:39.683114Z","shell.execute_reply":"2025-05-18T08:57:39.688177Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"3157"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Visualization some samples from the combined dataset \ndef visualize_simple_images_and_patches(dataset, num_images=2, seed=None):\n    \"\"\"\n    Visualizes full images and their extracted patches in a simple layout.\n    Shows images and their 3 patches in a clean format with error handling.\n    \n    Args:\n        dataset: A CombinedImageDataset or BCFImagePatchDataset instance\n        num_images: Number of images to visualize (default: 2)\n        seed: Random seed for reproducibility\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import random\n    from PIL import Image, ImageFile\n    from io import BytesIO\n    import os\n    \n    # Allow loading of truncated images\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n    \n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Find valid images (with patches)\n    valid_indices = []\n    attempts = 0\n    max_attempts = min(len(dataset) * 2, 100)  # Limit search attempts\n    \n    while len(valid_indices) < num_images and attempts < max_attempts:\n        idx = random.randint(0, len(dataset) - 1)\n        if idx not in valid_indices:  # Avoid duplicates\n            try:\n                patches, label = dataset[idx]\n                if patches and len(patches) > 0:\n                    valid_indices.append(idx)\n            except Exception as e:\n                print(f\"Error loading index {idx}: {e}\")\n            attempts += 1\n    \n    # If we couldn't find enough valid images\n    if len(valid_indices) < num_images:\n        print(f\"Warning: Could only find {len(valid_indices)} valid images with patches\")\n        if len(valid_indices) == 0:\n            print(\"No valid images found. Check your dataset.\")\n            return\n    \n    # Create figure with enough space for all elements\n    fig, axes = plt.subplots(len(valid_indices), 4, figsize=(16, 5 * len(valid_indices)))\n    \n    # If only one image is requested, make axes indexable as 2D\n    if len(valid_indices) == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i, idx in enumerate(valid_indices):\n        try:\n            # Get item directly from dataset\n            patches, label = dataset[idx]\n            \n            # Get the original full image\n            img_array = None\n            \n            if hasattr(dataset, 'jpeg_data') and idx < len(dataset.jpeg_data):\n                # From jpeg_data\n                img_path, _ = dataset.jpeg_data[idx]\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {os.path.basename(img_path)}\"\n                \n            elif hasattr(dataset, 'image_filenames') and not hasattr(dataset, 'num_images'):\n                # From BCFImagePatchDataset with JPEG source\n                img_path = os.path.join(dataset.data_source, dataset.image_filenames[idx])\n                img = Image.open(img_path).convert('L')\n                img_array = np.array(img)\n                source = f\"JPEG file: {dataset.image_filenames[idx]}\"\n                \n            else:\n                # From BCF file (either CombinedImageDataset or BCFImagePatchDataset)\n                if hasattr(dataset, 'bcf_data'):\n                    # CombinedImageDataset\n                    bcf_idx = idx - len(dataset.jpeg_data)\n                    if bcf_idx < 0 or bcf_idx >= len(dataset.bcf_data):\n                        print(f\"Invalid BCF index: {bcf_idx}\")\n                        continue\n                        \n                    offset = dataset.image_offsets[bcf_idx]\n                    size = dataset.image_sizes[bcf_idx]\n                    data_file = dataset.bcf_file\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {bcf_idx})\"\n                else:\n                    # BCFImagePatchDataset\n                    offset = dataset.image_offsets[idx]\n                    size = dataset.image_sizes[idx]\n                    data_file = dataset.data_source\n                    data_start = dataset.data_start_offset\n                    source = f\"BCF file (idx: {idx})\"\n                \n                with open(data_file, 'rb') as f:\n                    f.seek(data_start + offset)\n                    image_bytes = f.read(size)\n                img = Image.open(BytesIO(image_bytes)).convert('L')\n                img_array = np.array(img)\n            \n            # Plot original image if we successfully loaded it\n            if img_array is not None:\n                axes[i, 0].imshow(img_array, cmap='gray')\n                axes[i, 0].set_title(f\"Original Image\\nLabel: {label}\\nSource: {source} Shape: {img_array.shape}\")\n                axes[i, 0].axis('off')\n            else:\n                axes[i, 0].text(0.5, 0.5, \"Image loading failed\", ha='center', va='center')\n                axes[i, 0].axis('off')\n            \n            # Plot the patches - ensure we have patches to display\n            if patches and len(patches) > 0:\n                for j in range(3):\n                    if j < len(patches):\n                        patch = patches[j]\n                        axes[i, j+1].imshow(patch, cmap='gray')\n                        axes[i, j+1].set_title(f\"Patch {j+1}\\nShape: {patch.shape}\")\n                    else:\n                        # No more patches to display\n                        axes[i, j+1].text(0.5, 0.5, \"No patch\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            else:\n                # No patches for this image\n                for j in range(3):\n                    axes[i, j+1].text(0.5, 0.5, \"No patches extracted\", ha='center', va='center')\n                    axes[i, j+1].axis('off')\n            \n        except Exception as e:\n            print(f\"Error processing index {idx}: {e}\")\n            # Create error message in subplot\n            for j in range(4):\n                axes[i, j].text(0.5, 0.5, f\"Error: {str(e)[:50]}...\", ha='center', va='center')\n                axes[i, j].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return the indices we used (helpful for debugging)\n    return valid_indices\n\n# Example usage:\nvisualize_simple_images_and_patches(combined_dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, Subset\nimport gc\nimport torch\nimport numpy as np\n\nclass DatasetWrapper(Dataset):\n    \"\"\"\n    A wrapper for your dataset to ensure compatibility with DataLoader\n    \"\"\"\n    def __init__(self, original_dataset):\n        self.dataset = original_dataset\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # Get a single item by index, handling both direct dataset access\n        # and access through Subset indices\n        try:\n            # Handle if we're accessing through a Subset\n            if hasattr(self.dataset, 'dataset') and hasattr(self.dataset, 'indices'):\n                original_idx = self.dataset.indices[idx]\n                return self.dataset.dataset[original_idx]\n            # Normal access\n            return self.dataset[idx]\n        except Exception as e:\n            print(f\"Error accessing item {idx}: {e}\")\n            # Return a placeholder for invalid items\n            return [], -1\n\n\ndef create_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n    \"\"\"\n    Creates DataLoaders with proper handling for HuggingFace datasets.\n    \"\"\"\n    # Ensure the dataset is properly wrapped\n    wrapped_dataset = DatasetWrapper(dataset)\n    \n    # Calculate split sizes\n    dataset_size = len(wrapped_dataset)\n    indices = list(range(dataset_size))\n    np.random.shuffle(indices)\n    \n    split_idx = int(np.floor(val_split * dataset_size))\n    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n    \n    # Create subset datasets\n    train_dataset = Subset(wrapped_dataset, train_indices)\n    val_dataset = Subset(wrapped_dataset, val_indices)\n    \n    # Custom collate function\n    def custom_collate_fn(batch):\n        # Filter out invalid items\n        batch = [(img, label) for img, label in batch if img is not None and len(img) > 0 and label != -1]\n        \n        if not batch:\n            # Return empty tensors with appropriate dimensions\n            return torch.empty((0, 3, 105, 105), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n        \n        # Extract images and labels\n        images, labels = zip(*batch)\n        \n        # Convert to tensors\n        images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in images])\n        labels_tensor = torch.tensor(labels, dtype=torch.long)\n        \n        # Normalize images if needed\n        if images_tensor.max() > 1.0:\n            images_tensor = images_tensor / 255.0\n            \n        return images_tensor, labels_tensor\n    \n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=custom_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=custom_collate_fn,\n        pin_memory=False,\n        persistent_workers=num_workers > 0\n    )\n    \n    return train_loader, val_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_classifier_loader, val_classifier_loader = create_patch_dataloaders(\n    classifier_dataset,\n    batch_size=64,\n    num_workers=2,\n    val_split=0.1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:10.152632Z","iopub.execute_input":"2025-05-18T07:59:10.153381Z","iopub.status.idle":"2025-05-18T07:59:10.180728Z","shell.execute_reply.started":"2025-05-18T07:59:10.153352Z","shell.execute_reply":"2025-05-18T07:59:10.179956Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"len(train_classifier_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:12.753802Z","iopub.execute_input":"2025-05-18T07:59:12.754094Z","iopub.status.idle":"2025-05-18T07:59:12.759665Z","shell.execute_reply.started":"2025-05-18T07:59:12.754072Z","shell.execute_reply":"2025-05-18T07:59:12.758898Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"2841"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_sample = (next(iter(train_classifier_loader)))\nval_sample = (next(iter(val_classifier_loader)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_sample[1].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nscae = SCAE().to(device)\nscae.load_state_dict(torch.load(\"/kaggle/working/best_model.pt\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# classifier\nclass FontClassifier(nn.Module):\n    def __init__(self, pretrained_scae, num_classes=2383, normalization_type=\"batch_norm\", \n                 use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n        super().__init__()\n        self.pretrained_scae = pretrained_scae  # Use pretrained SCAE encoder\n        \n        # Define helper functions for creating layers\n        def norm_layer(num_features, spatial_size=None):\n            if normalization_type == \"batch_norm\":\n                return nn.BatchNorm2d(num_features)\n            elif normalization_type == \"group_norm\":\n                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n            elif normalization_type == \"layer_norm\" and spatial_size is not None:\n                return nn.LayerNorm([num_features, spatial_size, spatial_size])\n            else:\n                return nn.Identity()\n\n        def activation_layer():\n            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n\n        def dropout_layer():\n            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n        \n        # CNN head after the SCAE encoder\n        # SCAE encoder output is 128 x 26 x 26\n        self.cnn_head = nn.Sequential(\n            # Conv layer 4\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Out: 256 x 12 x 12\n            norm_layer(256, 12),\n            activation_layer(),\n            \n            # Conv layer 5\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Out: 256 x 12 x 12\n            norm_layer(256, 13),\n            activation_layer(),\n            dropout_layer()\n        )\n        \n        # Flatten layer\n        self.flatten = nn.Flatten()\n        \n        # Fully connected layers\n        # Input size is 256 * 12 * 12 = 43,264\n        self.fully_connected = nn.Sequential(\n            nn.Linear(256 * 12 * 12, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_prob if use_dropout else 0),\n            \n            nn.Linear(4096, 2048),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_prob if use_dropout else 0),\n            \n            nn.Linear(2048, num_classes),\n            # nn.Softmax(dim=1) no softmax here bro, crossentropy does the softmax automatically\n        )\n\n    def forward(self, x):\n        # Use the encoder part of SCAE\n        x = self.pretrained_scae.encoder(x)\n        # Continue with additional CNN layers\n        x = self.cnn_head(x)\n        \n        # Flatten and apply fully connected layers\n        x = self.flatten(x)\n        x = self.fully_connected(x)\n        \n        return x\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:26.689877Z","iopub.execute_input":"2025-05-18T07:59:26.690382Z","iopub.status.idle":"2025-05-18T07:59:26.702771Z","shell.execute_reply.started":"2025-05-18T07:59:26.69036Z","shell.execute_reply":"2025-05-18T07:59:26.701891Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"classifier = FontClassifier(scae, num_classes=2383).to(device)\n\nfor batch in train_classifier_loader:\n    print(batch[0].shape)\n    print(classifier(batch[0].to(device)).shape)\n    # show_images_in_grid(batch.permute(0, 2, 3, 1).numpy(), titles=[f'Patch {i+1}' for i in range(len(batch))], cols=4)\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_classifier_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\nimport numpy as np\n\n# Initialize model, loss, optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nscae = SCAE().to(device)\nscae.load_state_dict(torch.load(\"/kaggle/working/best_model.pt\", weights_only=True))\nclassifier = FontClassifier(scae, num_classes=2383).to(device)\n\n# For classification, use CrossEntropyLoss\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer with momentum and weight decay\noptimizer = optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n\n# Learning rate scheduler to reduce LR when validation loss plateaus\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n\n# Training parameters\nnum_epochs = 5\nbest_val_loss = float('inf')\npatience = 7  # For early stopping\npatience_counter = 0\n\n# Tracking metrics\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Training phase\n    classifier.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    batch_count = 0\n    \n    # Training loop\n    for inputs, labels in tqdm(train_classifier_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n        # Skip empty batches\n        if inputs.numel() == 0:\n            continue\n            \n        # Move tensors to device\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = classifier(inputs)\n        # print(outputs.shape, labels.shape)\n        # Calculate loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        batch_count += 1\n    \n    # Calculate epoch statistics\n    if batch_count > 0:\n        epoch_loss = running_loss / batch_count\n        epoch_acc = 100 * correct / total\n        train_losses.append(epoch_loss)\n        print(f'Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.2f}%')\n    else:\n        print(\"No batches processed during training\")\n    \n    # Validation phase\n    classifier.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    val_batch_count = 0\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(val_classifier_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n            # Skip empty batches\n            if inputs.numel() == 0:\n                continue\n                \n            # Move tensors to device\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = classifier(inputs)\n            # print(outputs.shape, labels.shape)\n            \n            # Calculate loss\n            loss = criterion(outputs, labels)\n            \n            # Statistics\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            val_batch_count += 1\n    \n    # Calculate validation statistics\n    if val_batch_count > 0:\n        epoch_val_loss = val_loss / val_batch_count\n        epoch_val_acc = 100 * val_correct / val_total\n        val_losses.append(epoch_val_loss)\n        val_accuracies.append(epoch_val_acc)\n        print(f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%')\n        \n        # Update learning rate based on validation loss\n        scheduler.step(epoch_val_loss)\n        \n        # Check if this is the best model so far\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss\n            patience_counter = 0\n            # Save best model\n            torch.save(classifier.state_dict(), 'best_font_classifier.pt')\n            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")\n        else:\n            patience_counter += 1\n            # Early stopping\n            if patience_counter >= patience:\n                print(f\"Early stopping after {epoch+1} epochs\")\n                break\n    else:\n        print(\"No batches processed during validation\")\n\nprint(\"Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:36.402263Z","iopub.execute_input":"2025-05-18T07:59:36.402881Z","iopub.status.idle":"2025-05-18T08:54:27.343932Z","shell.execute_reply.started":"2025-05-18T07:59:36.402839Z","shell.execute_reply":"2025-05-18T08:54:27.34301Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\nEpoch 1/5 [Train]: 100%|██████████| 2841/2841 [09:56<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0153 | Train Acc: 46.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5 [Val]: 100%|██████████| 316/316 [01:01<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.1898 | Val Acc: 62.78%\nSaved best model with validation loss: 1.1898\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5 [Train]: 100%|██████████| 2841/2841 [10:17<00:00,  4.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6154 | Train Acc: 79.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5 [Val]: 100%|██████████| 316/316 [01:04<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6010 | Val Acc: 79.95%\nSaved best model with validation loss: 0.6010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5 [Train]: 100%|██████████| 2841/2841 [09:44<00:00,  4.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3766 | Train Acc: 86.85%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5 [Val]: 100%|██████████| 316/316 [01:01<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.3114 | Val Acc: 88.99%\nSaved best model with validation loss: 0.3114\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5 [Train]: 100%|██████████| 2841/2841 [10:01<00:00,  4.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2857 | Train Acc: 89.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5 [Val]: 100%|██████████| 316/316 [01:01<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2628 | Val Acc: 90.60%\nSaved best model with validation loss: 0.2628\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5 [Train]: 100%|██████████| 2841/2841 [09:32<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2377 | Train Acc: 91.55%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5 [Val]: 100%|██████████| 316/316 [00:59<00:00,  5.31it/s]","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2674 | Val Acc: 90.76%\nTraining completed!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# evaluation\nimport torch\nfrom tqdm import tqdm\n\ndef calculate_top_k_error(model, test_loader, device, k_values=[1, 5]):\n    \"\"\"\n    Calculate top-k error rates for a model on a test dataset.\n    \n    Args:\n        model: The PyTorch model to evaluate\n        test_loader: DataLoader for the test dataset\n        device: Device to run evaluation on (cuda or cpu)\n        k_values: List of k values for top-k error calculation\n    \n    Returns:\n        dict: Dictionary containing error rates and accuracies\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    \n    # Initialize counters\n    total_samples = 0\n    correct_predictions = {k: 0 for k in k_values}\n    \n    with torch.no_grad():  # No need to track gradients\n        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            if inputs.numel() == 0:  # Skip empty batches\n                continue\n                \n            # Move data to the appropriate device\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            batch_size = labels.size(0)\n            total_samples += batch_size\n            \n            # Get topk predictions for each sample\n            for k in k_values:\n                # Get top-k class indices for each sample\n                _, topk_indices = torch.topk(outputs, k, dim=1)\n                \n                # Check if true label is in top-k predictions for each sample\n                for i in range(batch_size):\n                    if labels[i].item() in topk_indices[i]:\n                        correct_predictions[k] += 1\n    \n    # Calculate error rates and accuracies\n    error_rates = {k: 1.0 - (correct_predictions[k] / total_samples) for k in k_values}\n    accuracies = {k: correct_predictions[k] / total_samples for k in k_values}\n    \n    return {\"error_rates\": error_rates, \"accuracies\": accuracies}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:55:08.893386Z","iopub.execute_input":"2025-05-18T08:55:08.893749Z","iopub.status.idle":"2025-05-18T08:55:08.901897Z","shell.execute_reply.started":"2025-05-18T08:55:08.893716Z","shell.execute_reply":"2025-05-18T08:55:08.901114Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Make sure your model is properly loaded and in eval mode\nclassifier.eval()\n\n# Run the evaluation\nresults = calculate_top_k_error(\n    model=classifier,\n    test_loader=test_classifier_loader,\n    device=device,\n    k_values=[1, 5]\n)\n\n# Print the results with percentages\nprint(f\"Top-1 Error: {results['error_rates'][1]:.4f} ({results['error_rates'][1]*100:.2f}%)\")\nprint(f\"Top-5 Error: {results['error_rates'][5]:.4f} ({results['error_rates'][5]*100:.2f}%)\")\nprint(f\"Top-1 Accuracy: {results['accuracies'][1]:.4f} ({results['accuracies'][1]*100:.2f}%)\")\nprint(f\"Top-5 Accuracy: {results['accuracies'][5]:.4f} ({results['accuracies'][5]*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:57:52.060126Z","iopub.execute_input":"2025-05-18T08:57:52.060818Z","iopub.status.idle":"2025-05-18T09:08:22.610608Z","shell.execute_reply.started":"2025-05-18T08:57:52.060795Z","shell.execute_reply":"2025-05-18T09:08:22.609649Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 3157/3157 [10:30<00:00,  5.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Top-1 Error: 0.0914 (9.14%)\nTop-5 Error: 0.0042 (0.42%)\nTop-1 Accuracy: 0.9086 (90.86%)\nTop-5 Accuracy: 0.9958 (99.58%)\n","output_type":"stream"}],"execution_count":15}]}