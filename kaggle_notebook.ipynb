{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a376e7",
   "metadata": {
    "papermill": {
     "duration": 0.007179,
     "end_time": "2025-05-25T17:07:17.357039",
     "exception": false,
     "start_time": "2025-05-25T17:07:17.349860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://www.kaggle.com/code/ndannnop/computer-vision?scriptVersionId=240710736\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af865faf",
   "metadata": {
    "papermill": {
     "duration": 0.005566,
     "end_time": "2025-05-25T17:07:17.368391",
     "exception": false,
     "start_time": "2025-05-25T17:07:17.362825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133e53d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:17.381863Z",
     "iopub.status.busy": "2025-05-25T17:07:17.381633Z",
     "iopub.status.idle": "2025-05-25T17:07:28.291431Z",
     "shell.execute_reply": "2025-05-25T17:07:28.290519Z"
    },
    "papermill": {
     "duration": 10.918046,
     "end_time": "2025-05-25T17:07:28.292979",
     "exception": false,
     "start_time": "2025-05-25T17:07:17.374933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "import random\n",
    "import math # Needed for ceiling division\n",
    "\n",
    "import easyocr\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "\n",
    "# Global OCR reader for efficiency\n",
    "_ocr_reader = None\n",
    "\n",
    "def get_ocr_reader(languages=[\"en\"]):\n",
    "    global _ocr_reader\n",
    "    if _ocr_reader is None:\n",
    "        _ocr_reader = easyocr.Reader(languages)\n",
    "    return _ocr_reader\n",
    "\n",
    "def extract_patches(image_array, num_patch=3, patch_size=(105, 105), \n",
    "                    extract_text=True, min_text_coverage=0.3, max_attempts=20):\n",
    "    patch_h, patch_w = patch_size\n",
    "\n",
    "    # Determine if grayscale or color\n",
    "    if image_array.ndim == 2:\n",
    "        h, w = image_array.shape\n",
    "        is_grayscale = True\n",
    "    elif image_array.ndim == 3:\n",
    "        h, w, _ = image_array.shape\n",
    "        is_grayscale = False\n",
    "    else:\n",
    "        print(f\"Unexpected image shape: {image_array.shape}\")\n",
    "        return []\n",
    "\n",
    "    # === Step 1: Resize image to height = 105, maintain aspect ratio ===\n",
    "    scale_factor = patch_h / h\n",
    "    new_w = int(w * scale_factor)\n",
    "    if is_grayscale:\n",
    "        resized = cv2.resize(image_array, (new_w, patch_h), interpolation=cv2.INTER_LINEAR)\n",
    "    else:\n",
    "        resized = cv2.resize(image_array, (new_w, patch_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # === Step 2: Check if width is enough for patch ===\n",
    "    if new_w < patch_w:\n",
    "        return []\n",
    "\n",
    "    # === Step 3: If not extracting text, return random crops ===\n",
    "    if not extract_text:\n",
    "        patches = []\n",
    "        for _ in range(num_patch):\n",
    "            x = np.random.randint(0, new_w - patch_w + 1)\n",
    "            if is_grayscale:\n",
    "                patch = resized[:, x:x + patch_w]\n",
    "            else:\n",
    "                patch = resized[:, x:x + patch_w, :]\n",
    "            patches.append(patch)\n",
    "        return patches\n",
    "\n",
    "    # === Step 4: Try to find text patches using OCR ===\n",
    "    reader = get_ocr_reader()\n",
    "    text_patches = []\n",
    "    attempts = 0\n",
    "\n",
    "    while len(text_patches) < num_patch and attempts < max_attempts:\n",
    "        x = np.random.randint(0, new_w - patch_w + 1)\n",
    "        if is_grayscale:\n",
    "            patch = resized[:, x:x + patch_w]\n",
    "        else:\n",
    "            patch = resized[:, x:x + patch_w, :]\n",
    "\n",
    "        # Save patch to temporary file for OCR\n",
    "        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:\n",
    "            tmp_path = tmp.name\n",
    "            patch_img = Image.fromarray(patch)\n",
    "            patch_img.save(tmp_path)\n",
    "\n",
    "        try:\n",
    "            ocr_results = reader.readtext(tmp_path)\n",
    "            os.unlink(tmp_path)\n",
    "\n",
    "            patch_area = patch_h * patch_w\n",
    "            text_area = 0\n",
    "            for bbox, text, conf in ocr_results:\n",
    "                if conf < 0.5:\n",
    "                    continue\n",
    "                bbox = [[int(p[0]), int(p[1])] for p in bbox]\n",
    "                min_x = max(0, min(p[0] for p in bbox))\n",
    "                max_x = min(patch_w, max(p[0] for p in bbox))\n",
    "                min_y = max(0, min(p[1] for p in bbox))\n",
    "                max_y = min(patch_h, max(p[1] for p in bbox))\n",
    "                if max_x > min_x and max_y > min_y:\n",
    "                    text_area += (max_x - min_x) * (max_y - min_y)\n",
    "\n",
    "            if text_area / patch_area >= min_text_coverage:\n",
    "                text_patches.append(patch)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"OCR error: {e}\")\n",
    "            try:\n",
    "                os.unlink(tmp_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    return text_patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3dfff36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.305529Z",
     "iopub.status.busy": "2025-05-25T17:07:28.305183Z",
     "iopub.status.idle": "2025-05-25T17:07:28.317927Z",
     "shell.execute_reply": "2025-05-25T17:07:28.317346Z"
    },
    "papermill": {
     "duration": 0.020064,
     "end_time": "2025-05-25T17:07:28.318881",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.298817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# augmentation functions\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "TARGET_SIZE = (105, 105)\n",
    "\n",
    "def to_uint8(img: np.ndarray) -> np.ndarray:\n",
    "    return np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "def noise_image(img: np.ndarray, mean=0.0, std=3.0) -> np.ndarray:\n",
    "    \"\"\"Add Gaussian noise.\"\"\"\n",
    "    f = img.astype(np.float32)\n",
    "    n = np.random.normal(mean, std, f.shape).astype(np.float32)\n",
    "    return to_uint8(f + n)\n",
    "\n",
    "def blur_image(img: np.ndarray, sigma_range=(0.5, 1.5)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a mild, randomly‐parameterized Gaussian blur.\n",
    "    \n",
    "    Args:\n",
    "        img (np.ndarray): Input image, either H×W or H×W×C, dtype uint8.\n",
    "        sigma_range (tuple): Min/max sigma for the blur kernel.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Blurred image, same shape and dtype uint8.\n",
    "    \"\"\"\n",
    "    # Ensure float for convolution\n",
    "    f = img.astype(np.float32)\n",
    "    # Randomize sigma in the given range\n",
    "    sigma = random.uniform(*sigma_range)\n",
    "    # OpenCV: kernel size (0,0) triggers automatic size based on sigma\n",
    "    if f.ndim == 2:\n",
    "        blurred = cv2.GaussianBlur(f, ksize=(0, 0), sigmaX=sigma, sigmaY=sigma)\n",
    "    else:\n",
    "        # Split channels and blur each independently\n",
    "        channels = cv2.split(f)\n",
    "        channels = [cv2.GaussianBlur(ch, ksize=(0, 0), sigmaX=sigma, sigmaY=sigma)\n",
    "                    for ch in channels]\n",
    "        blurred = cv2.merge(channels)\n",
    "    # Restore uint8\n",
    "    return np.clip(blurred, 0, 255).astype(np.uint8)\n",
    "\n",
    "def affine_rotation(img: np.ndarray, max_deg=10) -> np.ndarray:\n",
    "    \"\"\"Small random affine warp.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    # random shift on three points\n",
    "    src = np.float32([[0,0],[w-1,0],[0,h-1]])\n",
    "    dx = w * 0.05\n",
    "    dy = h * 0.05\n",
    "    dst = src + np.random.uniform([-dx, -dy], [dx, dy], src.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(src, dst)\n",
    "    warped = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    return warped\n",
    "\n",
    "def shading_gradient(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Multiply by a random horizontal or vertical linear illumination gradient.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    start, end = random.uniform(0.6,1.4), random.uniform(0.6,1.4)\n",
    "    if random.choice([True,False]):\n",
    "        # horizontal\n",
    "        grad = np.linspace(start, end, w, dtype=np.float32)[None,:]\n",
    "        mask = np.repeat(grad, h, axis=0)\n",
    "    else:\n",
    "        # vertical\n",
    "        grad = np.linspace(start, end, h, dtype=np.float32)[:,None]\n",
    "        mask = np.repeat(grad, w, axis=1)\n",
    "    if img.ndim==3:\n",
    "        mask = mask[:,:,None]\n",
    "    shaded = img.astype(np.float32) * mask\n",
    "    return to_uint8(shaded)\n",
    "\n",
    "def variable_aspect_ratio_preprocess(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Squeeze width by a random factor in [5/6,7/6], then pad/crop to original.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    factor = random.uniform(5/6, 7/6)\n",
    "    new_w = max(1, int(w/factor))\n",
    "    resized = cv2.resize(img, (new_w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    # pad or crop back to w x h\n",
    "    if new_w < w:\n",
    "        pad = ( (0,0), ( (w-new_w)//2, (w-new_w)-(w-new_w)//2 ) ) + ((0,0),) if img.ndim==3 else ( (0,0), ( (w-new_w)//2, (w-new_w)-(w-new_w)//2 ) )\n",
    "        resized = np.pad(resized, pad, mode='reflect')\n",
    "    else:\n",
    "        x0 = (new_w - w)//2\n",
    "        resized = resized[:, x0:x0+w]\n",
    "    return resized\n",
    "\n",
    "def final_resize(img: np.ndarray, size=TARGET_SIZE) -> np.ndarray:\n",
    "    \"\"\"Resize to target patch size.\"\"\"\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def augmentation_pipeline(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a random subset of the six DeepFont augmentations:\n",
    "      - noise, blur, affine warp, shading, variable spacing (as AR squeeze)\n",
    "      - note: gradient_fill (Laplacian) dropped since shading covers illumination.\n",
    "    Finally, resize to TARGET_SIZE.\n",
    "    \"\"\"\n",
    "    # Ensure uint8\n",
    "    img = to_uint8(img)\n",
    "    # 1) variable aspect ratio\n",
    "    img = variable_aspect_ratio_preprocess(img)\n",
    "    # 2) choose 2–4 augmentations from the pool\n",
    "    pool = [\n",
    "        noise_image,\n",
    "        blur_image,\n",
    "        affine_rotation,\n",
    "        shading_gradient\n",
    "    ]\n",
    "    for fn in pool:\n",
    "        img = fn(img)\n",
    "    # 3) final resize\n",
    "    img = final_resize(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8cab9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.330734Z",
     "iopub.status.busy": "2025-05-25T17:07:28.330548Z",
     "iopub.status.idle": "2025-05-25T17:07:28.351462Z",
     "shell.execute_reply": "2025-05-25T17:07:28.350941Z"
    },
    "papermill": {
     "duration": 0.02806,
     "end_time": "2025-05-25T17:07:28.352405",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.324345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from io import BytesIO\n",
    "# from datasets import Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class that combines both .jpeg files and .bcf files into a single dataset.\n",
    "    This class can handle loading and patch extraction from both .jpeg and .bcf files.\n",
    "    \"\"\"\n",
    "    def __init__(self, jpeg_dir, bcf_file, label_file, testing=False, num_patch=3, patch_size=(105, 105), \n",
    "                 extract_text=False, min_text_coverage=0.3, max_attempts=20, ocr_languages=[\"en\"]):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading both jpeg files and bcf files into one dataset.\n",
    "\n",
    "        Args:\n",
    "            jpeg_dir (str): Directory containing .jpeg files.\n",
    "            bcf_file (str): Path to the .bcf file.\n",
    "            label_file (str): Path to the label file corresponding to the .bcf file.\n",
    "            num_patch (int): Number of patches to extract per image.\n",
    "            patch_size (tuple): Tuple (height, width) for the size of the patches.\n",
    "            extract_text (bool): Whether to prioritize patches containing text\n",
    "            min_text_coverage (float): Minimum ratio of text area to patch area (0-1)\n",
    "            max_attempts (int): Maximum number of attempts to find text patches\n",
    "            ocr_languages (list): Languages for EasyOCR to detect\n",
    "        \"\"\"\n",
    "        self.jpeg_dir = jpeg_dir\n",
    "        self.bcf_file = bcf_file\n",
    "        self.label_file = label_file\n",
    "        self.testing = testing\n",
    "        self.num_patch = num_patch\n",
    "        self.patch_size = patch_size\n",
    "        self.extract_text = extract_text\n",
    "        self.min_text_coverage = min_text_coverage\n",
    "        self.max_attempts = max_attempts\n",
    "        self.ocr_languages = ocr_languages\n",
    "\n",
    "        # Initialize OCR reader if needed\n",
    "        if extract_text:\n",
    "            self.reader = get_ocr_reader(ocr_languages)\n",
    "\n",
    "        self.jpeg_data = []\n",
    "        self.bcf_data = []\n",
    "\n",
    "        # Load jpeg data\n",
    "        self._load_jpeg_data(jpeg_dir)\n",
    "\n",
    "        # Load bcf data\n",
    "        self._load_bcf_data(bcf_file, label_file)\n",
    "\n",
    "    def _extract_patches_test(self, img_array):\n",
    "        h, w = img_array.shape[:2]\n",
    "        target_h, target_w = self.patch_size\n",
    "        # 1) resize height\n",
    "        new_w = int(w * (target_h / h))\n",
    "        img = cv2.resize(img_array, (new_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "        patches = []\n",
    "        for _scale in range(3):\n",
    "            factor = np.random.uniform(1.5, 3.5)\n",
    "            sw = max(1, int(new_w / factor))\n",
    "            squeezed = cv2.resize(img, (sw, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "            # nếu width < target_w thì pad reflect, else crop giữa\n",
    "            if sw < target_w:\n",
    "                pad = target_w - sw\n",
    "                left = pad//2; right = pad-left\n",
    "                squeezed = np.pad(squeezed,\n",
    "                                  ((0,0),(left,right)) + ((0,0),)*(img.ndim-2),\n",
    "                                  mode='reflect')\n",
    "            else:\n",
    "                x0 = (sw - target_w)//2\n",
    "                squeezed = squeezed[:, x0:x0+target_w]\n",
    "            # crop 5 patch random\n",
    "            for _ in range(5):\n",
    "                x = np.random.randint(0, target_w - target_w + 1)\n",
    "                y = np.random.randint(0,      0  + 1)  # vì height==target_h\n",
    "                patch = squeezed[y:y+target_h, x:x+target_w] if img.ndim==2 \\\n",
    "                        else squeezed[y:y+target_h, x:x+target_w, :]\n",
    "                patches.append(patch)\n",
    "        return patches\n",
    "\n",
    "    def _load_jpeg_data(self, jpeg_dir):\n",
    "        \"\"\"Loads the .jpeg files from the specified directory.\"\"\"\n",
    "        if not os.path.exists(jpeg_dir):\n",
    "            print(f\"Warning: JPEG directory {jpeg_dir} does not exist.\")\n",
    "            return\n",
    "            \n",
    "        image_filenames = [f for f in os.listdir(jpeg_dir) if f.lower().endswith(('.jpeg', '.jpg'))]\n",
    "        self.jpeg_data = [(os.path.join(jpeg_dir, f), 0) for f in image_filenames]  # Assuming label is 0 for .jpeg files\n",
    "        print(f\"Loaded {len(self.jpeg_data)} .jpeg images.\")\n",
    "\n",
    "    def _load_bcf_data(self, bcf_file, label_file):\n",
    "        \"\"\"Loads the .bcf file and the associated label file.\"\"\"\n",
    "        if not (os.path.exists(bcf_file) and os.path.exists(label_file)):\n",
    "            print(f\"Warning: BCF file {bcf_file} or label file {label_file} does not exist.\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            with open(label_file, 'rb') as f:\n",
    "                self.labels = np.frombuffer(f.read(), dtype=np.uint32)\n",
    "                print(f\"Loaded {len(self.labels)} labels from {label_file}.\")\n",
    "\n",
    "            with open(bcf_file, 'rb') as f:\n",
    "                self.num_images = np.frombuffer(f.read(8), dtype=np.int64)[0]\n",
    "                print(f\"Loaded {self.num_images} images from {bcf_file}.\")\n",
    "\n",
    "                sizes_bytes = f.read(self.num_images * 8)\n",
    "                self.image_sizes = np.frombuffer(sizes_bytes, dtype=np.int64)\n",
    "\n",
    "                self.data_start_offset = 8 + self.num_images * 8\n",
    "                self.image_offsets = np.zeros(self.num_images + 1, dtype=np.int64)\n",
    "                np.cumsum(self.image_sizes, out=self.image_offsets[1:])\n",
    "\n",
    "                for idx in range(self.num_images):\n",
    "                    self.bcf_data.append((idx, self.labels[idx]))\n",
    "                \n",
    "            print(f\"Loaded {len(self.bcf_data)} .bcf images.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading .bcf data: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of images in the combined dataset.\"\"\"\n",
    "        return len(self.jpeg_data) + len(self.bcf_data)\n",
    "\n",
    "    def _extract_patches(self, img_array):\n",
    "        \"\"\"Helper function to extract patches from an image.\"\"\"\n",
    "        return extract_patches(\n",
    "            img_array, \n",
    "            num_patch=self.num_patch, \n",
    "            patch_size=self.patch_size,\n",
    "            extract_text=self.extract_text, \n",
    "            min_text_coverage=self.min_text_coverage,\n",
    "            max_attempts=self.max_attempts\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches one item with robust error handling for corrupted images.\"\"\"\n",
    "        # Handle case where idx is a list (batch loading)\n",
    "        if isinstance(idx, list):\n",
    "            # Handle batch indices properly\n",
    "            results = []\n",
    "            labels = []\n",
    "            for single_idx in idx:\n",
    "                try:\n",
    "                    patches, label = self.__getitem__(single_idx)  # Call recursively with single index\n",
    "                    if patches and label != -1:\n",
    "                        results.append(patches)\n",
    "                        labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing index {single_idx}: {e}\")\n",
    "                    # Skip this item on error\n",
    "            \n",
    "            # Return whatever valid items we were able to get\n",
    "            return results, labels\n",
    "        \n",
    "        # Original single-item loading logic\n",
    "        max_retries = 3  # Try a few times before giving up on an index\n",
    "        \n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                if idx < len(self.jpeg_data):\n",
    "                    # JPEG image with error handling\n",
    "                    img_path, label = self.jpeg_data[idx]\n",
    "                    try:\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.simplefilter(\"ignore\")  # Ignore PIL warnings\n",
    "                            img = Image.open(img_path)\n",
    "                            img.verify()  # Verify image is not corrupted\n",
    "                        \n",
    "                        # Re-open since verify() closes the file\n",
    "                        img = Image.open(img_path).convert('L')\n",
    "                        img_array = np.array(img)\n",
    "                        patches = self._extract_patches(img_array)\n",
    "                        # patches = [augmentation_pipeline(patch) for patch in patches]\n",
    "                        \n",
    "                        # Clean memory\n",
    "                        del img, img_array\n",
    "                        \n",
    "                        return patches, label\n",
    "                    \n",
    "                    except (OSError, IOError, ValueError) as e:\n",
    "                        # Image is corrupted, return empty list\n",
    "                        print(f\"Warning: Corrupt image at {img_path}: {e}\")\n",
    "                        return [], -1\n",
    "                        \n",
    "                else:\n",
    "                    # BCF image with error handling\n",
    "                    bcf_idx = idx - len(self.jpeg_data)\n",
    "                    if bcf_idx >= len(self.bcf_data):\n",
    "                        return [], -1\n",
    "                        \n",
    "                    label = self.bcf_data[bcf_idx][1]\n",
    "                    offset = self.image_offsets[bcf_idx]\n",
    "                    size = self.image_sizes[bcf_idx]\n",
    "                    \n",
    "                    try:\n",
    "                        with open(self.bcf_file, 'rb') as f:\n",
    "                            f.seek(self.data_start_offset + offset)\n",
    "                            image_bytes = f.read(size)\n",
    "                        \n",
    "                        # Use BytesIO to catch corruption\n",
    "                        buffer = BytesIO(image_bytes)\n",
    "                        img = Image.open(buffer)\n",
    "                        img.verify()  # Verify it's valid\n",
    "                        \n",
    "                        # Re-open since verify() closes the file\n",
    "                        buffer.seek(0)\n",
    "                        img = Image.open(buffer).convert('L')\n",
    "                        img_array = np.array(img)\n",
    "                        \n",
    "                        if self.testing:\n",
    "                            patches = self._extract_patches_test(img_array)\n",
    "                        else:\n",
    "                            patches = self._extract_patches(img_array)\n",
    "                            patches = [augmentation_pipeline(patch) for patch in patches]\n",
    "                        \n",
    "                        # Clean memory\n",
    "                        del img, img_array, buffer, image_bytes\n",
    "                        \n",
    "                        return patches, label\n",
    "                    \n",
    "                    except (OSError, IOError, ValueError) as e:\n",
    "                        print(f\"Warning: Corrupt BCF image at index {bcf_idx}: {e}\")\n",
    "                        return [], -1\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error processing idx {idx}: {e}\")\n",
    "            \n",
    "            # If we got here, there was an issue with this index - try a different one\n",
    "            # Important: increment as an integer, not trying to add to a list\n",
    "            if retry < max_retries - 1:  # Only increment if we have retries left\n",
    "                idx = (int(idx) + 1) % len(self)\n",
    "        \n",
    "        # If all retries failed, return empty\n",
    "        return [], -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fd8b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.364191Z",
     "iopub.status.busy": "2025-05-25T17:07:28.363982Z",
     "iopub.status.idle": "2025-05-25T17:07:28.375949Z",
     "shell.execute_reply": "2025-05-25T17:07:28.375228Z"
    },
    "papermill": {
     "duration": 0.019217,
     "end_time": "2025-05-25T17:07:28.377003",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.357786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# memory_efficient_patch_collate_fn\n",
    "import gc\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "# Add this memory-efficient patch collate function\n",
    "def memory_efficient_patch_collate_fn(batch, patch_size_tuple):\n",
    "    \"\"\"\n",
    "    Memory-efficient version of patch_collate_fn that processes one patch at a time\n",
    "    and includes robust error handling.\n",
    "    \"\"\"\n",
    "    import gc  # Import inside function for worker processes\n",
    "    \n",
    "    all_patches = []\n",
    "    all_labels = []\n",
    "    valid_batch_items = 0\n",
    "\n",
    "    # Process one item at a time to avoid large memory allocations\n",
    "    for item in batch:\n",
    "        patches, label = item\n",
    "        # Ensure item is valid\n",
    "        if patches and label != -1:\n",
    "            # Process patches one by one\n",
    "            for patch in patches:\n",
    "                all_patches.append(patch)\n",
    "                all_labels.append(label)\n",
    "            valid_batch_items += 1\n",
    "    \n",
    "    # Periodically force garbage collection\n",
    "    if len(all_patches) > 100:\n",
    "        gc.collect()\n",
    "    \n",
    "    # Empty batch handling\n",
    "    if not all_patches:\n",
    "        patch_h, patch_w = patch_size_tuple\n",
    "        return torch.empty((0, 1, patch_h, patch_w), dtype=torch.float), torch.empty((0,), dtype=torch.long)\n",
    "\n",
    "    # Process in smaller chunks to reduce peak memory usage\n",
    "    max_chunk_size = 64  # Adjust based on your GPU memory\n",
    "    num_patches = len(all_patches)\n",
    "    patches_tensor_list = []\n",
    "    \n",
    "    for i in range(0, num_patches, max_chunk_size):\n",
    "        chunk = all_patches[i:i+max_chunk_size]\n",
    "        # Convert to NumPy array\n",
    "        chunk_np = np.stack(chunk)\n",
    "        # Convert to tensor, normalize and add channel dimension\n",
    "        chunk_tensor = torch.from_numpy(chunk_np).float() / 255.0\n",
    "        chunk_tensor = chunk_tensor.unsqueeze(1)\n",
    "        patches_tensor_list.append(chunk_tensor)\n",
    "        \n",
    "        # Clear variables to free memory\n",
    "        del chunk, chunk_np\n",
    "    \n",
    "    # Concatenate chunks\n",
    "    patches_tensor = torch.cat(patches_tensor_list, dim=0)\n",
    "    labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n",
    "    \n",
    "    # Clean up\n",
    "    del patches_tensor_list, all_patches, all_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    return patches_tensor, labels_tensor\n",
    "\n",
    "# Add this function to create optimized DataLoaders\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def create_optimized_dataloaders(dataset, batch_size=512, num_workers=2, val_split=0.1):\n",
    "    \"\"\"\n",
    "    Creates DataLoaders with proper error handling, avoiding HuggingFace datasets compatibility issues.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The image dataset instance\n",
    "        batch_size: Batch size for training\n",
    "        num_workers: Number of worker processes\n",
    "        val_split: Validation split ratio (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader)\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader, Subset\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    dataset_size = len(dataset)\n",
    "    indices = np.arange(dataset_size)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split_idx = int(np.floor(val_split * dataset_size))\n",
    "    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n",
    "    \n",
    "    # Create subset datasets - this avoids Hugging Face's __getitems__ implementation\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    # Custom collate function with error handling\n",
    "    def safe_collate(batch):\n",
    "        # Filter out empty or invalid items\n",
    "        valid_batch = [(patches, label) for patches, label in batch if patches and label != -1]\n",
    "        \n",
    "        if not valid_batch:\n",
    "            # Return empty tensors if no valid items\n",
    "            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n",
    "        \n",
    "        # Process valid items\n",
    "        all_patches = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for patches, label in valid_batch:\n",
    "            if isinstance(patches, list) and patches:\n",
    "                all_patches.extend(patches)\n",
    "                all_labels.extend([label] * len(patches))\n",
    "        \n",
    "        if not all_patches:\n",
    "            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n",
    "            \n",
    "        # Convert to PyTorch tensors\n",
    "        try:\n",
    "            patches_np = np.array(all_patches)\n",
    "            patches_tensor = torch.tensor(patches_np, dtype=torch.float) / 255.0\n",
    "            \n",
    "            # Add channel dimension if needed\n",
    "            if len(patches_tensor.shape) == 3:  # (B, H, W)\n",
    "                patches_tensor = patches_tensor.unsqueeze(1)  # -> (B, 1, H, W)\n",
    "                \n",
    "            labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n",
    "            return patches_tensor, labels_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error in collate function: {e}\")\n",
    "            return torch.empty((0, 1, 105, 105)), torch.empty((0,), dtype=torch.long)\n",
    "    \n",
    "    # Create DataLoaders with minimal worker configuration for stability\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=safe_collate,\n",
    "        pin_memory=False,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=safe_collate,\n",
    "        pin_memory=False,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0021b4da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.388745Z",
     "iopub.status.busy": "2025-05-25T17:07:28.388560Z",
     "iopub.status.idle": "2025-05-25T17:07:28.395243Z",
     "shell.execute_reply": "2025-05-25T17:07:28.394698Z"
    },
    "papermill": {
     "duration": 0.013653,
     "end_time": "2025-05-25T17:07:28.396184",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.382531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collate functions\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (patches_list, label)\n",
    "      - patches_list: list of P numpy arrays of shape (H, W) or (H, W, C)\n",
    "      - label: int\n",
    "    Returns:\n",
    "      - images: Tensor of shape (B, P, C, H, W)\n",
    "      - labels: Tensor of shape (B,)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for patches_list, label in batch:\n",
    "        patch_tensors = []\n",
    "        for patch in patches_list:\n",
    "            # patch is a numpy array\n",
    "            arr = patch\n",
    "            # grayscale or RGB?\n",
    "            if arr.ndim == 2:\n",
    "                # (H, W) → (1, H, W)\n",
    "                t = torch.from_numpy(arr).unsqueeze(0)\n",
    "            else:\n",
    "                # (H, W, C) → (C, H, W)\n",
    "                t = torch.from_numpy(arr).permute(2, 0, 1)\n",
    "            # normalize to [0,1] float\n",
    "            t = t.float().div(255.0)\n",
    "            patch_tensors.append(t)\n",
    "        # stack P patches → (P, C, H, W)\n",
    "        images.append(torch.stack(patch_tensors, dim=0))\n",
    "        labels.append(label)\n",
    "    # now batch them → (B, P, C, H, W) and (B,)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return images, labels\n",
    "    \n",
    "def train_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (patches_list, label) where\n",
    "      patches_list: list of P numpy arrays, each H×W or H×W×C\n",
    "      label: int\n",
    "    Returns:\n",
    "      images: Tensor of shape (B*P, C, H, W)\n",
    "      labels: Tensor of shape (B*P,)\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    lbls = []\n",
    "    for patches, label in batch:\n",
    "        for patch in patches:\n",
    "            arr = patch\n",
    "            # to torch tensor: (H,W)→(1,H,W), (H,W,C)→(C,H,W)\n",
    "            if arr.ndim == 2:\n",
    "                t = torch.from_numpy(arr).unsqueeze(0)\n",
    "            else:\n",
    "                t = torch.from_numpy(arr).permute(2, 0, 1)\n",
    "            # normalize to [0,1]\n",
    "            imgs.append(t.float().div(255.0))\n",
    "            lbls.append(label)\n",
    "    images = torch.stack(imgs, dim=0)\n",
    "    labels = torch.tensor(lbls, dtype=torch.long)\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13be1a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.407826Z",
     "iopub.status.busy": "2025-05-25T17:07:28.407653Z",
     "iopub.status.idle": "2025-05-25T17:07:28.411628Z",
     "shell.execute_reply": "2025-05-25T17:07:28.411118Z"
    },
    "papermill": {
     "duration": 0.010985,
     "end_time": "2025-05-25T17:07:28.412760",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.401775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save and load dataset\n",
    "import pickle\n",
    "\n",
    "def save_dataset(dataset, filepath: str):\n",
    "    \"\"\"\n",
    "    Serialize and save a CombinedImageDataset to disk.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    print(f\"Dataset saved to {filepath!r}\")\n",
    "\n",
    "def load_dataset(filepath: str):\n",
    "    \"\"\"\n",
    "    Load a pickled CombinedImageDataset from disk.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    print(f\"Dataset loaded from {filepath!r}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6f89c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.424369Z",
     "iopub.status.busy": "2025-05-25T17:07:28.424081Z",
     "iopub.status.idle": "2025-05-25T17:07:28.427129Z",
     "shell.execute_reply": "2025-05-25T17:07:28.426603Z"
    },
    "papermill": {
     "duration": 0.009913,
     "end_time": "2025-05-25T17:07:28.428091",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.418178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# synthetic_dataset = load_dataset(\"/kaggle/input/font-datasets/synthetic_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2ba78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.439864Z",
     "iopub.status.busy": "2025-05-25T17:07:28.439679Z",
     "iopub.status.idle": "2025-05-25T17:07:28.444598Z",
     "shell.execute_reply": "2025-05-25T17:07:28.444023Z"
    },
    "papermill": {
     "duration": 0.012122,
     "end_time": "2025-05-25T17:07:28.445669",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.433547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualization some samples from the combined dataset \n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from PIL import Image, ImageFile\n",
    "# from io import BytesIO\n",
    "# import os\n",
    "\n",
    "# def visualize_simple_images_and_patches(dataset, num_images=2, seed=None):\n",
    "#     # Allow loading of truncated images\n",
    "#     ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    \n",
    "#     # Set random seed if provided\n",
    "#     if seed is not None:\n",
    "#         random.seed(seed)\n",
    "    \n",
    "#     # Find valid images (with patches)\n",
    "#     valid_indices = []\n",
    "#     attempts = 0\n",
    "#     max_attempts = min(len(dataset) * 2, 100)  # Limit search attempts\n",
    "    \n",
    "#     while len(valid_indices) < num_images and attempts < max_attempts:\n",
    "#         idx = random.randint(0, len(dataset) - 1)\n",
    "#         if idx not in valid_indices:  # Avoid duplicates\n",
    "#             try:\n",
    "#                 patches, label = dataset[idx]\n",
    "#                 if patches and len(patches) > 0:\n",
    "#                     valid_indices.append(idx)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error loading index {idx}: {e}\")\n",
    "#             attempts += 1\n",
    "    \n",
    "#     # If we couldn't find enough valid images\n",
    "#     if len(valid_indices) < num_images:\n",
    "#         print(f\"Warning: Could only find {len(valid_indices)} valid images with patches\")\n",
    "#         if len(valid_indices) == 0:\n",
    "#             print(\"No valid images found. Check your dataset.\")\n",
    "#             return\n",
    "    \n",
    "#     # Create figure with enough space for all elements\n",
    "#     fig, axes = plt.subplots(len(valid_indices), 4, figsize=(16, 5 * len(valid_indices)))\n",
    "    \n",
    "#     # If only one image is requested, make axes indexable as 2D\n",
    "#     if len(valid_indices) == 1:\n",
    "#         axes = axes.reshape(1, -1)\n",
    "    \n",
    "#     for i, idx in enumerate(valid_indices):\n",
    "#         try:\n",
    "#             # Get item directly from dataset\n",
    "#             patches, label = dataset[idx]\n",
    "            \n",
    "#             # Get the original full image\n",
    "#             img_array = None\n",
    "            \n",
    "#             if hasattr(dataset, 'jpeg_data') and idx < len(dataset.jpeg_data):\n",
    "#                 # From jpeg_data\n",
    "#                 img_path, _ = dataset.jpeg_data[idx]\n",
    "#                 img = Image.open(img_path).convert('L')\n",
    "#                 img_array = np.array(img)\n",
    "#                 source = f\"JPEG file: {os.path.basename(img_path)}\"\n",
    "                \n",
    "#             elif hasattr(dataset, 'image_filenames') and not hasattr(dataset, 'num_images'):\n",
    "#                 # From BCFImagePatchDataset with JPEG source\n",
    "#                 img_path = os.path.join(dataset.data_source, dataset.image_filenames[idx])\n",
    "#                 img = Image.open(img_path).convert('L')\n",
    "#                 img_array = np.array(img)\n",
    "#                 source = f\"JPEG file: {dataset.image_filenames[idx]}\"\n",
    "                \n",
    "#             else:\n",
    "#                 # From BCF file (either CombinedImageDataset or BCFImagePatchDataset)\n",
    "#                 if hasattr(dataset, 'bcf_data'):\n",
    "#                     # CombinedImageDataset\n",
    "#                     bcf_idx = idx - len(dataset.jpeg_data)\n",
    "#                     if bcf_idx < 0 or bcf_idx >= len(dataset.bcf_data):\n",
    "#                         print(f\"Invalid BCF index: {bcf_idx}\")\n",
    "#                         continue\n",
    "                        \n",
    "#                     offset = dataset.image_offsets[bcf_idx]\n",
    "#                     size = dataset.image_sizes[bcf_idx]\n",
    "#                     data_file = dataset.bcf_file\n",
    "#                     data_start = dataset.data_start_offset\n",
    "#                     source = f\"BCF file (idx: {bcf_idx})\"\n",
    "#                 else:\n",
    "#                     # BCFImagePatchDataset\n",
    "#                     offset = dataset.image_offsets[idx]\n",
    "#                     size = dataset.image_sizes[idx]\n",
    "#                     data_file = dataset.data_source\n",
    "#                     data_start = dataset.data_start_offset\n",
    "#                     source = f\"BCF file (idx: {idx})\"\n",
    "                \n",
    "#                 with open(data_file, 'rb') as f:\n",
    "#                     f.seek(data_start + offset)\n",
    "#                     image_bytes = f.read(size)\n",
    "#                 img = Image.open(BytesIO(image_bytes)).convert('L')\n",
    "#                 img_array = np.array(img)\n",
    "            \n",
    "#             # Plot original image if we successfully loaded it\n",
    "#             if img_array is not None:\n",
    "#                 axes[i, 0].imshow(img_array, cmap='gray')\n",
    "#                 axes[i, 0].set_title(f\"Original Image\\nLabel: {label}\\nSource: {source}\")\n",
    "#                 axes[i, 0].axis('off')\n",
    "#             else:\n",
    "#                 axes[i, 0].text(0.5, 0.5, \"Image loading failed\", ha='center', va='center')\n",
    "#                 axes[i, 0].axis('off')\n",
    "            \n",
    "#             # Plot the patches - ensure we have patches to display\n",
    "#             if patches and len(patches) > 0:\n",
    "#                 for j in range(3):\n",
    "#                     if j < len(patches):\n",
    "#                         patch = patches[j]\n",
    "#                         axes[i, j+1].imshow(patch, cmap='gray')\n",
    "#                         axes[i, j+1].set_title(f\"Patch {j+1}\\nShape: {patch.shape}\")\n",
    "#                     else:\n",
    "#                         # No more patches to display\n",
    "#                         axes[i, j+1].text(0.5, 0.5, \"No patch\", ha='center', va='center')\n",
    "#                     axes[i, j+1].axis('off')\n",
    "#             else:\n",
    "#                 # No patches for this image\n",
    "#                 for j in range(3):\n",
    "#                     axes[i, j+1].text(0.5, 0.5, \"No patches extracted\", ha='center', va='center')\n",
    "#                     axes[i, j+1].axis('off')\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing index {idx}: {e}\")\n",
    "#             # Create error message in subplot\n",
    "#             for j in range(4):\n",
    "#                 axes[i, j].text(0.5, 0.5, f\"Error: {str(e)[:50]}...\", ha='center', va='center')\n",
    "#                 axes[i, j].axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Return the indices we used (helpful for debugging)\n",
    "#     return valid_indices\n",
    "\n",
    "# # Example usage:\n",
    "# visualize_simple_images_and_patches(synthetic_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5985a39f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.457458Z",
     "iopub.status.busy": "2025-05-25T17:07:28.457233Z",
     "iopub.status.idle": "2025-05-25T17:07:28.459882Z",
     "shell.execute_reply": "2025-05-25T17:07:28.459422Z"
    },
    "papermill": {
     "duration": 0.00964,
     "end_time": "2025-05-25T17:07:28.460881",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.451241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(synthetic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f07284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.472684Z",
     "iopub.status.busy": "2025-05-25T17:07:28.472480Z",
     "iopub.status.idle": "2025-05-25T17:07:28.798318Z",
     "shell.execute_reply": "2025-05-25T17:07:28.797430Z"
    },
    "papermill": {
     "duration": 0.333237,
     "end_time": "2025-05-25T17:07:28.799638",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.466401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: JPEG directory jpeg_dir does not exist.\n",
      "Loaded 200000 labels from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label.\n",
      "Loaded 200000 images from /kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf.\n",
      "Loaded 200000 .bcf images.\n",
      "Warning: JPEG directory jpeg_dir does not exist.\n",
      "Loaded 3202 labels from /kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label.\n",
      "Loaded 3202 images from /kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf.\n",
      "Loaded 3202 .bcf images.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Clean memory before starting\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "jpeg_dir = \"/kaggle/input/deepfont-unlab/scrape-wtf-new/scrape-wtf-new\"\n",
    "bcf_syn = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.bcf\"\n",
    "label_syn = \"/kaggle/input/deepfont-unlab/syn_train/VFR_syn_train_extracted.label\"\n",
    "\n",
    "bcf_test = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.bcf\"\n",
    "label_test = \"/kaggle/input/deepfont-unlab/real_test/VFR_real_test_extracted.label\"\n",
    "# Create dataset with smaller patch size and fewer patches per image\n",
    "synthetic_dataset = ImageDataset(\n",
    "    jpeg_dir=\"jpeg_dir\",\n",
    "    bcf_file=bcf_syn,\n",
    "    label_file=label_syn,\n",
    "    # testing=True,\n",
    "    num_patch=3,  # Number of patches per image\n",
    ")\n",
    "\n",
    "test_dataset = ImageDataset(\n",
    "    jpeg_dir=\"jpeg_dir\",\n",
    "    bcf_file= bcf_test,\n",
    "    label_file=label_test,\n",
    "    # testing=True,\n",
    "    num_patch=3,  # Number of patches per image\n",
    ")\n",
    "\n",
    "# save_dataset(test_dataset, \"/kaggle/working/test_datasets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f4dc00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.811922Z",
     "iopub.status.busy": "2025-05-25T17:07:28.811716Z",
     "iopub.status.idle": "2025-05-25T17:07:28.815050Z",
     "shell.execute_reply": "2025-05-25T17:07:28.814343Z"
    },
    "papermill": {
     "duration": 0.0106,
     "end_time": "2025-05-25T17:07:28.816186",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.805586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dataset(synthetic_dataset, \"/kaggle/working/synthetic_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38660a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.827996Z",
     "iopub.status.busy": "2025-05-25T17:07:28.827786Z",
     "iopub.status.idle": "2025-05-25T17:07:28.865889Z",
     "shell.execute_reply": "2025-05-25T17:07:28.865265Z"
    },
    "papermill": {
     "duration": 0.045119,
     "end_time": "2025-05-25T17:07:28.866920",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.821801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 180000,  Eval samples: 20000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# combined_dataset = load_dataset(\"/kaggle/working/combined_dataset.pkl\")\n",
    "\n",
    "dataset = synthetic_dataset\n",
    "# 1. Decide split sizes\n",
    "total = len(dataset)\n",
    "train_size = int(0.9 * total)\n",
    "val_size   = total - train_size\n",
    "\n",
    "# 2. Split with a fixed seed for reproducibility\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)},  Eval samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb124c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.879243Z",
     "iopub.status.busy": "2025-05-25T17:07:28.879039Z",
     "iopub.status.idle": "2025-05-25T17:07:28.883472Z",
     "shell.execute_reply": "2025-05-25T17:07:28.882877Z"
    },
    "papermill": {
     "duration": 0.011845,
     "end_time": "2025-05-25T17:07:28.884486",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.872641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_collate_fn # test_collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_collate_fn # test_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ac7aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.896535Z",
     "iopub.status.busy": "2025-05-25T17:07:28.896295Z",
     "iopub.status.idle": "2025-05-25T17:07:28.899809Z",
     "shell.execute_reply": "2025-05-25T17:07:28.899121Z"
    },
    "papermill": {
     "duration": 0.010754,
     "end_time": "2025-05-25T17:07:28.900821",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.890067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import torchvision.transforms.functional as TF\n",
    "\n",
    "# def show_batch(images, labels):\n",
    "#     \"\"\"\n",
    "#     Show all images in the batch returned by test_collate_fn.\n",
    "    \n",
    "#     images: Tensor of shape (B, P, C, H, W)\n",
    "#     labels: Tensor of shape (B,)\n",
    "#     \"\"\"\n",
    "#     B, P, C, H, W = images.shape\n",
    "#     for b in range(B):\n",
    "#         label = labels[b].item()\n",
    "#         print(f\"Image {b} - Label: {label}\")\n",
    "#         plt.figure(figsize=(P * 2, 2))  # width = P*2 inches, height = 2 inches\n",
    "#         for p in range(P):\n",
    "#             img = images[b, p]  # shape: (C, H, W)\n",
    "#             plt.subplot(1, P, p + 1)\n",
    "#             if C == 1:\n",
    "#                 plt.imshow(img.squeeze(0), cmap='gray')\n",
    "#             else:\n",
    "#                 plt.imshow(TF.to_pil_image(img))\n",
    "#             plt.axis('off')\n",
    "#             plt.title(f\"Patch {p+1}\")\n",
    "#         plt.show()\n",
    "\n",
    "# images, labels = next(iter(test_loader))\n",
    "# show_batch(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c964ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.912855Z",
     "iopub.status.busy": "2025-05-25T17:07:28.912661Z",
     "iopub.status.idle": "2025-05-25T17:07:28.915566Z",
     "shell.execute_reply": "2025-05-25T17:07:28.915024Z"
    },
    "papermill": {
     "duration": 0.010143,
     "end_time": "2025-05-25T17:07:28.916624",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.906481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images, labels = next(iter(val_loader))\n",
    "# show_batch(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "141d33f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.928542Z",
     "iopub.status.busy": "2025-05-25T17:07:28.928294Z",
     "iopub.status.idle": "2025-05-25T17:07:28.931441Z",
     "shell.execute_reply": "2025-05-25T17:07:28.930743Z"
    },
    "papermill": {
     "duration": 0.010383,
     "end_time": "2025-05-25T17:07:28.932529",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.922146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # synthetic_dataset = load_dataset(\"/kaggle/input/font-datasets/synthetic_dataset.pkl\")\n",
    "# filtered = test_dataset\n",
    "# unique_labels = np.unique(filtered.labels)\n",
    "\n",
    "# print(f\"Number of unique labels: {unique_labels.size}\")\n",
    "# print(f\"Labels: {unique_labels}\")\n",
    "\n",
    "# del filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a3287",
   "metadata": {
    "papermill": {
     "duration": 0.005237,
     "end_time": "2025-05-25T17:07:28.943135",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.937898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SCAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a9a1080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.954977Z",
     "iopub.status.busy": "2025-05-25T17:07:28.954781Z",
     "iopub.status.idle": "2025-05-25T17:07:28.968839Z",
     "shell.execute_reply": "2025-05-25T17:07:28.968337Z"
    },
    "papermill": {
     "duration": 0.021236,
     "end_time": "2025-05-25T17:07:28.969817",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.948581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_memory_efficient_model\n",
    "\n",
    "import torch.cuda.amp as amp\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_memory_efficient_model(model, train_loader, val_loader=None, \n",
    "                                num_epochs=5, learning_rate=0.0001,\n",
    "                                checkpoint_dir=\"/kaggle/working/\"):\n",
    "    \"\"\"\n",
    "    Memory-efficient training function for SCAE model.\n",
    "    \"\"\"\n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup device and optimization tools\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device} with {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "    print(f\"Memory reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set up mixed precision training\n",
    "    scaler = amp.GradScaler()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    # Track best model\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # Clean memory before each epoch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # TRAINING PHASE\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            valid_batches = 0\n",
    "            \n",
    "            # Use tqdm for progress tracking\n",
    "            pbar = tqdm(train_loader)\n",
    "            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "            \n",
    "            for batch_idx, (patches, _) in enumerate(pbar):\n",
    "                # Skip empty batches\n",
    "                if patches.numel() == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Move data to device\n",
    "                patches = patches.to(device, non_blocking=True)\n",
    "                \n",
    "                # Mixed precision forward pass\n",
    "                with amp.autocast():\n",
    "                    outputs = model(patches)\n",
    "                    loss = criterion(outputs, patches)\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                # Update metrics\n",
    "                running_loss += loss.item()\n",
    "                valid_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "                \n",
    "                # Aggressive memory cleanup every few batches\n",
    "                if batch_idx % 10 == 0:\n",
    "                    del outputs, loss, patches\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            if valid_batches > 0:\n",
    "                train_loss = running_loss / valid_batches\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, No valid batches!\")\n",
    "                continue\n",
    "                \n",
    "            # Save checkpoint every epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, f\"{checkpoint_dir}/model_epoch_{epoch+1}.pt\")\n",
    "            \n",
    "            # VALIDATION PHASE\n",
    "            if val_loader:\n",
    "                val_loss = validate_memory_efficient(model, val_loader, criterion, device)\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                # Early stopping logic\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pt\")\n",
    "                    print(f\"New best model saved with val_loss: {val_loss:.6f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= 3:  # Adjust patience as needed\n",
    "                        print(\"Early stopping triggered!\")\n",
    "                        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        # Save emergency checkpoint\n",
    "        torch.save(model.state_dict(), f\"{checkpoint_dir}/emergency_model.pt\")\n",
    "        raise\n",
    "        \n",
    "    return model\n",
    "\n",
    "def validate_memory_efficient(model, val_loader, criterion, device):\n",
    "    \"\"\"Memory-efficient validation function.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader)\n",
    "        pbar.set_description(f\"Validating\")\n",
    "        \n",
    "        for patches, _ in pbar:\n",
    "            if patches.numel() == 0:\n",
    "                continue\n",
    "                \n",
    "            patches = patches.to(device, non_blocking=True)\n",
    "            \n",
    "            # Using mixed precision even for validation\n",
    "            with amp.autocast():\n",
    "                outputs = model(patches)\n",
    "                loss = criterion(outputs, patches)\n",
    "                \n",
    "            running_loss += loss.item()\n",
    "            valid_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "            # Clean up\n",
    "            del outputs, patches, loss\n",
    "    \n",
    "    if valid_batches > 0:\n",
    "        val_loss = running_loss / valid_batches\n",
    "        print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "        return val_loss\n",
    "    else:\n",
    "        print(\"No valid validation batches!\")\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0919be24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:28.981793Z",
     "iopub.status.busy": "2025-05-25T17:07:28.981589Z",
     "iopub.status.idle": "2025-05-25T17:07:28.990164Z",
     "shell.execute_reply": "2025-05-25T17:07:28.989638Z"
    },
    "papermill": {
     "duration": 0.015896,
     "end_time": "2025-05-25T17:07:28.991257",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.975361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SCAE\n",
    "import torch.nn as nn\n",
    "class SCAE(nn.Module):\n",
    "    def __init__(self, normalization_type=\"batch_norm\", use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n",
    "        super(SCAE, self).__init__()\n",
    "\n",
    "        def norm_layer(num_features):\n",
    "            if normalization_type == \"batch_norm\":\n",
    "                return nn.BatchNorm2d(num_features)\n",
    "            elif normalization_type == \"group_norm\":\n",
    "                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n",
    "            elif normalization_type == \"layer_norm\":\n",
    "                return nn.LayerNorm([num_features, 12, 12])  # Updated for 12x12 feature maps\n",
    "            else:\n",
    "                return nn.Identity()\n",
    "\n",
    "        def activation_layer():\n",
    "            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n",
    "\n",
    "        def dropout_layer():\n",
    "            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n",
    "\n",
    "        # Encoder: Input 105x105 -> Output 12x12\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Layer 1: 105x105 -> 48x48\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=2, padding=0),\n",
    "            norm_layer(64),\n",
    "            activation_layer(),\n",
    "            dropout_layer(),\n",
    "            \n",
    "            # Layer 2: 48x48 -> 24x24\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Layer 3: 24x24 -> 24x24\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            norm_layer(128),\n",
    "            activation_layer(),\n",
    "            dropout_layer(),\n",
    "            \n",
    "            # Layer 4: 24x24 -> 12x12 (added to get 12x12 output)\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Decoder: Input 12x12 -> Output 105x105\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Layer 1: 12x12 -> 24x24\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            norm_layer(64),\n",
    "            activation_layer(),\n",
    "            dropout_layer(),\n",
    "            \n",
    "            # Layer 2: 24x24 -> 48x48\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            norm_layer(32),\n",
    "            activation_layer(),\n",
    "            dropout_layer(),\n",
    "            \n",
    "            # Layer 3: 48x48 -> 105x105 (with precise output size)\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=14, stride=2, padding=2, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through encoder\n",
    "        if x.size(1) == 3:\n",
    "            # Use standard RGB to grayscale conversion: 0.299*R + 0.587*G + 0.114*B\n",
    "            x = 0.299 * x[:, 0:1] + 0.587 * x[:, 1:2] + 0.114 * x[:, 2:3]\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            # print(x.shape)\n",
    "\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "            # print(x.shape)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5572d9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:29.003007Z",
     "iopub.status.busy": "2025-05-25T17:07:29.002835Z",
     "iopub.status.idle": "2025-05-25T17:07:29.005856Z",
     "shell.execute_reply": "2025-05-25T17:07:29.005297Z"
    },
    "papermill": {
     "duration": 0.010048,
     "end_time": "2025-05-25T17:07:29.006834",
     "exception": false,
     "start_time": "2025-05-25T17:07:28.996786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create model and train with memory optimization\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = SCAE().to(device)\n",
    "# trained_model = train_memory_efficient_model(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     num_epochs=5,\n",
    "#     learning_rate=0.0001\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "958419ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:29.018784Z",
     "iopub.status.busy": "2025-05-25T17:07:29.018588Z",
     "iopub.status.idle": "2025-05-25T17:07:29.313813Z",
     "shell.execute_reply": "2025-05-25T17:07:29.313200Z"
    },
    "papermill": {
     "duration": 0.302443,
     "end_time": "2025-05-25T17:07:29.314916",
     "exception": false,
     "start_time": "2025-05-25T17:07:29.012473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load checkpoints\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) Re-instantiate your model\n",
    "pretrained_scae = SCAE().to(device)\n",
    "\n",
    "# 2) Load the checkpoint dict\n",
    "ckpt = torch.load(\"/kaggle/input/font_models/pytorch/default/1/model_epoch_5.pt\", weights_only=True)\n",
    "\n",
    "# 3) Pull out and load the actual weights\n",
    "pretrained_scae.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "# 4) (Optional) if you saved epoch or optimizer state too:\n",
    "# start_epoch = ckpt[\"epoch\"] + 1\n",
    "# optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab05d88b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:29.327674Z",
     "iopub.status.busy": "2025-05-25T17:07:29.327447Z",
     "iopub.status.idle": "2025-05-25T17:07:29.330491Z",
     "shell.execute_reply": "2025-05-25T17:07:29.329905Z"
    },
    "papermill": {
     "duration": 0.010652,
     "end_time": "2025-05-25T17:07:29.331511",
     "exception": false,
     "start_time": "2025-05-25T17:07:29.320859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate_reconstruction(model, val_loader, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2fc37fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:29.343588Z",
     "iopub.status.busy": "2025-05-25T17:07:29.343357Z",
     "iopub.status.idle": "2025-05-25T17:07:29.346400Z",
     "shell.execute_reply": "2025-05-25T17:07:29.345823Z"
    },
    "papermill": {
     "duration": 0.010269,
     "end_time": "2025-05-25T17:07:29.347486",
     "exception": false,
     "start_time": "2025-05-25T17:07:29.337217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize_latent_space(model, val_loader, device, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34c9c3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:29.360039Z",
     "iopub.status.busy": "2025-05-25T17:07:29.359863Z",
     "iopub.status.idle": "2025-05-25T17:07:29.365447Z",
     "shell.execute_reply": "2025-05-25T17:07:29.364760Z"
    },
    "papermill": {
     "duration": 0.013209,
     "end_time": "2025-05-25T17:07:29.366554",
     "exception": false,
     "start_time": "2025-05-25T17:07:29.353345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # evaluation\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torchvision.utils import make_grid\n",
    "# from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n",
    "\n",
    "# # ——————————————————————————————————————————————\n",
    "# # 1) Reconstruction evaluation\n",
    "# # ——————————————————————————————————————————————\n",
    "\n",
    "# def evaluate_reconstruction(model, dataloader, device, num_samples=10, save_path=None):\n",
    "#     \"\"\"\n",
    "#     Compute MSE, SSIM, PSNR between inputs and auto‐encoder reconstructions.\n",
    "#     Also visualize num_samples side‐by‐side.\n",
    "#     Expects dataloader that yields (imgs, _), where imgs is (B,1,H,W) in [0,1].\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     total_mse = total_ssim = total_psnr = 0.0\n",
    "#     seen = 0\n",
    "#     vis_in, vis_re = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for imgs, _ in dataloader:\n",
    "#             imgs = imgs.to(device)                  # (B,1,105,105)\n",
    "#             recons = model(imgs)                    # (B,1,105,105)\n",
    "\n",
    "#             # batch MSE\n",
    "#             mse_batch = torch.mean((recons - imgs) ** 2).item()\n",
    "#             total_mse += mse_batch * imgs.size(0)\n",
    "\n",
    "#             # to numpy [0,1]\n",
    "#             inp_np  = imgs.cpu().squeeze(1).numpy()\n",
    "#             rec_np  = recons.cpu().squeeze(1).numpy()\n",
    "#             B = inp_np.shape[0]\n",
    "\n",
    "#             for i in range(B):\n",
    "#                 if seen >= num_samples: break\n",
    "\n",
    "#                 im = inp_np[i]\n",
    "#                 rc = rec_np[i]\n",
    "#                 total_ssim += ssim(im, rc, data_range=1.0)\n",
    "#                 total_psnr += psnr(im, rc, data_range=1.0)\n",
    "\n",
    "#                 vis_in .append(imgs [i].cpu())\n",
    "#                 vis_re .append(recons[i].cpu())\n",
    "#                 seen += 1\n",
    "\n",
    "#             if seen >= num_samples:\n",
    "#                 break\n",
    "\n",
    "#     N = len(dataloader.dataset) if hasattr(dataloader.dataset, \"__len__\") else seen\n",
    "#     avg_mse  = total_mse / N\n",
    "#     avg_ssim = total_ssim / seen\n",
    "#     avg_psnr = total_psnr / seen\n",
    "\n",
    "#     print(f\"Reconstruction → MSE: {avg_mse:.4f}, SSIM: {avg_ssim:.4f}, PSNR: {avg_psnr:.2f} dB\")\n",
    "\n",
    "#     if vis_in:\n",
    "#         # interleave originals and reconstructions\n",
    "#         grid = make_grid([*vis_in[:seen], *vis_re[:seen]],\n",
    "#                          nrow= seen,\n",
    "#                          normalize=True, pad_value=1)\n",
    "#         plt.figure(figsize=(seen*2, 4))\n",
    "#         plt.imshow(grid.permute(1,2,0).numpy())\n",
    "#         plt.axis('off')\n",
    "#         plt.title(\"Originals (top) vs Reconstructions (bottom)\")\n",
    "#         if save_path:\n",
    "#             plt.savefig(f\"{save_path}/recon.png\", bbox_inches=\"tight\")\n",
    "#         plt.show()\n",
    "\n",
    "#     return {\"mse\":avg_mse, \"ssim\":avg_ssim, \"psnr\":avg_psnr}\n",
    "\n",
    "\n",
    "# # ——————————————————————————————————————————————\n",
    "# # 2) Latent extraction & generation\n",
    "# # ——————————————————————————————————————————————\n",
    "\n",
    "# def extract_latent_features(model, x):\n",
    "#     \"\"\"\n",
    "#     Runs x through the encoder only.\n",
    "#     x: (B,1,105,105)\n",
    "#     returns: z (B,128,12,12)\n",
    "#     \"\"\"\n",
    "#     return model.encoder(x)\n",
    "\n",
    "# def generate_from_latent(model, z):\n",
    "#     \"\"\"\n",
    "#     Runs z through the decoder only.\n",
    "#     z: (B,128,12,12)\n",
    "#     returns: recon (B,1,105,105)\n",
    "#     \"\"\"\n",
    "#     return model.decoder(z)\n",
    "\n",
    "\n",
    "# # ——————————————————————————————————————————————\n",
    "# # 3) t-SNE visualization of latent space\n",
    "# # ——————————————————————————————————————————————\n",
    "\n",
    "# def visualize_latent_space(model, dataloader, device, max_samples=500, save_path=None):\n",
    "#     \"\"\"\n",
    "#     Collects up to max_samples latent vectors, runs t-SNE, and plots.\n",
    "#     Expects dataloader yielding (imgs, labels) with imgs in [0,1].\n",
    "#     \"\"\"\n",
    "#     from sklearn.manifold import TSNE\n",
    "\n",
    "#     model.eval()\n",
    "#     zs, ys = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for imgs, labels in dataloader:\n",
    "#             if len(zs) >= max_samples: break\n",
    "#             imgs = imgs.to(device)\n",
    "#             z = extract_latent_features(model, imgs)         # (B,128,12,12)\n",
    "#             zflat = z.view(z.size(0), -1).cpu().numpy()      # (B, 128*12*12)\n",
    "#             zs.append(zflat)\n",
    "#             ys.append(labels.numpy())\n",
    "#         zs = np.vstack(zs)[:max_samples]\n",
    "#         ys = np.concatenate(ys)[:max_samples]\n",
    "\n",
    "#     tsne = TSNE(n_components=2, random_state=42)\n",
    "#     Z2 = tsne.fit_transform(zs)\n",
    "\n",
    "#     plt.figure(figsize=(8,6))\n",
    "#     plt.scatter(Z2[:,0], Z2[:,1], c=ys, cmap=\"tab10\", s=5, alpha=0.7)\n",
    "#     plt.colorbar(label=\"Class\")\n",
    "#     plt.title(\"t-SNE of SCAE Latent Space\")\n",
    "#     if save_path:\n",
    "#         plt.savefig(f\"{save_path}/tsne.png\", bbox_inches=\"tight\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return Z2, ys\n",
    "\n",
    "\n",
    "# # ——————————————————————————————————————————————\n",
    "# # 4) Latent interpolation\n",
    "# # ——————————————————————————————————————————————\n",
    "\n",
    "# def interpolate_latent_space(model, dataloader, device, steps=10, save_path=None):\n",
    "#     \"\"\"\n",
    "#     Linearly interpolate between two latent codes and decode them.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     imgs = None\n",
    "#     # grab first two distinct samples\n",
    "#     with torch.no_grad():\n",
    "#         for x, _ in dataloader:\n",
    "#             if x.size(0) >= 2:\n",
    "#                 imgs = x[:2].to(device)\n",
    "#                 break\n",
    "#     if imgs is None:\n",
    "#         print(\"Not enough samples for interpolation\"); return\n",
    "\n",
    "#     z1 = extract_latent_features(model, imgs[0:1])   # (1,128,12,12)\n",
    "#     z2 = extract_latent_features(model, imgs[1:2])\n",
    "\n",
    "#     interps = []\n",
    "#     alphas = np.linspace(0,1,steps)\n",
    "#     for a in alphas:\n",
    "#         z = a*z1 + (1-a)*z2\n",
    "#         recon = generate_from_latent(model, z)        # (1,1,105,105)\n",
    "#         interps.append(recon.cpu())\n",
    "\n",
    "#     # build grid: [orig1, *interps, orig2]\n",
    "#     all_ = torch.cat([imgs[0:1].cpu(), *interps, imgs[1:2].cpu()], dim=0)\n",
    "#     grid = make_grid(all_, nrow=steps+2, normalize=True, pad_value=1)\n",
    "\n",
    "#     plt.figure(figsize=(12,3))\n",
    "#     plt.imshow(grid.permute(1,2,0).numpy())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(\"Latent Interpolation\")\n",
    "#     if save_path:\n",
    "#         plt.savefig(f\"{save_path}/interp.png\", bbox_inches=\"tight\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return interps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ab5d16d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:29.379935Z",
     "iopub.status.busy": "2025-05-25T17:07:29.379457Z",
     "iopub.status.idle": "2025-05-25T17:07:29.383078Z",
     "shell.execute_reply": "2025-05-25T17:07:29.382361Z"
    },
    "papermill": {
     "duration": 0.011977,
     "end_time": "2025-05-25T17:07:29.384161",
     "exception": false,
     "start_time": "2025-05-25T17:07:29.372184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !zip -r /kaggle/working/evaluation_results.zip /kaggle/working/evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da085b",
   "metadata": {
    "id": "1hbTCU2qndht",
    "papermill": {
     "duration": 0.005943,
     "end_time": "2025-05-25T17:07:29.399838",
     "exception": false,
     "start_time": "2025-05-25T17:07:29.393895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Font classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d36ecd47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:29.413662Z",
     "iopub.status.busy": "2025-05-25T17:07:29.413375Z",
     "iopub.status.idle": "2025-05-25T17:07:29.423549Z",
     "shell.execute_reply": "2025-05-25T17:07:29.422850Z"
    },
    "papermill": {
     "duration": 0.018976,
     "end_time": "2025-05-25T17:07:29.424699",
     "exception": false,
     "start_time": "2025-05-25T17:07:29.405723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FontClassifier\n",
    "class FontClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_scae, num_classes=200, normalization_type=\"batch_norm\", \n",
    "                 use_dropout=False, dropout_prob=0.3, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.pretrained_scae = pretrained_scae  # Use pretrained SCAE encoder\n",
    "        self.pretrained_scae.eval()\n",
    "        for param in self.pretrained_scae.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Define helper functions for creating layers\n",
    "        def norm_layer(num_features, spatial_size=None):\n",
    "            if normalization_type == \"batch_norm\":\n",
    "                return nn.BatchNorm2d(num_features)\n",
    "            elif normalization_type == \"group_norm\":\n",
    "                return nn.GroupNorm(num_groups=8, num_channels=num_features)\n",
    "            elif normalization_type == \"layer_norm\" and spatial_size is not None:\n",
    "                return nn.LayerNorm([num_features, spatial_size, spatial_size])\n",
    "            else:\n",
    "                return nn.Identity()\n",
    "\n",
    "        def activation_layer():\n",
    "            return nn.LeakyReLU(inplace=True) if activation == \"leaky_relu\" else nn.ReLU(inplace=True)\n",
    "\n",
    "        def dropout_layer():\n",
    "            return nn.Dropout2d(dropout_prob) if use_dropout else nn.Identity()\n",
    "        \n",
    "        # CNN head after the SCAE encoder\n",
    "        # SCAE encoder output is 128 x 26 x 26\n",
    "        self.cnn_head = nn.Sequential(\n",
    "            # Conv layer 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Out: 256 x 12 x 12\n",
    "            norm_layer(256, 12),\n",
    "            activation_layer(),\n",
    "            \n",
    "            # Conv layer 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Out: 256 x 12 x 12\n",
    "            norm_layer(256, 13),\n",
    "            activation_layer(),\n",
    "            dropout_layer()\n",
    "        )\n",
    "        \n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # Input size is 256 * 12 * 12 = 43,264\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(256 * 12 * 12, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_prob if use_dropout else 0),\n",
    "            \n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_prob if use_dropout else 0),\n",
    "            \n",
    "            nn.Linear(2048, num_classes),\n",
    "            # nn.Softmax(dim=1) no softmax here bro, crossentropy does the softmax automatically\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the encoder part of SCAE\n",
    "        x = self.pretrained_scae.encoder(x)\n",
    "        # Continue with additional CNN layers\n",
    "        x = self.cnn_head(x)\n",
    "        \n",
    "        # Flatten and apply fully connected layers\n",
    "        x = self.flatten(x)\n",
    "        x = self.fully_connected(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98c5d320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:29.437054Z",
     "iopub.status.busy": "2025-05-25T17:07:29.436856Z",
     "iopub.status.idle": "2025-05-25T17:07:36.051774Z",
     "shell.execute_reply": "2025-05-25T17:07:36.050854Z"
    },
    "papermill": {
     "duration": 6.622476,
     "end_time": "2025-05-25T17:07:36.053140",
     "exception": false,
     "start_time": "2025-05-25T17:07:29.430664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 1, 105, 105])\n",
      "torch.Size([768, 200])\n"
     ]
    }
   ],
   "source": [
    "classifier = FontClassifier(pretrained_scae, num_classes=200).to(device)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch[0].shape)\n",
    "    print(classifier(batch[0].to(device)).shape)\n",
    "    # show_images_in_grid(batch.permute(0, 2, 3, 1).numpy(), titles=[f'Patch {i+1}' for i in range(len(batch))], cols=4)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3594648b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:36.066449Z",
     "iopub.status.busy": "2025-05-25T17:07:36.066205Z",
     "iopub.status.idle": "2025-05-25T17:07:36.206865Z",
     "shell.execute_reply": "2025-05-25T17:07:36.205764Z"
    },
    "papermill": {
     "duration": 0.14897,
     "end_time": "2025-05-25T17:07:36.208358",
     "exception": false,
     "start_time": "2025-05-25T17:07:36.059388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/classifier_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e68dea78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:36.222192Z",
     "iopub.status.busy": "2025-05-25T17:07:36.221953Z",
     "iopub.status.idle": "2025-05-25T17:07:36.368071Z",
     "shell.execute_reply": "2025-05-25T17:07:36.367532Z"
    },
    "papermill": {
     "duration": 0.154873,
     "end_time": "2025-05-25T17:07:36.369406",
     "exception": false,
     "start_time": "2025-05-25T17:07:36.214533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    scheduler=None,\n",
    "    device=\"cuda\",\n",
    "    num_epochs=5,\n",
    "    early_stopping_patience=None,\n",
    "    checkpoint_path=None,\n",
    "    use_amp=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic training loop for any (batch_size,1,105,105) ➔ (batch_size,num_classes) model.\n",
    "    \n",
    "    Args:\n",
    "        model:         nn.Module that maps inputs ➔ logits\n",
    "        train_loader:  DataLoader for training\n",
    "        val_loader:    DataLoader for validation\n",
    "        optimizer:     torch.optim.Optimizer\n",
    "        criterion:     loss function (e.g. nn.CrossEntropyLoss())\n",
    "        scheduler:     learning-rate scheduler (optional)\n",
    "        device:        'cuda' or 'cpu'\n",
    "        num_epochs:    number of epochs\n",
    "        early_stopping_patience: stop if no val-loss improvement for this many epochs (optional)\n",
    "        checkpoint_path: path to save best model state_dict (optional)\n",
    "        use_amp:       whether to use mixed precision (bool)\n",
    "    \n",
    "    Returns:\n",
    "        model:         best model (weights restored from checkpoint if provided)\n",
    "        history:       dict with lists: train_loss, train_acc, val_loss, val_acc\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    scaler = GradScaler() if use_amp and device != \"cpu\" else None\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [], \"train_acc\": [],\n",
    "        \"val_loss\":   [], \"val_acc\":   []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # ——— Training ————————————————————————————————\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[Epoch {epoch}/{num_epochs}] Train\", leave=False)\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    logits = model(inputs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # metrics\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            running_total   += labels.size(0)\n",
    "            pbar.set_postfix(loss=running_loss/running_total, acc=100.*running_correct/running_total)\n",
    "\n",
    "        train_loss = running_loss / running_total\n",
    "        train_acc  = 100. * running_correct / running_total\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "        # ——— Validation ———————————————————————————————\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total   = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f\"[Epoch {epoch}/{num_epochs}] Val  \", leave=False)\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        logits = model(inputs)\n",
    "                        loss = criterion(logits, labels)\n",
    "                else:\n",
    "                    logits = model(inputs)\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total   += labels.size(0)\n",
    "                pbar.set_postfix(val_loss=val_loss/val_total, val_acc=100.*val_correct/val_total)\n",
    "\n",
    "        val_loss_epoch = val_loss / val_total\n",
    "        val_acc_epoch  = 100. * val_correct / val_total\n",
    "        history[\"val_loss\"].append(val_loss_epoch)\n",
    "        history[\"val_acc\"].append(val_acc_epoch)\n",
    "\n",
    "        # ——— Scheduler step ————————————————————————————\n",
    "        if scheduler is not None:\n",
    "            # if ReduceLROnPlateau, pass val_loss\n",
    "            if hasattr(scheduler, \"step\") and scheduler.__class__.__name__ == \"ReduceLROnPlateau\":\n",
    "                scheduler.step(val_loss_epoch)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        # ——— Checkpoint & Early Stopping —————————————————————\n",
    "        if checkpoint_path is not None and val_loss_epoch < best_val_loss:\n",
    "            best_val_loss = val_loss_epoch\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            epochs_no_improve = 0\n",
    "            print(f\"→ New best model saved (val_loss={best_val_loss:.4f})\")\n",
    "        elif early_stopping_patience is not None:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stopping_patience:\n",
    "                print(f\"→ Early stopping after {epoch} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "        # summary print\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{num_epochs} \"\n",
    "            f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "            f\"Val: loss={val_loss_epoch:.4f}, acc={val_acc_epoch:.2f}%\"\n",
    "        )\n",
    "\n",
    "    # reload best weights if checkpoint was used\n",
    "    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1969998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:36.382741Z",
     "iopub.status.busy": "2025-05-25T17:07:36.382498Z",
     "iopub.status.idle": "2025-05-25T17:07:36.389936Z",
     "shell.execute_reply": "2025-05-25T17:07:36.389230Z"
    },
    "papermill": {
     "duration": 0.015344,
     "end_time": "2025-05-25T17:07:36.390959",
     "exception": false,
     "start_time": "2025-05-25T17:07:36.375615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, test_loader, device='cuda', use_amp=False):\n",
    "    \"\"\"\n",
    "    Testing-phase: aggregates 15 patches per sample, computes loss + top1/top5 metrics.\n",
    "    Returns (top1_error, top5_error).\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    correct1 = 0\n",
    "    correct5 = 0\n",
    "\n",
    "    test_pbar = tqdm(\n",
    "        total=len(test_loader),\n",
    "        desc=\"Testing\",\n",
    "        unit=\"batch\",\n",
    "        leave=True,\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # inputs: [B, P, C, H, W]\n",
    "            B, P, C, H, W = inputs.shape\n",
    "            inputs = inputs.view(B*P, C, H, W).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward all patches\n",
    "            if use_amp and device != 'cpu':\n",
    "                with autocast():\n",
    "                    logits = model(inputs)\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "\n",
    "            # Reshape + average over patches → [B, num_classes]\n",
    "            num_classes = logits.size(1)\n",
    "            avg_logits = logits.view(B, P, num_classes).mean(dim=1)\n",
    "\n",
    "            # Compute loss on averaged logits\n",
    "            loss = criterion(avg_logits, labels)\n",
    "            total_loss += loss.item() * B\n",
    "\n",
    "            # Top-1\n",
    "            _, pred1 = avg_logits.max(1)\n",
    "            correct1 += pred1.eq(labels).sum().item()\n",
    "\n",
    "            # Top-5\n",
    "            _, pred5 = avg_logits.topk(5, dim=1, largest=True, sorted=True)\n",
    "            correct5 += (pred5 == labels.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "            total_samples += B\n",
    "\n",
    "            test_pbar.set_postfix({\n",
    "                'loss':    f\"{loss.item():.4f}\",\n",
    "                'top1_acc': f\"{100.*correct1/total_samples:.2f}%\",\n",
    "                'top5_acc': f\"{100.*correct5/total_samples:.2f}%\"\n",
    "            })\n",
    "            test_pbar.update()\n",
    "\n",
    "    test_pbar.close()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    top1_acc = 100. * correct1 / total_samples\n",
    "    top5_acc = 100. * correct5 / total_samples\n",
    "\n",
    "    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Top-1 Accuracy: {top1_acc:.2f}% | Top-1 Error: {100.-top1_acc:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {top5_acc:.2f}% | Top-5 Error: {100.-top5_acc:.2f}%\")\n",
    "\n",
    "    return 100. - top1_acc, 100. - top5_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65da11c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:36.403609Z",
     "iopub.status.busy": "2025-05-25T17:07:36.403405Z",
     "iopub.status.idle": "2025-05-25T17:07:36.406453Z",
     "shell.execute_reply": "2025-05-25T17:07:36.405927Z"
    },
    "papermill": {
     "duration": 0.010253,
     "end_time": "2025-05-25T17:07:36.407412",
     "exception": false,
     "start_time": "2025-05-25T17:07:36.397159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ---- setup outside ----\n",
    "# model     = FontClassifier(pretrained_scae, num_classes=200).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "\n",
    "# # ---- call train() ----\n",
    "# best_model, history = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     optimizer,\n",
    "#     criterion,\n",
    "#     scheduler=scheduler,\n",
    "#     device='cuda',\n",
    "#     num_epochs=10,\n",
    "#     early_stopping_patience=2,\n",
    "#     checkpoint_path=\"/kaggle/working/classifier_checkpoints/best_font_model.pt\",\n",
    "#     use_amp=False\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Font_Detect_Updated v1.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7359793,
     "sourceId": 11936653,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 351469,
     "modelInstanceId": 330631,
     "sourceId": 404488,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 351994,
     "modelInstanceId": 331095,
     "sourceId": 407546,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.252488,
   "end_time": "2025-05-25T17:07:39.435061",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-25T17:07:13.182573",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
